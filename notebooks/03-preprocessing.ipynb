{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fabc894a",
   "metadata": {},
   "source": [
    "# Preparing and Preprocessing Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8d35b",
   "metadata": {},
   "source": [
    "# Preparing and Preprocessing Your Data\n",
    "\n",
    "## Collection\n",
    "\n",
    "The first step to preparing your data is to collect it. Whether you use API's to gather your material or some other method depends on your research interests. For this workshop, we'll use pre-gathered data.\n",
    "\n",
    "During the setup instructions, we asked you to download a number of files. These included about forty texts downloaded from [Project Gutenberg](https://www.gutenberg.org/), which will make up our corpus of texts for our hands on lessons in this course.\n",
    "\n",
    "Take a moment to orient and familiarize yourself with them:\n",
    "\n",
    "- Austen\n",
    "  - Emma - [record](https://gutenberg.org/ebooks/158#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Emma_(novel))\n",
    "  - Lady Susan - [record](https://gutenberg.org/ebooks/946#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Lady_Susan)\n",
    "  - Northanger Abbey - [record](https://gutenberg.org/ebooks/121#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Northanger_Abbey)\n",
    "  - Persuasion - [record](https://www.gutenberg.org/ebooks/105#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Persuasion_(novel))\n",
    "  - Pride and Prejudice - [record](https://gutenberg.org/ebooks/1342#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Pride_and_Prejudice)\n",
    "  - Sense and Sensibility - [record](https://gutenberg.org/ebooks/21839#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Sense_and_Sensibility)\n",
    "- Chesteron\n",
    "  - The Ball and the Cross - [record](https://gutenberg.org/ebooks/5265#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/The_Ball_and_the_Cross)\n",
    "  - The Innocence of Father Brown - [record](https://gutenberg.org/ebooks/204#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Father_Brown)\n",
    "  - The Man Who Knew Too Much - [record](https://gutenberg.org/ebooks/1720#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/The_Man_Who_Knew_Too_Much_(book))\n",
    "  - The Napoleon of Notting Hill - [record](https://gutenberg.org/ebooks/20058#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/The_Napoleon_of_Notting_Hill)\n",
    "  - The Man Who was Thursday - [record](https://gutenberg.org/ebooks/1695#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/The_Man_Who_Was_Thursday)\n",
    "  - The Ballad of the White Horse - [record](https://gutenberg.org/ebooks/1719#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/The_Ballad_of_the_White_Horse)\n",
    "- Dickens\n",
    "  - Bleak House - [record](https://www.gutenberg.org/ebooks/1023#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Bleak_House)\n",
    "  - A Christmas Carol - [record](https://gutenberg.org/ebooks/24022#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/A_Christmas_Carol)\n",
    "  - David Copperfield - [record](https://gutenberg.org/ebooks/766#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/David_Copperfield)\n",
    "  - Great Expectations - [record](https://gutenberg.org/ebooks/1400#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Great_Expectations)\n",
    "  - Hard Times - [record](https://gutenberg.org/ebooks/786#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Hard_Times_(novel))\n",
    "  - Oliver Twist - [record](https://gutenberg.org/ebooks/730#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Oliver_Twist)\n",
    "  - Our Mutual Friend - [record](https://gutenberg.org/ebooks/883#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Our_Mutual_Friend)\n",
    "  - The Pickwick Papers - [record](https://gutenberg.org/ebooks/580#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/The_Pickwick_Papers)\n",
    "  - A Tale of Two Cities - [record](https://gutenberg.org/ebooks/98#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/A_Tale_of_Two_Cities)\n",
    "- Dumas\n",
    "  - The Black Tulip - [record](https://gutenberg.org/ebooks/965#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/The_Black_Tulip)\n",
    "  - The Man in the Iron Mask - [record](https://gutenberg.org/ebooks/2759#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/The_Vicomte_of_Bragelonne:_Ten_Years_Later#Part_Three:_The_Man_in_the_Iron_Mask_(Chapters_181%E2%80%93269))\n",
    "  - The Count of Monte Cristo - [record](https://www.gutenberg.org/ebooks/1184#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/The_Count_of_Monte_Cristo)\n",
    "  - Ten Years Later - [record](https://gutenberg.org/ebooks/2681#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/The_Vicomte_of_Bragelonne:_Ten_Years_Later)\n",
    "  - The Three Musketeers - [record](https://gutenberg.org/ebooks/1257#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/The_Three_Musketeers)\n",
    "  - Twenty Years After - [record](https://gutenberg.org/ebooks/1259#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Twenty_Years_After)\n",
    "- Melville\n",
    "  - Bartleby, the Scrivener - [record](https://gutenberg.org/ebooks/11231#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Bartleby,_the_Scrivener)\n",
    "  - The Confidence-Man - [record](https://www.gutenberg.org/ebooks/21816) &middot; [wiki](https://en.wikipedia.org/wiki/The_Confidence-Man)\n",
    "  - Moby Dick - [record](https://gutenberg.org/ebooks/2701#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Moby-Dick)\n",
    "  - Omoo - [record](https://gutenberg.org/ebooks/4045#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Omoo)\n",
    "  - The Piazza Tales - [record](https://gutenberg.org/ebooks/15859#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/The_Piazza_Tales)\n",
    "  - Pierre - [record](https://gutenberg.org/ebooks/34970#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Pierre;_or,_The_Ambiguities)\n",
    "  - Typee - [record](https://gutenberg.org/ebooks/1900#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Typee)\n",
    "- Shakespeare\n",
    "  - The Trajedy of Julius Caesar - [record](https://gutenberg.org/ebooks/1120#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Julius_Caesar_(play))\n",
    "  - The Trajedy of King Lear - [record](https://gutenberg.org/ebooks/1532#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/King_Lear)\n",
    "  - A Midsummer Night's Dream - [record](https://gutenberg.org/ebooks/1514#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/A_Midsummer_Night%27s_Dream)\n",
    "  - Much Ado about Nothing - [record](https://gutenberg.org/ebooks/1519#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Much_Ado_About_Nothing)\n",
    "  - Othello, the Moor of Venice - [record](https://www.gutenberg.org/ebooks/1531) &middot; [wiki](https://en.wikipedia.org/wiki/Othello)\n",
    "  - Romeo and Juliet - [record](https://gutenberg.org/ebooks/1513#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Romeo_and_Juliet)\n",
    "  - Twelfth Night - [record](https://gutenberg.org/ebooks/1526#bibrec) &middot; [wiki](https://en.wikipedia.org/wiki/Twelfth_Night)\n",
    "\n",
    "While a full-sized corpus can include thousands of texts, these forty-odd texts will be enough for our illustrative purposes.\n",
    "\n",
    "## Loading Data into Python\n",
    "\n",
    "We'll start by mounting our Google Drive so that Colab can read the helper functions. We'll also go through how many of these functions are written in this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a923603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to mount your Google Drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Show existing colab notebooks and helpers.py file\n",
    "from os import listdir\n",
    "wksp_dir = '/content/drive/My Drive/Colab Notebooks/text-analysis/code'\n",
    "listdir(wksp_dir)\n",
    "\n",
    "# Add folder to colab's path so we can import the helper functions\n",
    "import sys\n",
    "sys.path.insert(0, wksp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b365ef",
   "metadata": {},
   "source": [
    "Next, we have a corpus of text files we want to analyze. Let's create a method to list those files. To make this method more flexible, we will also use ```glob``` to allow us to put in regular expressions so we can filter the files if so desired. ```glob``` is a tool for listing files in a directory whose file names match some pattern, like all files ending in ```*.txt```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a57621",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pathlib parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8bd1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08369cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_list(directory, filter_str='*'):\n",
    "  files = Path(directory).glob(filter_str)\n",
    "  files_to_analyze = list(map(str, files))\n",
    "  return files_to_analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041840b",
   "metadata": {},
   "source": [
    "Alternatively, we can load this function from the ```helpers.py``` file we provided for learners in this course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce90fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import create_file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a57611",
   "metadata": {},
   "source": [
    "Either way, now we can use that function to list the books in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae612f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = '/content/drive/My Drive/Colab Notebooks/text-analysis/data/books'\n",
    "corpus_file_list = create_file_list(corpus_dir)\n",
    "print(corpus_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15636dde",
   "metadata": {},
   "source": [
    "We will use the full corpus later, but it might be useful to filter to just a few specific files. For example, if I want just documents written by Austen, I can filter on part of the file path name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f70155",
   "metadata": {},
   "outputs": [],
   "source": [
    "austen_list = create_file_list(corpus_dir, 'austen*')\n",
    "print(austen_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd40ec4",
   "metadata": {},
   "source": [
    "Let's take a closer look at Emma. We are looking at the first full sentence, which begins with character 50 and ends at character 290."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df1e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_len = 290\n",
    "emmapath = create_file_list(corpus_dir, 'austen-emma*')[0]\n",
    "print(emmapath)\n",
    "sentence = \"\"\n",
    "with open(emmapath, 'r') as f:\n",
    "  sentence = f.read(preview_len)[50:preview_len]\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e95c4c",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Currently, our data is still in a format that is best for humans to read. Humans, without having to think too consciously about it, understand how words and sentences group up and divide into discrete units of meaning. We also understand that the words *run*, *ran*, and *running* are just different grammatical forms of the same underlying concept. Finally, not only do we understand how punctuation affects the meaning of a text, we also can make sense of texts that have odd amounts or odd placements of punctuation.\n",
    "\n",
    "For example, Darcie Wilder's [*literally show me a healthy person*](https://www.mtv.com/news/1vw892/read-an-excerpt-of-darcie-wilders-literally-show-me-a-healthy-person) has very little capitalization or punctuation:\n",
    "\n",
    "> in the unauthorized biography of britney spears she says her advice is to lift 1 lb weights and always sing in elevators every time i left to skateboard in the schoolyard i would sing in the elevator i would sing britney spears really loud and once the door opened and there were so many people they heard everything so i never sang again\n",
    "\n",
    "Across the texts in our corpus, our authors write with different styles, preferring different dictions, punctuation, and so on.\n",
    "\n",
    "To prepare our data to be more uniformly understood by our NLP models, we need to (a) break it into smaller units, (b) replace words with their roots, and (c) remove unwanted common or unhelpful words and punctuation. These steps encompass the preprocessing stage of the interpretive loop.\n",
    "\n",
    "![The Interpretive Loop](https://raw.githubusercontent.com/carpentries-incubator/python-text-analysis/main/images/01-Interpretive_Loop.JPG)\n",
    "\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down texts (strings of characters) into words, groups of words, and sentences. A string of characters needs to be understood by a program as smaller units so that it can be embedded. These are called **tokens**.  \n",
    "\n",
    "While our tokens will be single words for now, this will not always be the case. Different models have different ways of tokenizing strings. The strings may be broken down into multiple word tokens, single word tokens, or even components of words like letters or morphology. Punctuation may or may not be included.\n",
    "\n",
    "We will be using a tokenizer that breaks documents into single words for this lesson.\n",
    "\n",
    "Let's load our tokenizer and test it with the first sentence of Emma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "spacyt = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf701242",
   "metadata": {},
   "source": [
    "We will define a tokenizer method with the text editor. Keep this open so we can add to it throughout the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Our_Tokenizer:\n",
    "  def __init__(self):\n",
    "    #import spacy tokenizer/language model\n",
    "    self.nlp = en_core_web_sm.load()\n",
    "    self.nlp.max_length = 4500000 # increase max number of characters that spacy can process (default = 1,000,000)\n",
    "  def __call__(self, document):\n",
    "    tokens = self.nlp(document)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe4525",
   "metadata": {},
   "source": [
    "This will load spacy and its preprocessing pipeline for English. **Pipelines** are a series of interrelated tasks, where the output of one task is used as an input for another. Different languages may have different rulesets, and therefore require different preprocessing pipelines. Running the document we created through the NLP model we loaded performs a variety of tasks for us. Let's look at these in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = spacyt(sentence)\n",
    "for t in tokens:\n",
    " print(t.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8f3d8b",
   "metadata": {},
   "source": [
    "The single sentence has been broken down into a set of tokens. Tokens in spacy aren't just strings: They're python objects with a variety of attributes. Full documentation for these attributes can be found at: <https://spacy.io/api/token>\n",
    "\n",
    "### Stems and Lemmas\n",
    "\n",
    "Think about similar words, such as *running*, *ran*, and *runs*. All of these words have a similar root, but a computer does not know this. Without preprocessing, each of these words would be a new token.\n",
    "\n",
    "Stemming and Lemmatization are used to group together words that are similar or forms of the same word.\n",
    "\n",
    "Stemming is removing the conjugation and pluralized endings for words. For example, words like *digitization*, and *digitizing* might chopped down to *digitiz*.\n",
    "\n",
    "Lemmatization is the more sophisticated of the two, and looks for the linguistic base of a word. Lemmatization can group words that mean the same thing but may not be grouped through simple stemming, such as irregular verbs like *bring* and *brought*.\n",
    "\n",
    "Similarly, in naive tokenization, capital letters are considered different from non-capital letters, meaning that capitalized versions of words are considered different from non-capitalized versions. Converting all words to lower case ensures that capitalized and non-capitalized versions of words are considered the same.\n",
    "\n",
    "These steps are taken to reduce the complexities of our NLP models and to allow us to train them from less data.\n",
    "\n",
    "When we tokenized the first sentence of Emma above, Spacy also created a lemmatized version of itt. Let's try accessing this by typing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f09eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tokens:\n",
    "  print(t.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9928409d",
   "metadata": {},
   "source": [
    "Spacy stores words by an ID number, and not as a full string, to save space in memory. Many spacy functions will return numbers and not words as you might expect. Fortunately, adding an underscore for spacy will return text representations instead. We will also add in the lower case function so that all words are lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b7b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tokens:\n",
    " print(str.lower(t.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e348d",
   "metadata": {},
   "source": [
    "Notice how words like *best* and *her* have been changed to their root words like *good* and *she*. Let's change our tokenizer to save the lower cased, lemmatized versions of words instead of the original words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbd1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Our_Tokenizer:\n",
    "  def __init__(self):\n",
    "    # import spacy tokenizer/language model\n",
    "    self.nlp = en_core_web_sm.load()\n",
    "    self.nlp.max_length = 4500000 # increase max number of characters that spacy can process (default = 1,000,000)\n",
    "  def __call__(self, document):\n",
    "    tokens = self.nlp(document)\n",
    "    simplified_tokens = [str.lower(token.lemma_) for token in tokens]\n",
    "    return simplified_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bdbf68",
   "metadata": {},
   "source": [
    "### Stop-Words and Punctuation\n",
    "\n",
    "Stop-words are common words that are often filtered out for more efficient natural language data processing. Words such as *the* and *and* don't necessarily tell us a lot about a document's content and are often removed in simpler models. Stop lists (groups of stop words) are curated by sorting terms by their collection frequency, or the total number of times that they appear in a document or corpus. Punctuation also is something we are not interested in, at least not until we get to more complex models. Many open-source software packages for language processing, such as Spacy, include stop lists. Let's look at Spacy's stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f26a1",
   "metadata": {},
   "source": [
    "It's possible to add and remove words as well, for example, *zebra*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71105a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember, we need to tokenize things in order for our model to analyze them.\n",
    "z = spacyt(\"zebra\")[0]\n",
    "print(z.is_stop) # False\n",
    "\n",
    "# add zebra to our stopword list\n",
    "STOP_WORDS.add(\"zebra\")\n",
    "spacyt = spacy.load(\"en_core_web_sm\")\n",
    "z = spacyt(\"zebra\")[0]\n",
    "print(z.is_stop) # True\n",
    "\n",
    "# remove zebra from our list.\n",
    "STOP_WORDS.remove(\"zebra\")\n",
    "spacyt = spacy.load(\"en_core_web_sm\")\n",
    "z = spacyt(\"zebra\")[0]\n",
    "print(z.is_stop) # False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b91e84b",
   "metadata": {},
   "source": [
    "Let's add \"Emma\" to our list of stopwords, since knowing that the name \"Emma\" is often in Jane Austin does not tell us anything interesting.\n",
    "\n",
    "This will only adjust the stopwords for the current session, but it is possible to save them if desired. More information about how to do this can be found in the Spacy documentation. You might use this stopword list to filter words from documents using spacy, or just by manually iterating through it like a list.\n",
    "\n",
    "Let's see what our example looks like without stopwords and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9113a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add emma to our stopword list\n",
    "STOP_WORDS.add(\"emma\")\n",
    "spacyt = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# retokenize our sentence\n",
    "tokens = spacyt(sentence)\n",
    "\n",
    "for token in tokens:\n",
    "  if not token.is_stop and not token.is_punct:\n",
    "    print(str.lower(token.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee0842",
   "metadata": {},
   "source": [
    "Notice that because we added *emma* to our stopwords, she is not in our preprocessed sentence any more. Other stopwords are also missing such as numbers.\n",
    "\n",
    "Let's filter out stopwords and punctuation from our custom tokenizer now as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Our_Tokenizer:\n",
    "  def __init__(self):\n",
    "    # import spacy tokenizer/language model\n",
    "    self.nlp = en_core_web_sm.load()\n",
    "    self.nlp.max_length = 4500000 # increase max number of characters that spacy can process (default = 1,000,000)\n",
    "  def __call__(self, document):\n",
    "    tokens = self.nlp(document)\n",
    "    simplified_tokens = []    \n",
    "    for token in tokens:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            simplified_tokens.append(str.lower(token.lemma_))\n",
    "    return simplified_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792cdb8",
   "metadata": {},
   "source": [
    "### Parts of Speech\n",
    "\n",
    "While we can manually add Emma to our stopword list, it may occur to you that novels are filled with characters with unique and unpredictable names. We've already missed the word \"Woodhouse\" from our list. Creating an enumerated list of all of the possible character names seems impossible.\n",
    "\n",
    "One way we might address this problem is by using **Parts of speech (POS)** tagging. POS are things such as nouns, verbs, and adjectives. POS tags often prove useful, so some tokenizers also have built in POS tagging done. Spacy is one such library. These tags are not 100% accurate, but they are a great place to start. Spacy's POS tags can be used by accessing the ```pos_``` method for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "  if token.is_stop == False and token.is_punct == False:\n",
    "    print(str.lower(token.lemma_)+\" \"+token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf60494",
   "metadata": {},
   "source": [
    "Because our dataset is relatively small, we may find that character names and places weigh very heavily in our early models. We also have a number of blank or white space tokens, which we will also want to remove.\n",
    "\n",
    "We will finish our special tokenizer by removing punctuation and proper nouns from our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Our_Tokenizer:\n",
    "  def __init__(self):\n",
    "    # import spacy tokenizer/language model\n",
    "    self.nlp = en_core_web_sm.load()\n",
    "    self.nlp.max_length = 4500000 # increase max number of characters that spacy can process (default = 1,000,000)\n",
    "  def __call__(self, document):\n",
    "    tokens = self.nlp(document)\n",
    "    simplified_tokens = [\n",
    "      #our helper function expects spacy tokens. It will take care of making them lowercase lemmas.\n",
    "      token for token in tokens\n",
    "      if not token.is_stop\n",
    "      and not token.is_punct\n",
    "      and token.pos_ != \"PROPN\"\n",
    "    ]\n",
    "    return simplified_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4309fc42",
   "metadata": {},
   "source": [
    "Alternative, instead of \"blacklisting\" all of the parts of speech we don't want to include, we can \"whitelist\" just the few that we want, based on what they information they might contribute to the meaning of a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Our_Tokenizer:\n",
    "  def __init__(self):\n",
    "    # import spacy tokenizer/language model\n",
    "    self.nlp = en_core_web_sm.load()\n",
    "    self.nlp.max_length = 4500000 # increase max number of characters that spacy can process (default = 1,000,000)\n",
    "  def __call__(self, document):\n",
    "    tokens = self.nlp(document)\n",
    "    simplified_tokens = [\n",
    "      #our helper function expects spacy tokens. It will take care of making them lowercase lemmas.\n",
    "      token for token in tokens\n",
    "      if not token.is_stop\n",
    "      and not token.is_punct\n",
    "      and token.pos_ in {\"ADJ\", \"ADV\", \"INTJ\", \"NOUN\", \"VERB\"}\n",
    "    ]\n",
    "    return simplified_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec1127",
   "metadata": {},
   "source": [
    "Either way, let's test our custom tokenizer on this selection of text to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Our_Tokenizer()\n",
    "tokens = tokenizer(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f674e6",
   "metadata": {},
   "source": [
    "## Putting it All Together\n",
    "\n",
    "Now that we've built a tokenizer we're happy with, lets use it to create lemmatized versions of all the books in our corpus.\n",
    "\n",
    "That is, we want to turn this:\n",
    "\n",
    "\"Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
    "and happy disposition, seemed to unite some of the best blessings\n",
    "of existence; and had lived nearly twenty-one years in the world\n",
    "with very little to distress or vex her.\"\n",
    "\n",
    "into this:\n",
    "\n",
    "\"handsome\n",
    "clever\n",
    "rich\n",
    "comfortable\n",
    "home\n",
    "happy\n",
    "disposition\n",
    "seem\n",
    "unite\n",
    "good\n",
    "blessing\n",
    "existence\n",
    "live\n",
    "nearly\n",
    "year\n",
    "world\n",
    "very\n",
    "little\n",
    "distress\n",
    "vex\"\n",
    "\n",
    "To help make this *relatively* quick for all the text in all our books, we'll use a helper function we prepared for learners to use our tokenizer, do the casing and lemmatization we discussed earlier, and write the results to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4336cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import lemmatize_files\n",
    "\n",
    "# SKIP THIS - it takes too long to run during a live class. Lemma files are preprocessed for you and saved to data/book_lemmas\n",
    "#lemma_file_list = lemmatize_files(tokenizer, corpus_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4247763",
   "metadata": {},
   "source": [
    "This process may take several minutes to run. If you don't want to wait, you can stop the cell running and use our pre-baked solution (lemma files) found in data/book_lemmas. The next section will walk you through both options.\n",
    "\n",
    "## Creating dataframe to work with files and lemmas easily\n",
    "\n",
    "Let's save a dataframe / spreadsheet that lists all our authors, books, and associated filenames, both the original and lemmatized copies.\n",
    "\n",
    "We'll use another helper we prepared to make this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54453145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import parse_into_dataframe\n",
    "pattern = \"/content/drive/My Drive/Colab Notebooks/text-analysis/data/books/{author}-{title}.txt\"\n",
    "data = parse_into_dataframe(pattern, corpus_file_list)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf684923",
   "metadata": {},
   "source": [
    "Next, we can add the lemma files to the dataframe. If you ran the lemmatize_files() function above successfully, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Lemma_File\"] = lemma_file_list\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cabe28",
   "metadata": {},
   "source": [
    "Otherwise, we can add the \"pre-baked\" lemmas to our dataframe using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77551e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_path(file_path):\n",
    "    # Convert to Path object for easier manipulation\n",
    "    p = Path(file_path)\n",
    "    # Extract the filename like 'austen-sense.txt'\n",
    "    file_name = p.name\n",
    "    # Create new path with 'book_lemmas' instead of 'books' and add .lemmas\n",
    "    lemma_name = file_name + \".lemmas\"\n",
    "    return str(p.parent.parent / \"book_lemmas\" / lemma_name)\n",
    "\n",
    "# Add new column\n",
    "data[\"Lemma_File\"] = data[\"File\"].apply(get_lemma_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79205b3f",
   "metadata": {},
   "source": [
    "Finally, we'll save this table to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f43884",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"/content/drive/My Drive/Colab Notebooks/text-analysis/data/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc11f65",
   "metadata": {},
   "source": [
    "## Outro and Conclusion\n",
    "\n",
    "This lesson has covered a number of preprocessing steps. We created a list of our files in our corpus, which we can use in future lessons. We customized a tokenizer from Spacy, to better suit the needs of our corpus, which we can also use moving forward.\n",
    "\n",
    "Next lesson, we will start talking about the concepts behind our model."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
