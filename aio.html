<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Text Analysis in Python: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/cp/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicons/cp/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicons/cp/favicon-16x16.png">
<link rel="manifest" href="favicons/cp/site.webmanifest">
<link rel="mask-icon" href="favicons/cp/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav carpentries"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="cp-logo" alt="Lesson Description" src="assets/images/carpentries-logo.svg"><span class="badge text-bg-info">
          <abbr title="This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-circle" style="border-radius: 5px"></i>
              Beta
            </a>
            <span class="visually-hidden">This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav carpentries" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="assets/images/carpentries-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Text Analysis in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Text Analysis in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<li><a class="dropdown-item" href="discuss.html">Discussion</a></li>
<li><a class="dropdown-item" href="reference.html">Glossary</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Text Analysis in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress carpentries">
    <div class="progress-bar carpentries" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-basicConcepts.html">1. Introduction to Natural Language Processing</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-apis.html">2. Corpus Development- Text Data Collection</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-preprocessing.html">3. Preparing and Preprocessing Your Data</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-vectorSpace.html">4. Vector Space and Distance</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-tf-idf-documentEmbeddings.html">5. Document Embeddings and TF-IDF</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-lsa.html">6. Latent Semantic Analysis</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-wordEmbed_intro.html">7. Intro to Word Embeddings</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="08-wordEmbed_word2vec-algorithm.html">8. The Word2Vec Algorithm</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush10">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading10">
        <a href="09-wordEmbed_train-word2vec.html">9. Training Word2Vec</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush11">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading11">
        <a href="10-finetuning-transformers.html">10. Finetuning LLMs</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush12">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading12">
        <a href="11-ethics.html">11. Ethics and Text Analysis</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="discuss.html">Discussion</a></li>
<li><a href="reference.html">Glossary</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-basicConcepts"><p>Content from <a href="01-basicConcepts.html">Introduction to Natural Language Processing</a></p>
<hr>
<p>Last updated on 2025-05-01 |

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/episodes/01-basicConcepts.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is Natural Language Processing?</li>
<li>What tasks can be done by Natural Language Processing?</li>
<li>What does a workflow for an NLP project look?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learn the tasks that NLP can do</li>
<li>Use a pretrained chatbot in python</li>
<li>Discuss our workflow for performing NLP tasks</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width"></section><section><h2 class="section-heading" id="what-is-natural-language-processing">What is Natural Language Processing?<a class="anchor" aria-label="anchor" href="#what-is-natural-language-processing"></a>
</h2>
<hr class="half-width">
<p>Text Analysis, also known as Natural Language Processing or NLP, is a
subdiscipline of the larger disciplines of machine learning and
artificial intelligence.</p>
<p>AI and machine learning both use complex mathematical constructs
called <strong>models</strong> to take data as an input and produce a
desired output.</p>
<p>What distinguishes NLP from other types of machine learning is that
text and human language is the main input for NLP tasks.</p>
</section><section><h2 class="section-heading" id="context-for-digital-humanists">Context for Digital Humanists<a class="anchor" aria-label="anchor" href="#context-for-digital-humanists"></a>
</h2>
<hr class="half-width">
<p>Before we get started, we would like to also provide a disclaimer.
The humanities involves a wide variety of fields. Each of those fields
brings a variety of research interests and methods to focus on a wide
variety of questions.</p>
<p>AI is not infallible or without bias. NLP is simply another tool you
can use to analyze texts and should be critically considered in the same
way any other tool would be. The goal of this workshop is not to replace
or discredit existing humanist methods, but to help humanists learn new
tools to help them accomplish their research.</p>
</section><section><h2 class="section-heading" id="the-interpretive-loop">The Interpretive Loop<a class="anchor" aria-label="anchor" href="#the-interpretive-loop"></a>
</h2>
<hr class="half-width">
<figure><img src="fig/01-Interpretive_Loop.JPG" alt="The Interpretive Loop" class="figure mx-auto d-block"></figure><p>Despite the array of tasks encompassed within text analysis, many
share common underlying processes and methodologies. Throughout our
exploration, we’ll navigate an ‘interpretive loop’ that connects our
research inquiries with the tools and techniques of natural language
processing (NLP). This loop comprises several recurring stages:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Formulating a research question or NLP task</strong>: Each
journey begins with defining a task or problem within the domain of the
digital humanities. This might involve authorship attribution, topic
modeling, named entity recognition (NER), sentiment analysis, text
prediction, or search, among others.</li>
<li>
<strong>Data collection and corpus building</strong>: With a clear
objective in mind, the next step involves gathering relevant data and
constructing a corpus (a set of documents). This corpus serves as the
foundation for our analysis and model training. It may include texts,
documents, articles, social media posts, or any other textual data
pertinent to the research task.</li>
<li>
<strong>Data preprocessing</strong>: Before our data can be fed into
NLP models, it undergoes preprocessing steps to clean, tokenize, and
format the text. This ensures compatibility with our chosen model and
facilitates efficient computation.</li>
<li>
<strong>Generating embeddings</strong>: Our processed data is then
transformed into mathematical representations known as embeddings. These
embeddings capture semantic and contextual information in the corpus,
bridging the gap between human intuition and machine algorithms.</li>
<li>
<strong>Embedding-related tasks</strong>: Leveraging embeddings, we
perform various tasks such as measuring similarity between documents,
summarizing texts, or extracting key insights.</li>
<li>
<strong>Results</strong>: Results are generated from specific
embedding-related tasks, such as measuring document similarity, document
summarization, or topic modeling to uncover latent themes within a
corpus.</li>
<li>
<strong>Interpreting results</strong>: Finally, we interpret the
outputs in the context of our research objectives, stakeholder
interests, and broader scholarly discourse. This critical analysis
allows us to draw conclusions, identify patterns, and refine our
approach as needed.</li>
</ol>
<p>Additionally, we consider how the results may inspire future
directions of inquiry, such as conducting repeat analyses with different
data cleaning methods, exploring related research questions, or refining
the original research question based on the insights gained. This
iterative process enables us to continually deepen our understanding and
contribute to ongoing scholarly conversations.</p>
</section><section><h2 class="section-heading" id="nlp-tasks">NLP Tasks<a class="anchor" aria-label="anchor" href="#nlp-tasks"></a>
</h2>
<hr class="half-width">
<p>We’ll start by trying to understand what tasks NLP can do. Some of
the many functions of NLP include topic modelling and categorization,
named entity recognition, search, summarization and more.</p>
<p>We’re going to explore some of these tasks in this lesson using the
popular “HuggingFace” library.</p>
<p>Launch a web browser and navigate to <a href="https://huggingface.co/tasks" class="external-link uri">https://huggingface.co/tasks</a>. Here we can see examples
of many of the tasks achievable using NLP.</p>
<p>What do these different tasks mean? Let’s take a look at an example.
A user engages in conversation with a bot. The bot generates a response
based on the user’s prompt. This is called text generation. Let’s click
on this task now: <a href="https://huggingface.co/tasks/text-generation" class="external-link uri">https://huggingface.co/tasks/text-generation</a></p>
<p>HuggingFace usefully provides an online demo as well as a description
of the task. On the right, we can see there is a demo of a particular
model that does this task. Give conversing with the chatbot a try.</p>
<p>If we scroll down, much more information is available. There is a
link to sample models and datasets HuggingFace has made available that
can do variations of this task. Documentation on how to use the model is
available by scrolling down the page. Model specific information is
available by clicking on the model.</p>
<div class="section level3">
<h3 id="worked-example-chatbot-in-python">Worked Example: Chatbot in Python<a class="anchor" aria-label="anchor" href="#worked-example-chatbot-in-python"></a>
</h3>
<p>We’ve got an overview of what different tasks we can accomplish. Now
let’s try getting started with doing these tasks in Python. We won’t
worry too much about how this model works for the time being, but will
instead just focusing trying it out. We’ll start by running a chatbot,
just like the one we used online.</p>
<p>NLP tasks often need to be broken down into simpler subtasks to be
executed in a particular order. These are called
<strong>pipelines</strong> since the output from one subtask is used as
the input to the next subtask. We will now define a “pipeline” in
Python.</p>
<p>Launch either colab or our Anaconda environment, depending on your
setup. Try following the example below.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> transformers.utils <span class="im">import</span> logging</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co">#disable warning about optional authentication</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>logging.set_verbosity_error()</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>text2text_generator <span class="op">=</span> pipeline(<span class="st">"text2text-generation"</span>)</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="bu">print</span>(text2text_generator(<span class="st">"question: What is 42 ? context: 42 is the answer to life, the universe and everything"</span>))</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">TXT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode txt" tabindex="0"><code class="sourceCode default"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>[{'generated_text': 'the answer to life, the universe and everything'}]</span></code></pre>
</div>
<p>Feel free to prompt the chatbot with a few prompts of your own.</p>
</div>
<div class="section level3">
<h3 id="group-activity-and-discussion">Group Activity and Discussion<a class="anchor" aria-label="anchor" href="#group-activity-and-discussion"></a>
</h3>
<p>With some experience with a task, let’s get a broader overview of the
types of tasks we can do. Relaunch a web browser and go back to <a href="https://huggingface.co/tasks" class="external-link uri">https://huggingface.co/tasks</a>. Break out into groups and
look at a couple of tasks for HuggingFace. The groups will be based on
general categories for each task. Discuss possible applications of this
type of model to your field of research. Try to brainstorm possible
applications for now, don’t worry about technical implementation.</p>
<ol style="list-style-type: decimal">
<li>Tasks that seek to convert non-text into text</li>
</ol>
<ul>
<li><a href="https://huggingface.co/tasks/image-to-text" class="external-link uri">https://huggingface.co/tasks/image-to-text</a></li>
<li><a href="https://huggingface.co/tasks/text-to-image" class="external-link uri">https://huggingface.co/tasks/text-to-image</a></li>
<li><a href="https://huggingface.co/tasks/automatic-speech-recognition" class="external-link uri">https://huggingface.co/tasks/automatic-speech-recognition</a></li>
<li><a href="https://huggingface.co/tasks/image-to-text" class="external-link uri">https://huggingface.co/tasks/image-to-text</a></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Searching and classifying documents as a whole</li>
</ol>
<ul>
<li><a href="https://huggingface.co/tasks/text-classification" class="external-link uri">https://huggingface.co/tasks/text-classification</a></li>
<li><a href="https://huggingface.co/tasks/sentence-similarity" class="external-link uri">https://huggingface.co/tasks/sentence-similarity</a></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Classifying individual words- Sequence based tasks</li>
</ol>
<ul>
<li><a href="https://huggingface.co/tasks/token-classification" class="external-link uri">https://huggingface.co/tasks/token-classification</a></li>
<li><a href="https://huggingface.co/tasks/translation" class="external-link uri">https://huggingface.co/tasks/translation</a></li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Interactive and generative tasks such as conversation and question
answering</li>
</ol>
<ul>
<li><a href="https://huggingface.co/tasks/text-generation" class="external-link uri">https://huggingface.co/tasks/text-generation</a></li>
<li><a href="https://huggingface.co/tasks/question-answering" class="external-link uri">https://huggingface.co/tasks/question-answering</a></li>
</ul>
<p>Briefly present a summary of some of the tasks you explored. What
types of applications could you see this type of task used in? How might
this be relevant to a research question you have? Summarize these tasks
and present your findings to the group.</p>
</div>
<div class="section level3">
<h3 id="what-tasks-can-nlp-do">What tasks can NLP do?<a class="anchor" aria-label="anchor" href="#what-tasks-can-nlp-do"></a>
</h3>
<p>There are many models for representing language. The model we chose
for our task will depend on what we want the output of our model to do.
In other words, our model will vary based on the <strong>task</strong>
we want it to accomplish.</p>
<p>We can think of the various tasks NLP can do as different types of
desired outputs, which may require different models depending on the
task.</p>
<p>Let’s discuss tasks you may find interesting in more detail. These
are not the only tasks NLP can accomplish, but they are frequently of
interest for Humanities scholars.</p>
<div class="section level4">
<h4 id="search">Search<a class="anchor" aria-label="anchor" href="#search"></a>
</h4>
<p>Search attempts to retrieve documents that are similar to a query. In
order to do this, there must be some way to compute the similarity
between documents. A search query can be thought of as a small input
document, and the outputs could be a score of relevant documents stored
in the corpus. While we may not be building a search engine, we will
find that similarity metrics such as those used in search are important
to understanding NLP.</p>
<figure><img src="fig/01-search.png" alt="Search and Document Summarization" class="figure mx-auto d-block"></figure>
</div>
<div class="section level4">
<h4 id="topic-modeling">Topic Modeling<a class="anchor" aria-label="anchor" href="#topic-modeling"></a>
</h4>
<p>Topic modeling is a type of analysis that attempts to categorize
documents into categories. These categories could be human generated
labels, or we could ask our model to group together similar texts and
create its own labels. For example, the Federalist Papers are a set of
85 essays written by three American Founding Fathers- Alexander
Hamilton, James Madison and John Jay. These papers were written under
pseudonyms, but many of the papers authors were later identified. One
use for topic modelling might be to present a set of papers from each
author that are known, and ask our model to label the federalist papers
whose authorship is in dispute.</p>
<p>Alternatively, the computer might be asked to come up with a set
number of topics, and create categories without precoded documents, in a
process called unsupervised learning. Supervised learning requires human
labelling and intervention, where unsupervised learning does not.
Scholars may then look at the categories created by the NLP model and
try to interpret them. One example of this is <a href="https://dsl.richmond.edu/dispatch/" class="external-link">Mining the Dispatch</a>, which
tries to categorize articles based on unsupervised learning topics.</p>
<figure><img src="fig/01-topicmodelling.png" alt="Topic Modeling Graph" class="figure mx-auto d-block"></figure>
</div>
<div class="section level4">
<h4 id="token-classification">Token Classification<a class="anchor" aria-label="anchor" href="#token-classification"></a>
</h4>
<p>The task of token classification is trying to apply labels on a more
granular level- labelling words as belonging to a certain group. The
entities we are looking to recognize may be common. Parts of Speech
(POS) Tagging looks to give labels to entities such as verbs, nouns, and
so on. Named Entity Recognition (NER) seeks to label things such as
places, names of individuals, or countries might not be easily
enumerated. A possible application of this would be to track
co-occurrence of characters in different chapters in a book.</p>
<figure><img src="fig/01-ner.png" alt="Named Entity Recognition" class="figure mx-auto d-block"></figure>
</div>
<div class="section level4">
<h4 id="document-summarization">Document Summarization<a class="anchor" aria-label="anchor" href="#document-summarization"></a>
</h4>
<p>Document summarization takes documents which are longer, and attempts
to output a document with the same meaning by finding relevant snippets
or by generating a smaller document that conveys the meaning of the
first document. Think of this as taking a large set of input data of
words and outputting a smaller output of words that describe our
original text.</p>
</div>
<div class="section level4">
<h4 id="text-prediction">Text Prediction<a class="anchor" aria-label="anchor" href="#text-prediction"></a>
</h4>
<p>Text prediction attempts to predict future text inputs from a user
based on previous text inputs. Predictive text is used in search engines
and also on smartphones to help correct inputs and speed up the process
of text input. It is also used in popular models such as ChatGPT.</p>
</div>
</div>
</section><section><h2 class="section-heading" id="summary-and-outro">Summary and Outro<a class="anchor" aria-label="anchor" href="#summary-and-outro"></a>
</h2>
<hr class="half-width">
<p>We’ve looked at a general process or ‘interpretive loop’ for NLP.
We’ve also seen a variety of different tasks you can accomplish with
NLP. We used Python to generate text based on one of the models
available through HuggingFace. Hopefully this gives some ideas about how
you might use NLP in your area of research.</p>
<p>In the lessons that follow, we will be working on better
understanding what is happening in these models. Before we can use a
model though, we need to make sure we have data to build our model on.
Our next lesson will be looking at one tool to build a dataset called
APIs.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>NLP is comprised of models that perform different tasks.</li>
<li>Our workflow for an NLP project consists of designing,
preprocessing, representation, running, creating output, and
interpreting that output.</li>
<li>NLP tasks can be adapted to suit different research interests.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-02-apis"><p>Content from <a href="02-apis.html">Corpus Development- Text Data Collection</a></p>
<hr>
<p>Last updated on 2025-05-01 |

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/episodes/02-apis.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do I evaluate what kind of data to use for my project?</li>
<li>What do I need to consider when building my corpus?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Become familiar with technical and legal/ethical considerations for
data collection.</li>
<li>Practice evaluating text data files created through different
processes.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="corpus-development--text-data-collection">Corpus Development- Text Data Collection<a class="anchor" aria-label="anchor" href="#corpus-development--text-data-collection"></a>
</h2>
<hr class="half-width"></section><section><h2 class="section-heading" id="building-your-corpus">Building Your Corpus<a class="anchor" aria-label="anchor" href="#building-your-corpus"></a>
</h2>
<hr class="half-width">
<p>The best sources to build a corpus, or dataset, for text analysis
will ultimately depend on the needs of your project. Datasets and
sources are not usually prepared to be used in specific kinds of
projects, therefore the burden is on the researcher to select materials
that will be suitable for their corpus.</p>
</section><section><h2 class="section-heading" id="evaluating-data">Evaluating Data<a class="anchor" aria-label="anchor" href="#evaluating-data"></a>
</h2>
<hr class="half-width">
<p>It can be tempting to find a source and grab its data in bulk,
trusting that it will be a fit for your analyses because it meets
certain criteria. However, it is important to think critically about the
data you are gathering, its context, and the corpus you are assembling.
Doing so will allow you to create a corpus that can both meet your
project’s needs and possibly serve as its own contribution to your
field. As you collect your data and assemble your corpus, you will need
to think critically about the content, file types, reduction of bias,
rights and permissions, quality, and features needed for your analysis.
You may find that no one source fits all of these needs and it may be
best to put together a corpus from a variety of sources.</p>
</section><section><h2 class="section-heading" id="content-type">Content type<a class="anchor" aria-label="anchor" href="#content-type"></a>
</h2>
<hr class="half-width">
<p>Materials used in projects can be either born digital, meaning that
they originated in a digital format, or digitized, meaning that they
were converted to a digital format. Common sources of text data for text
analysis projects include digitized archival or primary source
materials, newspapers, books, social media, and research articles.
Depending on your project, you may even need to digitize some materials
yourself. If you are accessing born digital materials, you will want to
document the dates you accessed the resources as sources that are born
digital may change over time and diverge from those in your corpus. If
you are digitizing materials, you will want to document your process for
digitization and make sure you are considering the rights and
restrictions that apply to your materials.</p>
<p>The nature of your research question will inform the content type and
your potential data sources. A question like “How are women represented
in 19th century texts?” is very broad. A corpus that explores this
question might quickly exceed your computing power as it is large enough
to include all content types. Instead, it would be helpful to narrow the
scope of the question and this will also narrow down the content type
and potential sources. Which women? Where? What kind of texts -
newspapers, novels, magazines, legal documents? A question like “How are
women represented in classic 19th century American novels?” narrows the
scope and content type to 19th century classic American novels.</p>
<p>Once you know the type of materials you need, you can begin exploring
data sources for your project. Sources of text data can include
government documents, cultural institutions such as archives, libraries,
and museums, online projects, and commercial sources. Many sources make
their data easily available for download or through an API. Depending on
the source, you may also be able to reach out and ask for a copy of
data. Other sources, such as commercial vendors, including vendors that
work with libraries, can restrict access to their full text data or not
allow for download outside of their platform. Although researchers tend
to prefer full text data for text analysis, metadata from a source can
also be useful for analysis.</p>
</section><section><h2 class="section-heading" id="file-types">File types<a class="anchor" aria-label="anchor" href="#file-types"></a>
</h2>
<hr class="half-width">
<p>Text data can come in different forms, including unstructured text in
a plain text file, or in a structured file such as json, html, or xml.
As you collect files for potential use in your corpus, creating an
inventory of the file types will be helpful as you decide whether to
include files, which files to convert, and what kind of analyses you may
want to explore.</p>
<p>You may find that the documents you want to analyze may not be in the
format you want them to be. They may not even be in text form. A common
source of data for text analysis in the digital humanities includes
digitized sources. Digitized documents result in jpeg images, which
aren’t very useful for text analysis. Some sources also provide a text
file for the digitized image which is generated by either Optical
Character Recognition (ORC) or, if the document was handwritten, by
Handwritten Text Recognition (HTR), which converts images to text. A
source may have audio files that are important to your corpus and may or
may not contain a transcript generated by speech transcription software.
The process of converting files is out of scope for this lesson, but it
is worth mentioning that you can also use an OCR tool such as Tesseract,
an HTR tool like eScriptorium, or a speech to text tool like DeepSpeech,
which are all open source, to convert your files from image or audio to
text.</p>
</section><section><h2 class="section-heading" id="rights-and-restrictions">Rights and Restrictions<a class="anchor" aria-label="anchor" href="#rights-and-restrictions"></a>
</h2>
<hr class="half-width">
<p>One of the most important criteria for inclusion in your corpus is
whether or not you have the right to use the data in the way your
project requires. When evaluating data sources for your project, you may
need to navigate a variety of legal and ethical issues. We’ll briefly
mention some of them below, but to learn more about these issues, we
recommend the open access book Building Legal Literacies for Text and
Data Mining. If you are working with foreign-held or licensed content or
your project involves international research collaborations, we
recommend reviewing resources from the Legal Literacies for Text Data
Mining- Cross Border Project (LLTDMX).</p>
<ul>
<li>
<strong>Copyright</strong> - Copyright law in the United States
protects original works of authorship and grants the right to reproduce
the work, to create derivative works, distribute copies, perform the
work publicly, and to share the work publicly. Fair use may create
exceptions for some TDM activities, but if you are analyzing the full
text of copyrighted material, publicly sharing that corpus would most
likely not be allowed.</li>
<li>
<strong>Licensing</strong> - Licenses grant permission to use
materials in certain ways while usually restricting others. If you are
working with databases or other licensed collections of materials, make
sure that you understand the license and how it applies to text and data
mining.</li>
<li>
<strong>Terms of Use</strong> - If you are collecting text data from
other sources, such as websites or applications, make sure that you
understand any retrictions on how the data can be used.</li>
<li>
<strong>Technology Protection Measures</strong> - Some publishers
and content hosts protect their copyrighted/licensed materials through
encryption. While commercial versions of ebooks, for example, would make
for easy content to analyze, circumventing these protections would be
illegal in the United States under the Digital Millennium Copyright
Act.</li>
<li>
<strong>Privacy</strong> - Before sharing a corpus publicly,
consider whether doing so would constitute any legal or ethical
violations, especially with regards to privacy. Consulting with digital
scholarship librarians at your university or professional organizations
in your field would be a good place to learn about privacy issues that
might arise with the type of data you are working with.</li>
<li>
<strong>Research Protections</strong> - Depending on the type of
corpus you are creating, you might need to consider human subject
research protections such as informed consent. Your institution’s
Institutional Review Board may be able to help you navigate emerging
issues surrounding text data that is publicly available but could be
sensitive, such as social media data.</li>
</ul></section><section><h2 class="section-heading" id="assessing-data-sources-for-bias">Assessing Data Sources for Bias<a class="anchor" aria-label="anchor" href="#assessing-data-sources-for-bias"></a>
</h2>
<hr class="half-width">
<p>Thinking critically about sources of data and the bias they may
introduce to your project is important. It can be tempting to think that
datasets are objective and that computational analysis can give you
objective answers, however, the strength of the humanities is being able
to interpret and understand subjectivity. Who created the data you are
considering and for what purpose? What biases might they have held and
how might that impact what is included or excluded from your data?</p>
<p>It is also important to think about the bias you may create as you
choose your sources and assemble your corpus. If you are creating a
corpus to explore how immigrant women are represented in 19th century
American novels, you should consider who you are representing in your
own corpus. Are any of the authors you are including women? Are any of
them immigrants? Including different perspectives can give you a richer
corpus that could lead to multiple insights that wouldn’t have been
possible with a more limited corpus.</p>
<p>Another source of bias that you should consider is the bias in
datasets used to train models you might use in your research and what
impact they might have on your analysis. Research the models you are
considering. What data were they trained on? Are there known issues with
those datasets? If there are known bias issues with the model or you
discover some, you will need to consider your options. Is it possible to
remediate the model by either removing the biased dataset or adding new
training data? Is there an alternative model trained on different
data?</p>
</section><section><h2 class="section-heading" id="data-quality-and-features">Data Quality and Features<a class="anchor" aria-label="anchor" href="#data-quality-and-features"></a>
</h2>
<hr class="half-width">
<p>Sources of text data each have their own characteristics depending on
content type and whether the source was digitized, born digital, or
converted from another medium. This may impact the quality of the data
or give it certain characteristics. As you assemble your corpus, you
should think critically about how the quality of the data and its
features might impact your analysis or your decision to include it.</p>
<p>Text data sources that are born digital, meaning that they are
created in digital formats rather than being converted or digitized,
tend to have better quality data. However, this does not mean that they
will necessarily be the best for your project or easier to work with.
You should become familiar with your data sources, the way the data
source impacts the text data, and options for improving the data quality
if necessary.</p>
<p>Let’s look at two different content types, a novel and a newspaper,
and how they are formatted. We’ll be working with novels from Project
Gutenberg in the next lesson, including the novel “Emma” by Jane Austen.
In this lesson we’ll compare the data from that ebook with OCR text data
from a digitized newspaper of an article about Jane Austen.</p>
<p>Let’s explore the Project Gutenberg file for “Emma.” Project
Gutenberg offers public domain ebooks in HTML or plain text. Uploaded
versions must be proofread and often have had page numbers, headers, and
footers removed. This makes for good quality plain text data that is
easy to work with. However, it includes language about the project and
the rights associated with the ebook at the beginning of each file that
may need to be removed for cleaner text.</p>
<p>This novel is formatted to include a table of contents at the
beginning that outlines its structure. Depending on your analysis, you
could use these features to either divide the text data into its volumes
and chapters or if you don’t need it, you can decide to remove the
capitalized words volume and chapters from the corpus.</p>
<p>Now let’s look at a digitized image of an article about Jane Austen
from the Library of Congress’s Chronicling America: Historic American
Newspapers collection and its accompanying OCR text.</p>
<p>You can see that the text in the image is in columns. Because of the
way the OCR process works, the OCR text data will be in columns as well
and will preserve all the instances of words being broken up by this
feature. When you look at the OCR text file, you can see that it also
includes the text of all the other articles in the same image.</p>
<p>When you look at the quality of the text data, you can see that it is
full of misspelled and broken up words. If you wanted to include it in a
corpus, you might want to improve the quality of the text data by
increasing the contrast or sharpening the image of the text you want and
running it through OCR tools. An advanced technique involves running the
image through three OCR programs and comparing the outputs against each
other.</p>
</section><section><h2 class="section-heading" id="assembling-your-corpus">Assembling Your Corpus<a class="anchor" aria-label="anchor" href="#assembling-your-corpus"></a>
</h2>
<hr class="half-width">
<p>Now that you have an understanding of what you need to consider when
collecting data for a corpus, it can be useful to create a list with the
requirements of your specific project to help you evaluate your data.
Your corpus might be made up from different sources that you are
bringing together. It is important for you to document the sources for
your data, including the date accessed, search terms you used, and any
decisions you made about what to include or exclude. Whether you are
able to make your corpus public later on will depend on the rights and
restrictions of the sources used, so make sure to document that
information as well.</p>
<p>Although it sounds impressive, Big Data doesn’t always make for a
better project. The size of your corpus should depend on your project’s
needs, your storage capacity, and your computing power. A smaller
dataset with more targeted documents might actually be better at helping
you arrive at the insights that you need, depending on your use case.
Whether your corpus consists of hundreds of documents or millions, the
important thing is to create the corpus that works best for your
project.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>You will need to evaluate the suitability of data for inclusion in
your corpus and will need to take into consideration issues such as
legal/ethical restrictions and data quality among others.</li>
<li>It is important to think critically about data sources and the
context of how they were created or assembled.</li>
<li>Becoming familiar with your data and its characteristics can help
you prepare your data for analysis.</li>
<li>NULL</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-03-preprocessing"><p>Content from <a href="03-preprocessing.html">Preparing and Preprocessing Your Data</a></p>
<hr>
<p>Last updated on 2025-05-01 |

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/episodes/03-preprocessing.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can I prepare data for NLP?</li>
<li>What are tokenization, casing and lemmatization?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Load a test document into Spacy.</li>
<li>Learn preprocessing tasks.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="preparing-and-preprocessing-your-data">Preparing and Preprocessing Your Data<a class="anchor" aria-label="anchor" href="#preparing-and-preprocessing-your-data"></a>
</h2>
<hr class="half-width"></section><section><h2 class="section-heading" id="collection">Collection<a class="anchor" aria-label="anchor" href="#collection"></a>
</h2>
<hr class="half-width">
<p>The first step to preparing your data is to collect it. Whether you
use API’s to gather your material or some other method depends on your
research interests. For this workshop, we’ll use pre-gathered data.</p>
<p>During the setup instructions, we asked you to download a number of
files. These included about forty texts downloaded from <a href="https://www.gutenberg.org/" class="external-link">Project Gutenberg</a>, which will make
up our corpus of texts for our hands on lessons in this course.</p>
<p>Take a moment to orient and familiarize yourself with them:</p>
<ul>
<li>Austen
<ul>
<li>Emma - <a href="https://gutenberg.org/ebooks/158#bibrec" class="external-link">record</a>
· <a href="https://en.wikipedia.org/wiki/Emma_(novel)" class="external-link">wiki</a>
</li>
<li>Lady Susan - <a href="https://gutenberg.org/ebooks/946#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Lady_Susan" class="external-link">wiki</a>
</li>
<li>Northanger Abbey - <a href="https://gutenberg.org/ebooks/121#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Northanger_Abbey" class="external-link">wiki</a>
</li>
<li>Persuasion - <a href="https://www.gutenberg.org/ebooks/105#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Persuasion_(novel)" class="external-link">wiki</a>
</li>
<li>Pride and Prejudice - <a href="https://gutenberg.org/ebooks/1342#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Pride_and_Prejudice" class="external-link">wiki</a>
</li>
<li>Sense and Sensibility - <a href="https://gutenberg.org/ebooks/21839#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Sense_and_Sensibility" class="external-link">wiki</a>
</li>
</ul>
</li>
<li>Chesteron
<ul>
<li>The Ball and the Cross - <a href="https://gutenberg.org/ebooks/5265#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/The_Ball_and_the_Cross" class="external-link">wiki</a>
</li>
<li>The Innocence of Father Brown - <a href="https://gutenberg.org/ebooks/204#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Father_Brown" class="external-link">wiki</a>
</li>
<li>The Man Who Knew Too Much - <a href="https://gutenberg.org/ebooks/1720#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/The_Man_Who_Knew_Too_Much_(book)" class="external-link">wiki</a>
</li>
<li>The Napoleon of Notting Hill - <a href="https://gutenberg.org/ebooks/20058#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/The_Napoleon_of_Notting_Hill" class="external-link">wiki</a>
</li>
<li>The Man Who was Thursday - <a href="https://gutenberg.org/ebooks/1695#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/The_Man_Who_Was_Thursday" class="external-link">wiki</a>
</li>
<li>The Ballad of the White Horse - <a href="https://gutenberg.org/ebooks/1719#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/The_Ballad_of_the_White_Horse" class="external-link">wiki</a>
</li>
</ul>
</li>
<li>Dickens
<ul>
<li>Bleak House - <a href="https://www.gutenberg.org/ebooks/1023#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Bleak_House" class="external-link">wiki</a>
</li>
<li>A Christmas Carol - <a href="https://gutenberg.org/ebooks/24022#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/A_Christmas_Carol" class="external-link">wiki</a>
</li>
<li>David Copperfield - <a href="https://gutenberg.org/ebooks/766#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/David_Copperfield" class="external-link">wiki</a>
</li>
<li>Great Expectations - <a href="https://gutenberg.org/ebooks/1400#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Great_Expectations" class="external-link">wiki</a>
</li>
<li>Hard Times - <a href="https://gutenberg.org/ebooks/786#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Hard_Times_(novel)" class="external-link">wiki</a>
</li>
<li>Oliver Twist - <a href="https://gutenberg.org/ebooks/730#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Oliver_Twist" class="external-link">wiki</a>
</li>
<li>Our Mutual Friend - <a href="https://gutenberg.org/ebooks/883#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Our_Mutual_Friend" class="external-link">wiki</a>
</li>
<li>The Pickwick Papers - <a href="https://gutenberg.org/ebooks/580#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/The_Pickwick_Papers" class="external-link">wiki</a>
</li>
<li>A Tale of Two Cities - <a href="https://gutenberg.org/ebooks/98#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/A_Tale_of_Two_Cities" class="external-link">wiki</a>
</li>
</ul>
</li>
<li>Dumas
<ul>
<li>The Black Tulip - <a href="https://gutenberg.org/ebooks/965#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/The_Black_Tulip" class="external-link">wiki</a>
</li>
<li>The Man in the Iron Mask - <a href="https://gutenberg.org/ebooks/2759#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/The_Vicomte_of_Bragelonne:_Ten_Years_Later#Part_Three:_The_Man_in_the_Iron_Mask_(Chapters_181%E2%80%93269)" class="external-link">wiki</a>
</li>
<li>The Count of Monte Cristo - <a href="https://www.gutenberg.org/ebooks/1184#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/The_Count_of_Monte_Cristo" class="external-link">wiki</a>
</li>
<li>Ten Years Later - <a href="https://gutenberg.org/ebooks/2681#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/The_Vicomte_of_Bragelonne:_Ten_Years_Later" class="external-link">wiki</a>
</li>
<li>The Three Musketeers - <a href="https://gutenberg.org/ebooks/1257#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/The_Three_Musketeers" class="external-link">wiki</a>
</li>
<li>Twenty Years After - <a href="https://gutenberg.org/ebooks/1259#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Twenty_Years_After" class="external-link">wiki</a>
</li>
</ul>
</li>
<li>Melville
<ul>
<li>Bartleby, the Scrivener - <a href="https://gutenberg.org/ebooks/11231#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Bartleby,_the_Scrivener" class="external-link">wiki</a>
</li>
<li>The Confidence-Man - <a href="https://www.gutenberg.org/ebooks/21816" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/The_Confidence-Man" class="external-link">wiki</a>
</li>
<li>Moby Dick - <a href="https://gutenberg.org/ebooks/2701#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Moby-Dick" class="external-link">wiki</a>
</li>
<li>Omoo - <a href="https://gutenberg.org/ebooks/4045#bibrec" class="external-link">record</a>
· <a href="https://en.wikipedia.org/wiki/Omoo" class="external-link">wiki</a>
</li>
<li>The Piazza Tales - <a href="https://gutenberg.org/ebooks/15859#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/The_Piazza_Tales" class="external-link">wiki</a>
</li>
<li>Pierre - <a href="https://gutenberg.org/ebooks/34970#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Pierre;_or,_The_Ambiguities" class="external-link">wiki</a>
</li>
<li>Typee - <a href="https://gutenberg.org/ebooks/1900#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Typee" class="external-link">wiki</a>
</li>
</ul>
</li>
<li>Shakespeare
<ul>
<li>The Trajedy of Julius Caesar - <a href="https://gutenberg.org/ebooks/1120#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Julius_Caesar_(play)" class="external-link">wiki</a>
</li>
<li>The Trajedy of King Lear - <a href="https://gutenberg.org/ebooks/1532#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/King_Lear" class="external-link">wiki</a>
</li>
<li>A Midsummer Night’s Dream - <a href="https://gutenberg.org/ebooks/1514#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/A_Midsummer_Night%27s_Dream" class="external-link">wiki</a>
</li>
<li>Much Ado about Nothing - <a href="https://gutenberg.org/ebooks/1519#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Much_Ado_About_Nothing" class="external-link">wiki</a>
</li>
<li>Othello, the Moor of Venice - <a href="https://www.gutenberg.org/ebooks/1531" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Othello" class="external-link">wiki</a>
</li>
<li>Romeo and Juliet - <a href="https://gutenberg.org/ebooks/1513#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Romeo_and_Juliet" class="external-link">wiki</a>
</li>
<li>Twelfth Night - <a href="https://gutenberg.org/ebooks/1526#bibrec" class="external-link">record</a> · <a href="https://en.wikipedia.org/wiki/Twelfth_Night" class="external-link">wiki</a>
</li>
</ul>
</li>
</ul>
<p>While a full-sized corpus can include thousands of texts, these
forty-odd texts will be enough for our illustrative purposes.</p>
</section><section><h2 class="section-heading" id="loading-data-into-python">Loading Data into Python<a class="anchor" aria-label="anchor" href="#loading-data-into-python"></a>
</h2>
<hr class="half-width">
<p>We’ll start by mounting our Google Drive so that Colab can read the
helper functions. We’ll also go through how many of these functions are
written in this lesson.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># Run this cell to mount your Google Drive.</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>drive.mount(<span class="st">'/content/drive'</span>)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># Show existing colab notebooks and helpers.py file</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> os <span class="im">import</span> listdir</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>wksp_dir <span class="op">=</span> <span class="st">'/content/drive/My Drive/Colab Notebooks/text-analysis/code'</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>listdir(wksp_dir)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co"># Add folder to colab's path so we can import the helper functions</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>sys.path.insert(<span class="dv">0</span>, wksp_dir)</span></code></pre>
</div>
<p>Next, we have a corpus of text files we want to analyze. Let’s create
a method to list those files. To make this method more flexible, we will
also use <code>glob</code> to allow us to put in regular expressions so
we can filter the files if so desired. <code>glob</code> is a tool for
listing files in a directory whose file names match some pattern, like
all files ending in <code>*.txt</code>.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="op">!</span>pip install pathlib parse</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">import</span> glob</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">def</span> create_file_list(directory, filter_str<span class="op">=</span><span class="st">'*'</span>):</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>  files <span class="op">=</span> Path(directory).glob(filter_str)</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>  files_to_analyze <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="bu">str</span>, files))</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>  <span class="cf">return</span> files_to_analyze</span></code></pre>
</div>
<p>Alternatively, we can load this function from the
<code>helpers.py</code> file we provided for learners in this
course:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">from</span> helpers <span class="im">import</span> create_file_list</span></code></pre>
</div>
<p>Either way, now we can use that function to list the books in our
corpus:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>corpus_dir <span class="op">=</span> <span class="st">'/content/drive/My Drive/Colab Notebooks/text-analysis/data/books'</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>corpus_file_list <span class="op">=</span> create_file_list(corpus_dir)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="bu">print</span>(corpus_file_list)</span></code></pre>
</div>
<p>We will use the full corpus later, but it might be useful to filter
to just a few specific files. For example, if I want just documents
written by Austen, I can filter on part of the file path name:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>austen_list <span class="op">=</span> create_file_list(corpus_dir, <span class="st">'austen*'</span>)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="bu">print</span>(austen_list)</span></code></pre>
</div>
<p>Let’s take a closer look at Emma. We are looking at the first full
sentence, which begins with character 50 and ends at character 290.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>preview_len <span class="op">=</span> <span class="dv">290</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>emmapath <span class="op">=</span> create_file_list(corpus_dir, <span class="st">'austen-emma*'</span>)[<span class="dv">0</span>]</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="bu">print</span>(emmapath)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>sentence <span class="op">=</span> <span class="st">""</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(emmapath, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>  sentence <span class="op">=</span> f.read(preview_len)[<span class="dv">50</span>:preview_len]</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a><span class="bu">print</span>(sentence)</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="preprocessing">Preprocessing<a class="anchor" aria-label="anchor" href="#preprocessing"></a>
</h2>
<hr class="half-width">
<p>Currently, our data is still in a format that is best for humans to
read. Humans, without having to think too consciously about it,
understand how words and sentences group up and divide into discrete
units of meaning. We also understand that the words <em>run</em>,
<em>ran</em>, and <em>running</em> are just different grammatical forms
of the same underlying concept. Finally, not only do we understand how
punctuation affects the meaning of a text, we also can make sense of
texts that have odd amounts or odd placements of punctuation.</p>
<p>For example, Darcie Wilder’s <a href="https://www.mtv.com/news/1vw892/read-an-excerpt-of-darcie-wilders-literally-show-me-a-healthy-person" class="external-link"><em>literally
show me a healthy person</em></a> has very little capitalization or
punctuation:</p>
<blockquote>
<p>in the unauthorized biography of britney spears she says her advice
is to lift 1 lb weights and always sing in elevators every time i left
to skateboard in the schoolyard i would sing in the elevator i would
sing britney spears really loud and once the door opened and there were
so many people they heard everything so i never sang again</p>
</blockquote>
<p>Across the texts in our corpus, our authors write with different
styles, preferring different dictions, punctuation, and so on.</p>
<p>To prepare our data to be more uniformly understood by our NLP
models, we need to (a) break it into smaller units, (b) replace words
with their roots, and (c) remove unwanted common or unhelpful words and
punctuation. These steps encompass the preprocessing stage of the
interpretive loop.</p>
<figure><img src="fig/01-Interpretive_Loop.JPG" alt="The Interpretive Loop" class="figure mx-auto d-block"></figure><div class="section level3">
<h3 id="tokenization">Tokenization<a class="anchor" aria-label="anchor" href="#tokenization"></a>
</h3>
<p>Tokenization is the process of breaking down texts (strings of
characters) into words, groups of words, and sentences. A string of
characters needs to be understood by a program as smaller units so that
it can be embedded. These are called <strong>tokens</strong>.</p>
<p>While our tokens will be single words for now, this will not always
be the case. Different models have different ways of tokenizing strings.
The strings may be broken down into multiple word tokens, single word
tokens, or even components of words like letters or morphology.
Punctuation may or may not be included.</p>
<p>We will be using a tokenizer that breaks documents into single words
for this lesson.</p>
<p>Let’s load our tokenizer and test it with the first sentence of
Emma:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="im">import</span> en_core_web_sm</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>spacyt <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span></code></pre>
</div>
<p>We will define a tokenizer method with the text editor. Keep this
open so we can add to it throughout the lesson.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">class</span> Our_Tokenizer:</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>    <span class="co">#import spacy tokenizer/language model</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>    <span class="va">self</span>.nlp <span class="op">=</span> en_core_web_sm.load()</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>    <span class="va">self</span>.nlp.max_length <span class="op">=</span> <span class="dv">4500000</span> <span class="co"># increase max number of characters that spacy can process (default = 1,000,000)</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, document):</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>    tokens <span class="op">=</span> <span class="va">self</span>.nlp(document)</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>    <span class="cf">return</span> tokens</span></code></pre>
</div>
<p>This will load spacy and its preprocessing pipeline for English.
<strong>Pipelines</strong> are a series of interrelated tasks, where the
output of one task is used as an input for another. Different languages
may have different rulesets, and therefore require different
preprocessing pipelines. Running the document we created through the NLP
model we loaded performs a variety of tasks for us. Let’s look at these
in greater detail.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>tokens <span class="op">=</span> spacyt(sentence)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> tokens:</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a> <span class="bu">print</span>(t.text)</span></code></pre>
</div>
<p>The single sentence has been broken down into a set of tokens. Tokens
in spacy aren’t just strings: They’re python objects with a variety of
attributes. Full documentation for these attributes can be found at: <a href="https://spacy.io/api/token" class="external-link uri">https://spacy.io/api/token</a></p>
</div>
<div class="section level3">
<h3 id="stems-and-lemmas">Stems and Lemmas<a class="anchor" aria-label="anchor" href="#stems-and-lemmas"></a>
</h3>
<p>Think about similar words, such as <em>running</em>, <em>ran</em>,
and <em>runs</em>. All of these words have a similar root, but a
computer does not know this. Without preprocessing, each of these words
would be a new token.</p>
<p>Stemming and Lemmatization are used to group together words that are
similar or forms of the same word.</p>
<p>Stemming is removing the conjugation and pluralized endings for
words. For example, words like <em>digitization</em>, and
<em>digitizing</em> might chopped down to <em>digitiz</em>.</p>
<p>Lemmatization is the more sophisticated of the two, and looks for the
linguistic base of a word. Lemmatization can group words that mean the
same thing but may not be grouped through simple stemming, such as
irregular verbs like <em>bring</em> and <em>brought</em>.</p>
<p>Similarly, in naive tokenization, capital letters are considered
different from non-capital letters, meaning that capitalized versions of
words are considered different from non-capitalized versions. Converting
all words to lower case ensures that capitalized and non-capitalized
versions of words are considered the same.</p>
<p>These steps are taken to reduce the complexities of our NLP models
and to allow us to train them from less data.</p>
<p>When we tokenized the first sentence of Emma above, Spacy also
created a lemmatized version of itt. Let’s try accessing this by typing
the following:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> tokens:</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>  <span class="bu">print</span>(t.lemma)</span></code></pre>
</div>
<p>Spacy stores words by an ID number, and not as a full string, to save
space in memory. Many spacy functions will return numbers and not words
as you might expect. Fortunately, adding an underscore for spacy will
return text representations instead. We will also add in the lower case
function so that all words are lower case.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> tokens:</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a> <span class="bu">print</span>(<span class="bu">str</span>.lower(t.lemma_))</span></code></pre>
</div>
<p>Notice how words like <em>best</em> and <em>her</em> have been
changed to their root words like <em>good</em> and <em>she</em>. Let’s
change our tokenizer to save the lower cased, lemmatized versions of
words instead of the original words.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="kw">class</span> Our_Tokenizer:</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>    <span class="co"># import spacy tokenizer/language model</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>    <span class="va">self</span>.nlp <span class="op">=</span> en_core_web_sm.load()</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>    <span class="va">self</span>.nlp.max_length <span class="op">=</span> <span class="dv">4500000</span> <span class="co"># increase max number of characters that spacy can process (default = 1,000,000)</span></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, document):</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>    tokens <span class="op">=</span> <span class="va">self</span>.nlp(document)</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>    simplified_tokens <span class="op">=</span> [<span class="bu">str</span>.lower(token.lemma_) <span class="cf">for</span> token <span class="kw">in</span> tokens]</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>    <span class="cf">return</span> simplified_tokens</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="stop-words-and-punctuation">Stop-Words and Punctuation<a class="anchor" aria-label="anchor" href="#stop-words-and-punctuation"></a>
</h3>
<p>Stop-words are common words that are often filtered out for more
efficient natural language data processing. Words such as <em>the</em>
and <em>and</em> don’t necessarily tell us a lot about a document’s
content and are often removed in simpler models. Stop lists (groups of
stop words) are curated by sorting terms by their collection frequency,
or the total number of times that they appear in a document or corpus.
Punctuation also is something we are not interested in, at least not
until we get to more complex models. Many open-source software packages
for language processing, such as Spacy, include stop lists. Let’s look
at Spacy’s stopword list.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="im">from</span> spacy.lang.en.stop_words <span class="im">import</span> STOP_WORDS</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="bu">print</span>(STOP_WORDS)</span></code></pre>
</div>
<p>It’s possible to add and remove words as well, for example,
<em>zebra</em>:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="co"># remember, we need to tokenize things in order for our model to analyze them.</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>z <span class="op">=</span> spacyt(<span class="st">"zebra"</span>)[<span class="dv">0</span>]</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="bu">print</span>(z.is_stop) <span class="co"># False</span></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a><span class="co"># add zebra to our stopword list</span></span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>STOP_WORDS.add(<span class="st">"zebra"</span>)</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>spacyt <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>z <span class="op">=</span> spacyt(<span class="st">"zebra"</span>)[<span class="dv">0</span>]</span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a><span class="bu">print</span>(z.is_stop) <span class="co"># True</span></span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a><span class="co"># remove zebra from our list.</span></span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a>STOP_WORDS.remove(<span class="st">"zebra"</span>)</span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a>spacyt <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb16-14"><a href="#cb16-14" tabindex="-1"></a>z <span class="op">=</span> spacyt(<span class="st">"zebra"</span>)[<span class="dv">0</span>]</span>
<span id="cb16-15"><a href="#cb16-15" tabindex="-1"></a><span class="bu">print</span>(z.is_stop) <span class="co"># False</span></span></code></pre>
</div>
<p>Let’s add “Emma” to our list of stopwords, since knowing that the
name “Emma” is often in Jane Austin does not tell us anything
interesting.</p>
<p>This will only adjust the stopwords for the current session, but it
is possible to save them if desired. More information about how to do
this can be found in the Spacy documentation. You might use this
stopword list to filter words from documents using spacy, or just by
manually iterating through it like a list.</p>
<p>Let’s see what our example looks like without stopwords and
punctuation:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="co"># add emma to our stopword list</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>STOP_WORDS.add(<span class="st">"emma"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>spacyt <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a><span class="co"># retokenize our sentence</span></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>tokens <span class="op">=</span> spacyt(sentence)</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a><span class="cf">for</span> token <span class="kw">in</span> tokens:</span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a>  <span class="cf">if</span> <span class="kw">not</span> token.is_stop <span class="kw">and</span> <span class="kw">not</span> token.is_punct:</span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">str</span>.lower(token.lemma_))</span></code></pre>
</div>
<p>Notice that because we added <em>emma</em> to our stopwords, she is
not in our preprocessed sentence any more. Other stopwords are also
missing such as numbers.</p>
<p>Let’s filter out stopwords and punctuation from our custom tokenizer
now as well:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="kw">class</span> Our_Tokenizer:</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>    <span class="co"># import spacy tokenizer/language model</span></span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>    <span class="va">self</span>.nlp <span class="op">=</span> en_core_web_sm.load()</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>    <span class="va">self</span>.nlp.max_length <span class="op">=</span> <span class="dv">4500000</span> <span class="co"># increase max number of characters that spacy can process (default = 1,000,000)</span></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, document):</span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>    tokens <span class="op">=</span> <span class="va">self</span>.nlp(document)</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>    simplified_tokens <span class="op">=</span> []    </span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a>    <span class="cf">for</span> token <span class="kw">in</span> tokens:</span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> token.is_stop <span class="kw">and</span> <span class="kw">not</span> token.is_punct:</span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>            simplified_tokens.append(<span class="bu">str</span>.lower(token.lemma_))</span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a>    <span class="cf">return</span> simplified_tokens</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="parts-of-speech">Parts of Speech<a class="anchor" aria-label="anchor" href="#parts-of-speech"></a>
</h3>
<p>While we can manually add Emma to our stopword list, it may occur to
you that novels are filled with characters with unique and unpredictable
names. We’ve already missed the word “Woodhouse” from our list. Creating
an enumerated list of all of the possible character names seems
impossible.</p>
<p>One way we might address this problem is by using <strong>Parts of
speech (POS)</strong> tagging. POS are things such as nouns, verbs, and
adjectives. POS tags often prove useful, so some tokenizers also have
built in POS tagging done. Spacy is one such library. These tags are not
100% accurate, but they are a great place to start. Spacy’s POS tags can
be used by accessing the <code>pos_</code> method for each token.</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="cf">for</span> token <span class="kw">in</span> tokens:</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>  <span class="cf">if</span> token.is_stop <span class="op">==</span> <span class="va">False</span> <span class="kw">and</span> token.is_punct <span class="op">==</span> <span class="va">False</span>:</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">str</span>.lower(token.lemma_)<span class="op">+</span><span class="st">" "</span><span class="op">+</span>token.pos_)</span></code></pre>
</div>
<p>Because our dataset is relatively small, we may find that character
names and places weigh very heavily in our early models. We also have a
number of blank or white space tokens, which we will also want to
remove.</p>
<p>We will finish our special tokenizer by removing punctuation and
proper nouns from our documents:</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="kw">class</span> Our_Tokenizer:</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>    <span class="co"># import spacy tokenizer/language model</span></span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>    <span class="va">self</span>.nlp <span class="op">=</span> en_core_web_sm.load()</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a>    <span class="va">self</span>.nlp.max_length <span class="op">=</span> <span class="dv">4500000</span> <span class="co"># increase max number of characters that spacy can process (default = 1,000,000)</span></span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, document):</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a>    tokens <span class="op">=</span> <span class="va">self</span>.nlp(document)</span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a>    simplified_tokens <span class="op">=</span> [</span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a>      <span class="co">#our helper function expects spacy tokens. It will take care of making them lowercase lemmas.</span></span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a>      token <span class="cf">for</span> token <span class="kw">in</span> tokens</span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a>      <span class="cf">if</span> <span class="kw">not</span> token.is_stop</span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a>      <span class="kw">and</span> <span class="kw">not</span> token.is_punct</span>
<span id="cb20-13"><a href="#cb20-13" tabindex="-1"></a>      <span class="kw">and</span> token.pos_ <span class="op">!=</span> <span class="st">"PROPN"</span></span>
<span id="cb20-14"><a href="#cb20-14" tabindex="-1"></a>    ]</span>
<span id="cb20-15"><a href="#cb20-15" tabindex="-1"></a>    <span class="cf">return</span> simplified_tokens</span></code></pre>
</div>
<p>Alternative, instead of “blacklisting” all of the parts of speech we
don’t want to include, we can “whitelist” just the few that we want,
based on what they information they might contribute to the meaning of a
text:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="kw">class</span> Our_Tokenizer:</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a>    <span class="co"># import spacy tokenizer/language model</span></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>    <span class="va">self</span>.nlp <span class="op">=</span> en_core_web_sm.load()</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a>    <span class="va">self</span>.nlp.max_length <span class="op">=</span> <span class="dv">4500000</span> <span class="co"># increase max number of characters that spacy can process (default = 1,000,000)</span></span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, document):</span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a>    tokens <span class="op">=</span> <span class="va">self</span>.nlp(document)</span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a>    simplified_tokens <span class="op">=</span> [</span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>      <span class="co">#our helper function expects spacy tokens. It will take care of making them lowercase lemmas.</span></span>
<span id="cb21-10"><a href="#cb21-10" tabindex="-1"></a>      token <span class="cf">for</span> token <span class="kw">in</span> tokens</span>
<span id="cb21-11"><a href="#cb21-11" tabindex="-1"></a>      <span class="cf">if</span> <span class="kw">not</span> token.is_stop</span>
<span id="cb21-12"><a href="#cb21-12" tabindex="-1"></a>      <span class="kw">and</span> <span class="kw">not</span> token.is_punct</span>
<span id="cb21-13"><a href="#cb21-13" tabindex="-1"></a>      <span class="kw">and</span> token.pos_ <span class="kw">in</span> {<span class="st">"ADJ"</span>, <span class="st">"ADV"</span>, <span class="st">"INTJ"</span>, <span class="st">"NOUN"</span>, <span class="st">"VERB"</span>}</span>
<span id="cb21-14"><a href="#cb21-14" tabindex="-1"></a>    ]</span>
<span id="cb21-15"><a href="#cb21-15" tabindex="-1"></a>    <span class="cf">return</span> simplified_tokens</span></code></pre>
</div>
<p>Either way, let’s test our custom tokenizer on this selection of text
to see how it works.</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>tokenizer <span class="op">=</span> Our_Tokenizer()</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>tokens <span class="op">=</span> tokenizer(sentence)</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a><span class="bu">print</span>(tokens)</span></code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="putting-it-all-together">Putting it All Together<a class="anchor" aria-label="anchor" href="#putting-it-all-together"></a>
</h2>
<hr class="half-width">
<p>Now that we’ve built a tokenizer we’re happy with, lets use it to
create lemmatized versions of all the books in our corpus.</p>
<p>That is, we want to turn this:</p>
<p>“Emma Woodhouse, handsome, clever, and rich, with a comfortable home
and happy disposition, seemed to unite some of the best blessings of
existence; and had lived nearly twenty-one years in the world with very
little to distress or vex her.”</p>
<p>into this:</p>
<p>“handsome clever rich comfortable home happy disposition seem unite
good blessing existence live nearly year world very little distress
vex”</p>
<p>To help make this <em>relatively</em> quick for all the text in all
our books, we’ll use a helper function we prepared for learners to use
our tokenizer, do the casing and lemmatization we discussed earlier, and
write the results to a file:</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="im">from</span> helpers <span class="im">import</span> lemmatize_files</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a><span class="co"># SKIP THIS - it takes too long to run during a live class. Lemma files are preprocessed for you and saved to data/book_lemmas</span></span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a><span class="co">#lemma_file_list = lemmatize_files(tokenizer, corpus_file_list)</span></span></code></pre>
</div>
<p>This process may take several minutes to run. If you don’t want to
wait, you can stop the cell running and use our pre-baked solution
(lemma files) found in data/book_lemmas. The next section will walk you
through both options.</p>
</section><section><h2 class="section-heading" id="creating-dataframe-to-work-with-files-and-lemmas-easily">Creating dataframe to work with files and lemmas easily<a class="anchor" aria-label="anchor" href="#creating-dataframe-to-work-with-files-and-lemmas-easily"></a>
</h2>
<hr class="half-width">
<p>Let’s save a dataframe / spreadsheet that lists all our authors,
books, and associated filenames, both the original and lemmatized
copies.</p>
<p>We’ll use another helper we prepared to make this easy:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="im">from</span> helpers <span class="im">import</span> parse_into_dataframe</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>pattern <span class="op">=</span> <span class="st">"/content/drive/My Drive/Colab Notebooks/text-analysis/data/books/</span><span class="sc">{author}</span><span class="st">-</span><span class="sc">{title}</span><span class="st">.txt"</span></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a>data <span class="op">=</span> parse_into_dataframe(pattern, corpus_file_list)</span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>data.head()</span></code></pre>
</div>
<p>Next, we can add the lemma files to the dataframe. If you ran the
lemmatize_files() function above successfully, you can use:</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>data[<span class="st">"Lemma_File"</span>] <span class="op">=</span> lemma_file_list</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a>data.head()</span></code></pre>
</div>
<p>Otherwise, we can add the “pre-baked” lemmas to our dataframe
using</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="kw">def</span> get_lemma_path(file_path):</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>    <span class="co"># Convert to Path object for easier manipulation</span></span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>    p <span class="op">=</span> Path(file_path)</span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>    <span class="co"># Extract the filename like 'austen-sense.txt'</span></span>
<span id="cb26-5"><a href="#cb26-5" tabindex="-1"></a>    file_name <span class="op">=</span> p.name</span>
<span id="cb26-6"><a href="#cb26-6" tabindex="-1"></a>    <span class="co"># Create new path with 'book_lemmas' instead of 'books' and add .lemmas</span></span>
<span id="cb26-7"><a href="#cb26-7" tabindex="-1"></a>    lemma_name <span class="op">=</span> file_name <span class="op">+</span> <span class="st">".lemmas"</span></span>
<span id="cb26-8"><a href="#cb26-8" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">str</span>(p.parent.parent <span class="op">/</span> <span class="st">"book_lemmas"</span> <span class="op">/</span> lemma_name)</span>
<span id="cb26-9"><a href="#cb26-9" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" tabindex="-1"></a><span class="co"># Add new column</span></span>
<span id="cb26-11"><a href="#cb26-11" tabindex="-1"></a>data[<span class="st">"Lemma_File"</span>] <span class="op">=</span> data[<span class="st">"File"</span>].<span class="bu">apply</span>(get_lemma_path)</span>
<span id="cb26-12"><a href="#cb26-12" tabindex="-1"></a>data.head()</span></code></pre>
</div>
<p>Finally, we’ll save this table to a file:</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a>data.to_csv(<span class="st">"/content/drive/My Drive/Colab Notebooks/text-analysis/data/data.csv"</span>, index<span class="op">=</span><span class="va">False</span>)</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="outro-and-conclusion">Outro and Conclusion<a class="anchor" aria-label="anchor" href="#outro-and-conclusion"></a>
</h2>
<hr class="half-width">
<p>This lesson has covered a number of preprocessing steps. We created a
list of our files in our corpus, which we can use in future lessons. We
customized a tokenizer from Spacy, to better suit the needs of our
corpus, which we can also use moving forward.</p>
<p>Next lesson, we will start talking about the concepts behind our
model.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Tokenization breaks strings into smaller parts for analysis.</li>
<li>Casing removes capital letters.</li>
<li>Stopwords are common words that do not contain much useful
information.</li>
<li>Lemmatization reduces words to their root form.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-04-vectorSpace"><p>Content from <a href="04-vectorSpace.html">Vector Space and Distance</a></p>
<hr>
<p>Last updated on 2025-05-01 |

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/episodes/04-vectorSpace.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can we model documents effectively?</li>
<li>How can we measure similarity between documents?</li>
<li>What’s the difference between cosine similarity and distance?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Visualize vector space in a 2D model.</li>
<li>Learn about embeddings.</li>
<li>Learn about cosine similarity and distance.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="vector-space">Vector Space<a class="anchor" aria-label="anchor" href="#vector-space"></a>
</h2>
<hr class="half-width">
<p>Now that we’ve preprocessed our data, let’s move to the next step of
the interpretative loop: <em>generating a text embedding</em>.</p>
<figure><img src="fig/01-Interpretive_Loop.JPG" alt="The Interpretive Loop" class="figure mx-auto d-block"></figure><p>Many NLP models make use of a concept called Vector Space. The
concept works like this:</p>
<ol style="list-style-type: decimal">
<li>We create <strong>embeddings</strong>, or mathematical surrogates,
of words and documents in vector space. These embeddings can be
represented as sets of coordinates in multidimensional space, or as
multi-dimensional matrices.</li>
<li>These embeddings should be based on some sort of <strong>feature
extraction</strong>, meaning that meaningful features from our original
documents are somehow represented in our embedding. This will make it so
that relationships between embeddings in this vector space will
correspond to relationships in the actual documents.</li>
</ol></section><section><h2 class="section-heading" id="bags-of-words">Bags of Words<a class="anchor" aria-label="anchor" href="#bags-of-words"></a>
</h2>
<hr class="half-width">
<p>In the models we’ll look at today, we have a <strong>“bag of
words”</strong> assumption as well. We will not consider the placement
of words in sentences, their context, or their conjugation into
different forms (run vs ran), not until later in this course.</p>
<p>A “bag of words” model is like putting all words from a sentence in a
bag and just being concerned with how many of each word you have, not
their order or context.</p>
<div class="section level3">
<h3 id="worked-example-bag-of-words">Worked Example: Bag of Words<a class="anchor" aria-label="anchor" href="#worked-example-bag-of-words"></a>
</h3>
<p>Let’s suppose we want to model a small, simple set of toy documents.
Our entire corpus of documents will only have two words, <em>to</em> and
<em>be</em>. We have four documents, A, B, C and D:</p>
<ul>
<li>A: be be be be be be be be be be to</li>
<li>B: to be to be to be to be to be to be to be to be</li>
<li>C: to to be be</li>
<li>D: to be to be</li>
</ul>
<p>We will start by embedding words using a “one hot” embedding
algorithm. Each document is a new row in our table. Every time word ‘to’
shows up in a document, we add one to our value for the ‘to’ dimension
for that row, and zero to every other dimension. Every time ‘be’ shows
up in our document, we will add one to our value for the ‘be’ dimension
for that row, and zero to every other dimension.</p>
<p>How does this corpus look in vector space? We can display our model
using a <strong>document-term matrix</strong>, which looks like the
following:</p>
<table class="table">
<thead><tr class="header">
<th>Document</th>
<th>to</th>
<th>be</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Document A</td>
<td>1</td>
<td>10</td>
</tr>
<tr class="even">
<td>Document B</td>
<td>8</td>
<td>8</td>
</tr>
<tr class="odd">
<td>Document C</td>
<td>2</td>
<td>2</td>
</tr>
<tr class="even">
<td>Document D</td>
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>Notice that documents C and D are represented exactly the same. This
is unavoidable right now because of our “bag of words” assumption, but
much later on we will try to represent positions of words in our models
as well. Let’s visualize this using Python.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>corpus <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">10</span>],[<span class="dv">8</span>,<span class="dv">8</span>],[<span class="dv">2</span>,<span class="dv">2</span>],[<span class="dv">2</span>,<span class="dv">2</span>]])</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="bu">print</span>(corpus)</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="graphing-our-model">Graphing our model<a class="anchor" aria-label="anchor" href="#graphing-our-model"></a>
</h3>
<p>We don’t just have to think of our words as columns. We can also
think of them as dimensions, and the values as coordinates for each
document.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># matplotlib expects a list of values by column, not by row.</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="co"># We can simply turn our table on its edge so rows become columns and vice versa.</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>corpusT <span class="op">=</span> np.transpose(corpus)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="bu">print</span>(corpusT)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>X <span class="op">=</span> corpusT[<span class="dv">0</span>]</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>Y <span class="op">=</span> corpusT[<span class="dv">1</span>]</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="co"># define some colors for each point. Since points A and B are the same, we'll have them as the same color.</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>mycolors <span class="op">=</span> [<span class="st">'r'</span>,<span class="st">'g'</span>,<span class="st">'b'</span>,<span class="st">'b'</span>]</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co"># display our visualization</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>plt.scatter(X,Y, c<span class="op">=</span>mycolors)</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="dv">12</span>)</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">12</span>)</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/02-plot-points.png" alt="png" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="distance-and-similarity">Distance and Similarity<a class="anchor" aria-label="anchor" href="#distance-and-similarity"></a>
</h3>
<p>What can we do with this simple model? At the heart of many research
tasks is <strong>distance</strong> or <strong>similarity</strong>, in
some sense. When we classify or search for documents, we are asking for
documents that are “close to” some known examples or search terms. When
we explore the topics in our documents, we are asking for a small set of
concepts that capture and help explain as much as the ways our documents
might differ from one another. And so on.</p>
<p>There are two measures of distance/similarity we’ll consider here:
<strong>Euclidean distance</strong> and <strong>cosine
similarity</strong>.</p>
<div class="section level4">
<h4 id="euclidean-distance">Euclidean Distance<a class="anchor" aria-label="anchor" href="#euclidean-distance"></a>
</h4>
<p>The Euclidian distance formula makes use of the Pythagorean theorem,
where <span class="math inline">\(a^2 + b^2 = c^2\)</span>. We can draw
a triangle between two points, and calculate the hypotenuse to find the
distance. This distance formula works in two dimensions, but can also be
generalized over as many dimensions as we want. Let’s use distance to
compare A to B, C and D. We’ll say the closer two points are, the
smaller their distance, so the more similar they are.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> euclidean_distances <span class="im">as</span> dist</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="co">#What is closest to document D?</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>D <span class="op">=</span> [corpus[<span class="dv">3</span>]]</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="bu">print</span>(D)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">TXT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode txt" tabindex="0"><code class="sourceCode default"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>[array([2, 2])]</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>dist(corpus, D)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">TXT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode txt" tabindex="0"><code class="sourceCode default"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>array([[8.06225775],</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>       [8.48528137],</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>       [0.        ],</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>       [0.        ]])</span></code></pre>
</div>
<p>Distance may seem like a decent metric at first. Certainly, it makes
sense that document D has zero distance from itself. C and D are also
similar, which makes sense given our bag of words assumption. But take a
closer look at documents B and D. Document B is just document D copy and
pasted 4 times! How can it be less similar to document D than document
B?</p>
<p>Distance is highly sensitive to document length. Because document A
is shorter than document B, it is closer to document D. While distance
may be an intuitive measure of similarity, it is actually highly
dependent on document length.</p>
<p>We need a different metric that will better represent similarity.
This is where vectors come in. Vectors are geometric objects with both
length and direction. They can be thought of as a ray or an arrow
pointing from one point to another.</p>
<p>Vectors can be added, subtracted, or multiplied together, just like
regular numbers can. Our model will consider documents as vectors
instead of points, going from the origin at <span class="math inline">\((0,0)\)</span> to each document. Let’s visualize
this.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># we need the point of origin in order to draw a vector. Numpy has a function to create an array full of zeroes.</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>origin <span class="op">=</span> np.zeros([<span class="dv">1</span>,<span class="dv">4</span>])</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="bu">print</span>(origin)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># draw our vectors</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>plt.quiver(origin, origin, X, Y, color<span class="op">=</span>mycolors, angles<span class="op">=</span><span class="st">'xy'</span>, scale_units<span class="op">=</span><span class="st">'xy'</span>, scale<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="dv">12</span>)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">12</span>)</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<p>Document A and document D are headed in exactly the same direction,
which matches our intution that both documents are in some way similar
to each other, even though they differ in length.</p>
</div>
<div class="section level4">
<h4 id="cosine-similarity">Cosine Similarity<a class="anchor" aria-label="anchor" href="#cosine-similarity"></a>
</h4>
<p><strong>Cosine Similarity</strong> is a metric which is only
concerned with the direction of the vector, not its length. This means
the length of a document will no longer factor into our similarity
metric. The more similar two vectors are in direction, the closer the
cosine similarity score gets to 1. The more orthogonal two vectors get
(the more at a right angle they are), the closer it gets to 0. And as
the more they point in opposite directions, the closer it gets to
-1.</p>
<p>You can think of cosine similarity between vectors as signposts aimed
out into multidimensional space. Two similar documents going in the same
direction have a high cosine similarity, even if one of them is much
further away in that direction.</p>
<p>Now that we know what cosine similarity is, how does this metric
compare our documents?</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity <span class="im">as</span> cs</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>cs(corpus, D)</span></code></pre>
</div>
<p>Both A and D are considered similar by this metric. Cosine similarity
is used by many models as a measure of similarity between documents and
words.</p>
</div>
</div>
<div class="section level3">
<h3 id="generalizing-over-more-dimensions">Generalizing over more dimensions<a class="anchor" aria-label="anchor" href="#generalizing-over-more-dimensions"></a>
</h3>
<p>If we want to add another word to our model, we can add another
dimension, which we can represent as another column in our table. Let’s
add more documents with new words in them.</p>
<ul>
<li>E: be or not be</li>
<li>F: to be or not to be</li>
</ul>
<table class="table">
<thead><tr class="header">
<th>Document</th>
<th>to</th>
<th>be</th>
<th>or</th>
<th>not</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Document A</td>
<td>1</td>
<td>10</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>Document B</td>
<td>8</td>
<td>8</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>Document C</td>
<td>2</td>
<td>2</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>Document D</td>
<td>2</td>
<td>2</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>Document E</td>
<td>0</td>
<td>2</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>Document F</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>We can keep adding dimensions for however many words we want to add.
It’s easy to imagine vector space with two or three dimensions, but
visualizing this mentally will rapidly become downright impossible as we
add more and more words. Vocabularies for natural languages can easily
reach tens of thousands of words.</p>
<p>Keep in mind, it’s not necessary to visualize how a high dimensional
vector space looks. These relationships and formulae work over an
arbitrary number of dimensions. Our methods for how to measure
similarity will carry over, even if drawing a graph is no longer
possible.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># add two new dimensions to our corpus</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>corpus <span class="op">=</span> np.hstack((corpus, np.zeros((<span class="dv">4</span>,<span class="dv">2</span>))))</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="bu">print</span>(corpus)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>E <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>]])</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>F <span class="op">=</span> np.array([[<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>]])</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="co">#add document E to our corpus</span></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>corpus <span class="op">=</span> np.vstack((corpus, E))</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a><span class="bu">print</span>(corpus)</span></code></pre>
</div>
<p>What do you think the most similar document is to document F?</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>cs(corpus, F)</span></code></pre>
</div>
<p>This new document seems most similar to the documents B,C and D.</p>
<p>This principle of using vector space will hold up over an arbitrary
number of dimensions, and therefore over a vocabulary of arbitrary
size.</p>
<p>This is the essence of vector space modeling: documents are embedded
as vectors in very high dimensional space.</p>
<p>How we define these dimensions and the methods for feature extraction
may change and become more complex, but the essential idea remains the
same.</p>
<p>Next, we will discuss TF-IDF, which balances the above “bag of words”
approach against the fact that some words are more or less interesting:
<em>whale</em> conveys more useful information than <em>the</em>, for
example.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>We model documents by plotting them in high dimensional space.</li>
<li>Distance is highly dependent on document length.</li>
<li>Documents are modeled as vectors so cosine similarity can be used as
a similarity metric.</li>
</ul>
</div>
</div>
</div>
</div>
</section></section><section id="aio-05-tf-idf-documentEmbeddings"><p>Content from <a href="05-tf-idf-documentEmbeddings.html">Document Embeddings and TF-IDF</a></p>
<hr>
<p>Last updated on 2025-05-01 |

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/episodes/05-tf-idf-documentEmbeddings.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is a document embedding?</li>
<li>What is TF-IDF?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Produce TF-IDF matrix on a corpus</li>
<li>Understand how TF-IDF relates to rare/common words</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>The method of using word counts is just one way we might embed a
document in vector space.<br>
Let’s talk about more complex and representational ways of constructing
document embeddings.<br>
To start, imagine we want to represent each word in our model
individually, instead of considering an entire document. How individual
words are represented in vector space is something called “word
embeddings” and they are an important concept in NLP.</p>
<section><h2 class="section-heading" id="one-hot-encoding-limitations">One hot encoding: Limitations<a class="anchor" aria-label="anchor" href="#one-hot-encoding-limitations"></a>
</h2>
<hr class="half-width">
<p>How would we make word embeddings for a simple document such as “Feed
the duck”?</p>
<p>Let’s imagine we have a vector space with a million different words
in our corpus, and we are just looking at part of the vector space
below.</p>
<table class="table">
<colgroup>
<col width="12%">
<col width="11%">
<col width="9%">
<col width="8%">
<col width="9%">
<col width="14%">
<col width="9%">
<col width="8%">
<col width="8%">
<col width="8%">
</colgroup>
<thead><tr class="header">
<th></th>
<th>dodge</th>
<th>duck</th>
<th>…</th>
<th>farm</th>
<th>feather</th>
<th>feed</th>
<th>…</th>
<th>tan</th>
<th>the</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>feed</td>
<td>0</td>
<td>0</td>
<td></td>
<td>0</td>
<td>0</td>
<td>1</td>
<td></td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>the</td>
<td>0</td>
<td>0</td>
<td></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td></td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>duck</td>
<td>0</td>
<td>1</td>
<td></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td></td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>-—–</td>
<td>-——</td>
<td>-—–</td>
<td>-—-</td>
<td>-—–</td>
<td>-——–</td>
<td>-—–</td>
<td>-—-</td>
<td>-—-</td>
<td>-—-</td>
</tr>
<tr class="odd">
<td>Document</td>
<td>0</td>
<td>1</td>
<td></td>
<td>0</td>
<td>0</td>
<td>1</td>
<td></td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Similar to what we did in the previous lesson, we can see that each
word embedding gives a 1 for a dimension corresponding to the word, and
a zero for every other dimension. This kind of encoding is known as “one
hot” encoding, where a single value is 1 and all others are 0.</p>
<p>Once we have all the word embeddings for each word in the document,
we sum them all up to get the document embedding. This is the simplest
and most intuitive way to construct a document embedding from a set of
word embeddings.</p>
<p>But does it accurately represent the importance of each word?</p>
<p>Our next model, TF-IDF, will embed words with different values rather
than just 0 or 1.</p>
</section><section><h2 class="section-heading" id="tf-idf-basics">TF-IDF Basics<a class="anchor" aria-label="anchor" href="#tf-idf-basics"></a>
</h2>
<hr class="half-width">
<p>Currently our model assumes all words are created equal and are all
equally important. However, in the real world we know that certain words
are more important than others.</p>
<p>For example, in a set of novels, knowing one novel contains the word
<em>the</em> 100 times does not tell us much about it. However, if the
novel contains a rarer word such as <em>whale</em> 100 times, that may
tell us quite a bit about its content.</p>
<p>A more accurate model would weigh these rarer words more heavily, and
more common words less heavily, so that their relative importance is
part of our model.</p>
<p>However, rare is a relative term. In a corpus of documents about blue
whales, the term <em>whale</em> may be present in nearly every document.
In that case, other words may be rarer and more informative. How do we
determine how these weights are done?</p>
<p>One method for constructing more advanced word embeddings is a model
called TF-IDF.</p>
<p>TF-IDF stands for term frequency-inverse document frequency and can
be calculated for each document, <em>d</em>, and term, <em>t</em>, in a
corpus. The calculation consists of two parts: term frequency and
inverse document frequency. We multiply the two terms to get the TF-IDF
value.</p>
<p><strong>Term frequency(<em>t</em>,<em>d</em>)</strong> is a measure
for how frequently a term, <em>t</em>, occurs in a document, <em>d</em>.
The simplest way to calculate term frequency is by simply adding up the
number of times a term occurs in a document, and dividing by the total
word count in the document.</p>
<p><strong>Inverse document frequency</strong> measures a term’s
importance. Document frequency is the number of documents a term occurs
in, so inverse document frequency gives higher scores to words that
occur in fewer documents. This is represented by the equation:</p>
<p>IDF(<em>t</em>) = ln[(<em>N</em>+1) / (DF(<em>t</em>)+1)]</p>
<p>where…</p>
<ul>
<li>
<em>N</em> represents the total number of documents in the
corpus</li>
<li>DF(<em>t</em>) represents document frequency for a particular
term/word, <em>t</em>. This is the number of documents a term occurs
in.</li>
</ul>
<p>The key thing to understand is that words that occur in many
documents produce smaller IDF values since the denominator grows with
DF(<em>t</em>).</p>
<p>We can also embed documents in vector space using TF-IDF scores
rather than simple word counts. This also weakens the impact of
stop-words, since due to their common nature, they have very low
scores.</p>
<p>Now that we’ve seen how TF-IDF works, let’s put it into practice.</p>
<div class="section level3">
<h3 id="worked-example-td-idf">Worked Example: TD-IDF<a class="anchor" aria-label="anchor" href="#worked-example-td-idf"></a>
</h3>
<p>Earlier, we preprocessed our data to lemmatize each file in our
corpus, then saved our results for later.</p>
<p>Let’s load our data back in to continue where we left off. First,
we’ll mount our google drive to get access to our data folder again.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># Run this cell to mount your Google Drive.</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>drive.mount(<span class="st">'/content/drive'</span>)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># Show existing colab notebooks and helpers.py file</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> os <span class="im">import</span> listdir</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>wksp_dir <span class="op">=</span> <span class="st">'/content/drive/My Drive/Colab Notebooks/text-analysis/code'</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>listdir(wksp_dir)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co"># Add folder to colab's path so we can import the helper functions</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>sys.path.insert(<span class="dv">0</span>, wksp_dir)</span></code></pre>
</div>
<p>Then, read the data.csv file we outputted in the last episode.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> read_csv</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>data <span class="op">=</span> read_csv(<span class="st">"/content/drive/My Drive/Colab Notebooks/text-analysis/data/data.csv"</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>data.head()</span></code></pre>
</div>
<div class="section level4">
<h4 id="td-idf-vectorizer">TD-IDF Vectorizer<a class="anchor" aria-label="anchor" href="#td-idf-vectorizer"></a>
</h4>
<p>Next, let’s load a vectorizer from <code>sklearn</code> that will
help represent our corpus in TF-IDF vector space for us.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer(<span class="bu">input</span><span class="op">=</span><span class="st">'filename'</span>, max_df<span class="op">=</span><span class="fl">.6</span>, min_df<span class="op">=</span><span class="fl">.1</span>)</span></code></pre>
</div>
<p>Here, <code>max_df=.6</code> removes terms that appear in more than
60% of our documents (overly common words like the, a, an) and
<code>min_df=.1</code> removes terms that appear in less than 10% of our
documents (overly rare words like specific character names, typos, or
punctuation the tokenizer doesn’t understand). We’re looking for that
sweet spot where terms are frequent enough for us to build theoretical
understanding of what they mean for our corpus, but not so frequent that
they can’t help us tell our documents apart.</p>
<p>Now that we have our vectorizer loaded, let’s used it to represent
our data.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>tfidf <span class="op">=</span> vectorizer.fit_transform(<span class="bu">list</span>(data[<span class="st">"Lemma_File"</span>]))</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="bu">print</span>(tfidf.shape)</span></code></pre>
</div>
<p>Here, <code>tfidf.shape</code> shows us the number of rows (books)
and columns (words) are in our model.</p>
<div id="check-your-understanding-max_df-and-min_df" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="check-your-understanding-max_df-and-min_df" class="callout-inner">
<h3 class="callout-title">Check Your Understanding: <code>max_df</code>
and <code>min_df</code>
</h3>
<div class="callout-content">
<p>Try different values for <code>max_df</code> and <code>min_df</code>.
How does increasing/decreasing each value affect the number of columns
(words) that get included in the model?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>Increasing <code>max_df</code> results in more words being included
in the more, since a higher <code>max_df</code> corresponds to accepting
more common words in the model. A higher <code>max_df</code> accepts
more words likely to be stopwords.</p>
<p>Inversely, increasing <code>min_df</code> reduces the number of words
in the more, since a higher <code>min_df</code> corresponds to removing
more rare words from the model. A higher <code>min_df</code> removes
more words likely to be typos, names of characters, and so on.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="inspecting-results">Inspecting Results<a class="anchor" aria-label="anchor" href="#inspecting-results"></a>
</h3>
<p>We have a huge number of dimensions in the columns of our matrix
(just shy of 10,000), where each one of which represents a word. We also
have a number of documents (about forty), each represented as a row.</p>
<p>Let’s take a look at some of the words in our documents. Each of
these represents a dimension in our model.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>vectorizer.get_feature_names_out()[<span class="dv">0</span>:<span class="dv">5</span>]</span></code></pre>
</div>
<p>What is the weight of those words?</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="bu">print</span>(vectorizer.idf_[<span class="dv">0</span>:<span class="dv">5</span>]) <span class="co"># weights for each token</span></span></code></pre>
</div>
<p>Let’s show the weight for all the words:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> DataFrame</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>tfidf_data <span class="op">=</span> DataFrame(vectorizer.idf_, index<span class="op">=</span>vectorizer.get_feature_names_out(), columns<span class="op">=</span>[<span class="st">"Weight"</span>])</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>tfidf_data</span></code></pre>
</div>
<p>That was ordered alphabetically. Let’s try from lowest to heighest
weight:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>tfidf_data.sort_values(by<span class="op">=</span><span class="st">"Weight"</span>)</span></code></pre>
</div>
<div id="your-mileage-may-vary" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="your-mileage-may-vary" class="callout-inner">
<h3 class="callout-title">Your Mileage May Vary</h3>
<div class="callout-content">
<p>The results above will differ based on how you configured your
tokenizer and vectorizer earlier.</p>
</div>
</div>
</div>
<p>Values are no longer just whole numbers such as 0, 1 or 2. Instead,
they are weighted according to how often they occur. More common words
have lower weights, and less common words have higher weights.</p>
</div>
</section><section><h2 class="section-heading" id="tf-idf-summary">TF-IDF Summary<a class="anchor" aria-label="anchor" href="#tf-idf-summary"></a>
</h2>
<hr class="half-width">
<p>In this lesson, we learned about document embeddings and how they
could be done in multiple ways. While one hot encoding is a simple way
of doing embeddings, it may not be the best representation. TF-IDF is
another way of performing these embeddings that improves the
representation of words in our model by weighting them. TF-IDF is often
used as an intermediate step in some of the more advanced models we will
construct later.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Some words convey more information about a corpus than others</li>
<li>One-hot encodings treat all words equally</li>
<li>TF-IDF encodings weigh overly common words lower</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-06-lsa"><p>Content from <a href="06-lsa.html">Latent Semantic Analysis</a></p>
<hr>
<p>Last updated on 2025-05-01 |

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/episodes/06-lsa.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is topic modeling?</li>
<li>What is Latent Semantic Analysis (LSA)?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Use LSA to explore topics in a corpus</li>
<li>Produce and interpret an LSA plot</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>So far, we’ve learned the kinds of task NLP can be used for,
preprocessed our data, and represented it as a TF-IDF vector space.</p>
<p>Now, we begin to close the loop with Topic Modeling — one of many
embedding-related tasks possible with NLP.</p>
<figure><img src="fig/01-Interpretive_Loop.JPG" alt="The Interpretive Loop" class="figure mx-auto d-block"></figure><p>Topic Modeling is a frequent goal of text analysis. Topics are the
things that a document is about, by some sense of “about.” We could
think of topics as:</p>
<ul>
<li>discrete categories that your documents belong to, such as fiction
vs. non-fiction</li>
<li>or spectra of subject matter that your documents contain in
differing amounts, such as being about politics, cooking, racing,
dragons, …</li>
</ul>
<p>In the first case, we could use machine learning to predict discrete
categories, such as <a href="https://towardsdatascience.com/hamilton-a-text-analysis-of-the-federalist-papers-e64cb1764fbf" class="external-link">trying
to determine the author of the Federalist Papers</a>.</p>
<p>In the second case, we could try to determine the least number of
topics that provides the most information about how our documents differ
from one another, then use those concepts to gain insight about the
“stuff” or “story” of our corpus as a whole.</p>
<p>In this lesson we’ll focus on this second case, where topics are
treated as spectra of subject matter. There are a variety of ways of
doing this, and not all of them use the vector space model we have
learned. For example:</p>
<ul>
<li>Vector-space models:
<ul>
<li>Principle Component Analysis (PCA)</li>
<li>Epistemic Network Analysis (ENA)</li>
<li>Latent Semantic Analysis (LSA)</li>
</ul>
</li>
<li>Probability models:
<ul>
<li>Latent Dirichlet Allocation (LDA)</li>
</ul>
</li>
</ul>
<p>Specifically, we will be discussing Latent Semantic Analysis (LSA).
We’re narrowing our focus to LSA because it introduces us to concepts
and workflows that we will use in the future, in particular that of
dimensional reduction.</p>
<section><h2 class="section-heading" id="what-is-dimensional-reduction">What is dimensional reduction?<a class="anchor" aria-label="anchor" href="#what-is-dimensional-reduction"></a>
</h2>
<hr class="half-width">
<p>Think of a map of the Earth. The Earth is a three dimensional sphere,
but we often represent it as a two dimensional shape such as a square or
circle. We are performing dimensional reduction- taking a three
dimensional object and trying to represent it in two dimensions.</p>
<figure><img src="fig/05-projections.jpg" alt="Maps with different projections of the Earth" class="figure mx-auto d-block"></figure><p>Why do we create maps? It can often be helpful to have a two
dimensional representation of the Earth. It may be used to get an
approximate idea of the sizes and shapes of various countries next to
each other, or to determine at a glance what things are roughly in the
same direction.</p>
<p>How do we create maps? There’s many ways to do it, depending on what
properties are important to us. We cannot perfectly capture area, shape,
direction, bearing and distance all in the same model- we must make
tradeoffs. Different projections will better preserve different
properties we find desirable. But not all the relationships will be
preserved- some projections will distort area in certain regions, others
will distort directions or proximity. Our technique will likely depend
on what our application is and what we determine is valuable.</p>
<p>Dimensional reduction for our data is the same principle. Why do we
do dimensional reduction? When we perform dimensional reduction we hope
to take our highly dimensional language data and get a useful ‘map’ of
our data with fewer dimensions. We have various tasks we may want our
map to help us with. We can determine what words and documents are
semantically “close” to each other, or create easy to visualise clusters
of points.</p>
<p>How do we do dimensional reduction? There are many ways to do
dimensional reduction, in the same way that we have many projections for
maps. Like maps, different dimensional reduction techniques have
different properties we have to choose between- high performance in
tasks, ease of human interpretation, and making the model easily
trainable are a few. They are all desirable but not always compatible.
When we lose a dimension, we inevitably lose data from our original
representation. This problem is multiplied when we are reducing so many
dimensions. We try to bear in mind the tradeoffs and find useful models
that don’t lose properties and relationships we find important. But
“importance” depends on your moral theoretical stances. Because of this,
it is important to carefully inspect the results of your model,
carefully interpret the “topics” it identifies, and check all that
against your qualitative and theoretical understanding of your
documents.</p>
<p>This will likely be an iterative process where you refine your model
several times. Keep in mind the adage: all models are wrong, some are
useful, and a less accurate model may be easier to explain to your
stakeholders.</p>
</section><section><h2 class="section-heading" id="lsa">LSA<a class="anchor" aria-label="anchor" href="#lsa"></a>
</h2>
<hr class="half-width">
<p>The assumption behind LSA is that underlying the thousands of words
in our vocabulary are a smaller number of hidden (“latent”) topics, and
that those topics help explain the distribution of the words we see
across our documents. In all our models so far, each dimension has
corresponded to a single word. But in LSA, each dimension now
corresponds to a hidden topic, and each of those in turn corresponds to
the words that are most strongly associated with it.</p>
<p>For example, a hidden topic might be <a href="https://museumhack.com/english-language-changed/" class="external-link">the lasting
influence of the Battle of Hastings on the English language</a>, with
some documents using more words with Anglo-Saxon roots and other
documents using more words with Latin roots. This dimension is “hidden”
because authors don’t usually stamp a label on their books with a
summary of the linguistic histories of their words. Still, we can
imagine a spectrum between words that are strongly indicative of authors
with more Anglo-Saxon diction vs. words strongly indicative of authors
with more Latin diction. Once we have that spectrum, we can place our
documents along it, then move on to the next hidden topic, then the
next, and so on, until we’ve discussed the fewest, strongest hidden
topics that capture the most “story” about our corpus.</p>
<p>LSA requires two steps- first we must create a TF-IDF matrix, which
we have already covered in our previous lesson.</p>
<p>Next, we will perform dimensional reduction using a technique called
SVD.</p>
<div class="section level3">
<h3 id="worked-example-lsa">Worked Example: LSA<a class="anchor" aria-label="anchor" href="#worked-example-lsa"></a>
</h3>
<p>In case you are starting from a fresh notebook, you will need to (1),
mount Google drive (2) add the helper code to your path, (3) load the
data.csv file, and (4) pip install parse which is used in the helper
function code.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># Run this cell to mount your Google Drive.</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>drive.mount(<span class="st">'/content/drive'</span>)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># Show existing colab notebooks and helpers.py file</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> os <span class="im">import</span> listdir</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>wksp_dir <span class="op">=</span> <span class="st">'/content/drive/My Drive/Colab Notebooks/text-analysis/code'</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="bu">print</span>(listdir(wksp_dir))</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co"># Add folder to colab's path so we can import the helper functions</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>sys.path.insert(<span class="dv">0</span>, wksp_dir)</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="co"># Read the data back in.</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> read_csv</span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>data <span class="op">=</span> read_csv(<span class="st">"/content/drive/My Drive/Colab Notebooks/text-analysis/data/data.csv"</span>)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>data.head()</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="op">!</span>pip install pathlib parse <span class="co"># parse is used by helper functions</span></span></code></pre>
</div>
<p>Mathematically, these “latent semantic” dimensions are derived from
our TF-IDF matrix, so let’s begin there. From the previous lesson:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer(<span class="bu">input</span><span class="op">=</span><span class="st">'filename'</span>, max_df<span class="op">=</span><span class="fl">.6</span>, min_df<span class="op">=</span><span class="fl">.1</span>) <span class="co"># Here, max_df=.6 removes terms that appear in more than 60% of our documents (overly common words like the, a, an) and min_df=.1 removes terms that appear in less than 10% of our documents (overly rare words like specific character names, typos, or punctuation the tokenizer doesn't understand)</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>tfidf <span class="op">=</span> vectorizer.fit_transform(<span class="bu">list</span>(data[<span class="st">"Lemma_File"</span>]))</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="bu">print</span>(tfidf.shape)</span></code></pre>
</div>
<p>What do these dimensions mean? We have 41 documents, which we can
think of as rows. And we have several thousands of tokens, which is like
a dictionary of all the types of words we have in our documents, and
which we represent as columns.</p>
<div class="section level4">
<h4 id="dimension-reduction-via-singular-value-decomposition-svd">Dimension Reduction Via Singular Value Decomposition (SVD)<a class="anchor" aria-label="anchor" href="#dimension-reduction-via-singular-value-decomposition-svd"></a>
</h4>
<p>Now we want to reduce the number of dimensions used to represent our
documents. We will use a technique called Singular Value Decomposition
(SVD) to do so. SVD is a powerful linear algebra tool that works by
capturing the underlying patterns and relationships within a given
matrix. When applied to a TF-IDF matrix, it identifies the most
significant patterns of word co-occurrence across documents and
condenses this information into a smaller set of “topics,” which are
abstract representations of semantic themes present in the corpus. By
reducing the number of dimensions, we gradually distill the essence of
our corpus into a concise set of topics that capture the key themes and
concepts across our documents. This streamlined representation not only
simplifies further analysis but also uncovers the latent structure
inherent in our text data, enabling us to gain deeper insights into its
content and meaning.</p>
<p>To see this, let’s begin to reduce the dimensionality of our TF-IDF
matrix using SVD, starting with the greatest number of dimensions
(min(#rows, #cols)). In this case the maxiumum number of ‘topics’
corresponds to the number of documents- 41.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> TruncatedSVD</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>maxDimensions <span class="op">=</span> <span class="bu">min</span>(tfidf.shape)<span class="op">-</span><span class="dv">1</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>svdmodel <span class="op">=</span> TruncatedSVD(n_components<span class="op">=</span>maxDimensions, algorithm<span class="op">=</span><span class="st">"arpack"</span>) <span class="co"># The "arpack" algorithm is typically more efficient for large sparse matrices compared to the default "randomized" algorithm. This is particularly important when dealing with high-dimensional data, such as TF-IDF matrices, where the number of features (terms) may be large. SVD is typically computed as an approximation when working with large matrices.</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>lsa <span class="op">=</span> svdmodel.fit_transform(tfidf)</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a><span class="bu">print</span>(lsa)</span></code></pre>
</div>
<p>Unlike with a globe, we must make a choice of how many dimensions to
cut out. We could have anywhere between 41 topics to 2.</p>
<p>How should we pick a number of topics to keep? Fortunately, the
dimension reducing technique we used produces something to help us
understand how much data each topic explains. Let’s take a look and see
how much data each topic explains. We will visualize it on a graph.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co">#this shows us the amount of dropoff in explanation we have in our sigma matrix. </span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="bu">print</span>(svdmodel.explained_variance_ratio_)</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="co"># Calculate cumulative sum of explained variance ratio</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>cumulative_variance_ratio <span class="op">=</span> np.cumsum(svdmodel.explained_variance_ratio_)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, maxDimensions <span class="op">+</span> <span class="dv">1</span>), cumulative_variance_ratio <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of Topics"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>plt.ylabel(<span class="st">"Cumulative </span><span class="sc">% o</span><span class="st">f Information Retained"</span>)</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">100</span>)  <span class="co"># Adjust y-axis limit to 0-100</span></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>plt.grid(<span class="va">True</span>)    <span class="co"># Add grid lines</span></span></code></pre>
</div>
<figure><img src="fig/LSA_cumulative_information_retained_plot.png" alt="Image of drop-off of variance explained" class="figure mx-auto d-block"></figure><p>Often a heuristic used by researchers to determine a topic count is
to look at the dropoff in percentage of data explained by each
topic.</p>
<p>Typically the rate of data explained will be high at first, dropoff
quickly, then start to level out. We can pick a point on the “elbow”
where it goes from a high level of explanation to where it starts
leveling out and not explaining as much per topic. Past this point, we
begin to see diminishing returns on the amount of the “stuff” of our
documents we can cover quickly. This is also often a good sweet spot
between overfitting our model and not having enough topics.</p>
<p>Alternatively, we could set some target sum for how much of our data
we want our topics to explain, something like 90% or 95%. However, with
a small dataset like this, that would result in a large number of
topics, so we’ll pick an elbow instead.</p>
<p>Looking at our results so far, a good number in the middle of the
“elbow” appears to be around 5-7 topics. So, let’s fit a model using
only 6 topics and then take a look at what each topic looks like.</p>
<div id="why-is-the-first-topic-topic-0-so-low" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="why-is-the-first-topic-topic-0-so-low" class="callout-inner">
<h3 class="callout-title">Why is the first topic, “Topic 0,” so
low?</h3>
<div class="callout-content">
<p>It has to do with how our SVD was setup. Truncated SVD does not mean
center the data beforehand, which takes advantage of sparse matrix
algorithms by leaving most of the data at zero. Otherwise, our matrix
will me mostly filled with the negative of the mean for each column or
row, which takes much more memory to store. The math is outside the
scope for this lesson, but it’s expected in this scenario that topic 0
will be less informative than the ones that come after it, so we’ll skip
it.</p>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>numDimensions <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>svdmodel <span class="op">=</span> TruncatedSVD(n_components<span class="op">=</span>numDimensions, algorithm<span class="op">=</span><span class="st">"arpack"</span>)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>lsa <span class="op">=</span> svdmodel.fit_transform(tfidf)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="bu">print</span>(lsa)</span></code></pre>
</div>
<p>And put all our results together in one DataFrame so we can save it
to a spreadsheet to save all the work we’ve done so far. This will also
make plotting easier in a moment.</p>
<p>Since we don’t know what these topics correspond to yet, for now I’ll
call the first topic X, the second Y, the third Z, and so on.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>data[[<span class="st">"X"</span>, <span class="st">"Y"</span>, <span class="st">"Z"</span>, <span class="st">"W"</span>, <span class="st">"P"</span>, <span class="st">"Q"</span>]] <span class="op">=</span> lsa[:, [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]]</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>data.head()</span></code></pre>
</div>
<p>Let’s also mean-center the data, so that the “average” value per
topic (across all our documents) lies at the origin when we plot things
in a moment. By mean-centering, you are ensuring that the “average”
value for each topic becomes the reference point (0,0) in the plot,
which can provide more informative insights into the relative
distribution and relationships between topics.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>data[[<span class="st">"X"</span>, <span class="st">"Y"</span>, <span class="st">"Z"</span>, <span class="st">"W"</span>, <span class="st">"P"</span>, <span class="st">"Q"</span>]] <span class="op">=</span> lsa[:, [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]]<span class="op">-</span>lsa[:, [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]].mean(<span class="dv">0</span>)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>data[[<span class="st">"X"</span>, <span class="st">"Y"</span>, <span class="st">"Z"</span>, <span class="st">"W"</span>, <span class="st">"P"</span>, <span class="st">"Q"</span>]].mean()</span></code></pre>
</div>
<p>Finally, let’s save our progress so far.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>data.to_csv(<span class="st">"/content/drive/My Drive/Colab Notebooks/text-analysis/data/data.csv"</span>, index<span class="op">=</span><span class="va">False</span>)</span></code></pre>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="inspecting-lsa-results">Inspecting LSA Results<a class="anchor" aria-label="anchor" href="#inspecting-lsa-results"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="plotting">Plotting<a class="anchor" aria-label="anchor" href="#plotting"></a>
</h3>
<p>Let’s plot the results, using a helper we prepared for learners.
We’ll focus on the X and Y topics for now to illustrate the workflow.
We’ll return to the other topics in our model as a further exercise.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="im">from</span> helpers <span class="im">import</span> lsa_plot</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>lsa_plot(data, svdmodel)</span></code></pre>
</div>
<figure><img src="fig/05-lsa-plot.png" alt="Plot results of our LSA model" class="figure mx-auto d-block"></figure><p>What do you think these X and Y axes are capturing, conceptually?</p>
<p>To help figure that out, lets color-code by author to see if any
patterns are immediately apparent.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>colormap <span class="op">=</span> {</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>    <span class="st">"austen"</span>: <span class="st">"red"</span>,</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>    <span class="st">"chesterton"</span>: <span class="st">"blue"</span>,</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    <span class="st">"dickens"</span>: <span class="st">"green"</span>,</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>    <span class="st">"dumas"</span>: <span class="st">"orange"</span>,</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>    <span class="st">"melville"</span>: <span class="st">"cyan"</span>,</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>    <span class="st">"shakespeare"</span>: <span class="st">"magenta"</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>}</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>lsa_plot(data, svdmodel, groupby<span class="op">=</span><span class="st">"author"</span>, colors<span class="op">=</span>colormap)</span></code></pre>
</div>
<figure><img src="fig/05-lsa-plot-color.png" alt="Plot results of our LSA model, color-coded by author" class="figure mx-auto d-block"></figure><p>It seems that some of the books by the same author are clumping up
together in our plot.</p>
<p>We don’t know <em>why</em> they are getting arranged this way, since
we don’t know what more concepts X and Y correspond to. But we can work
do some work to figure that out.</p>
</div>
<div class="section level3">
<h3 id="topics">Topics<a class="anchor" aria-label="anchor" href="#topics"></a>
</h3>
<p>Let’s write a helper to get the strongest words for each topic. This
will show the terms with the <em>highest</em> and <em>lowest</em>
association with a topic. In LSA, each topic is a spectra of subject
matter, from the kinds of terms on the low end to the kinds of terms on
the high end. So, inspecting the <em>contrast</em> between these high
and low terms (and checking that against our domain knowledge) can help
us interpret what our model is identifying.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="kw">def</span> show_topics(vectorizer, svdmodel, topic_number, n):</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    <span class="co"># Get the feature names (terms) from the TF-IDF vectorizer</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>    terms <span class="op">=</span> vectorizer.get_feature_names_out()</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>    </span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>    <span class="co"># Get the weights of the terms for the specified topic from the SVD model</span></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>    weights <span class="op">=</span> svdmodel.components_[topic_number]</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>    </span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>    <span class="co"># Create a DataFrame with terms and their corresponding weights</span></span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame({<span class="st">"Term"</span>: terms, <span class="st">"Weight"</span>: weights})</span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a>    </span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>    <span class="co"># Sort the DataFrame by weights in descending order to get top n terms (largest positive weights)</span></span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a>    highs <span class="op">=</span> df.sort_values(by<span class="op">=</span>[<span class="st">"Weight"</span>], ascending<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>:n]</span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>    </span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a>    <span class="co"># Sort the DataFrame by weights in ascending order to get bottom n terms (largest negative weights)</span></span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a>    lows <span class="op">=</span> df.sort_values(by<span class="op">=</span>[<span class="st">"Weight"</span>], ascending<span class="op">=</span><span class="va">False</span>)[<span class="op">-</span>n:]</span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a>    </span>
<span id="cb12-19"><a href="#cb12-19" tabindex="-1"></a>    <span class="co"># Concatenate top and bottom terms into a single DataFrame and return</span></span>
<span id="cb12-20"><a href="#cb12-20" tabindex="-1"></a>    <span class="cf">return</span> pd.concat([highs, lows])</span>
<span id="cb12-21"><a href="#cb12-21" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" tabindex="-1"></a><span class="co"># Get the top 5 and bottom 5 terms for each specified topic</span></span>
<span id="cb12-23"><a href="#cb12-23" tabindex="-1"></a>topic_words_x <span class="op">=</span> show_topics(vectorizer, svdmodel, <span class="dv">1</span>, <span class="dv">5</span>)  <span class="co"># Topic 1</span></span>
<span id="cb12-24"><a href="#cb12-24" tabindex="-1"></a>topic_words_y <span class="op">=</span> show_topics(vectorizer, svdmodel, <span class="dv">2</span>, <span class="dv">5</span>)  <span class="co"># Topic 2</span></span></code></pre>
</div>
<p>You can also use a helper we prepared for learners:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">from</span> helpers <span class="im">import</span> show_topics</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>topic_words_x <span class="op">=</span> show_topics(vectorizer, svdmodel, topic_number<span class="op">=</span><span class="dv">1</span>, n<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>topic_words_y <span class="op">=</span> show_topics(vectorizer, svdmodel, topic_number<span class="op">=</span><span class="dv">2</span>, n<span class="op">=</span><span class="dv">5</span>)</span></code></pre>
</div>
<p>Either way, let’s look at the terms for the X topic.</p>
<p>What does this topic seem to represent to you? What’s the contrast
between the top and bottom terms?</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="bu">print</span>(topic_words_x)</span></code></pre>
</div>
<p>And the Y topic. What’s the contrast between the top and bottom
terms?</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="bu">print</span>(topic_words_y)</span></code></pre>
</div>
<p>Now that we have names for our first two topics, let’s redo the plot
with better axis labels.</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>lsa_plot(data, svdmodel, groupby<span class="op">=</span><span class="st">"author"</span>, colors<span class="op">=</span>colormap, xlabel<span class="op">=</span><span class="st">"Victorian vs. Elizabethan"</span>, ylabel<span class="op">=</span><span class="st">"English vs. French"</span>)</span></code></pre>
</div>
<figure><img src="fig/05-lsa-plot-labeled.png" alt="Plot results of our LSA model, revised with new axis labels" class="figure mx-auto d-block"></figure><div id="check-your-understanding-intrepreting-lsa-results" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="check-your-understanding-intrepreting-lsa-results" class="callout-inner">
<h3 class="callout-title">Check Your Understanding: Intrepreting LSA
Results</h3>
<div class="callout-content">
<p>Let’s repeat this process with the other 4 topics, which we
tentatively called Z, W, P, and Q.</p>
<p>In the first two topics (X and Y), some authors were clearly
separated, but others overlapped. If we hadn’t color coded them, we
wouldn’t be easily able to tell them apart.</p>
<p>But in remaining topics, different combinations of authors get pulled
apart or together. This is because these topics (Z, W, P, and Q)
highlight different features of the data, <em>independent</em> of the
features we’ve already captured above.</p>
<p>Take a few moments to work through the steps above for the remaining
dimensions Z, W, P, and Q, and chat with one another about what you
think the topics being represented are.</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Topic modeling helps explore and describe the content of a
corpus</li>
<li>LSA defines topics as spectra that the corpus is distributed
over</li>
<li>Each dimension (topic) in LSA corresponds to a contrast between
positively and negatively weighted words</li>
</ul>
</div>
</div>
</div>
</div>
</section></section><section id="aio-07-wordEmbed_intro"><p>Content from <a href="07-wordEmbed_intro.html">Intro to Word Embeddings</a></p>
<hr>
<p>Last updated on 2025-05-01 |

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/episodes/07-wordEmbed_intro.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can we extract vector representations of individual words rather
than documents?</li>
<li>What sort of research questions can be answered with word embedding
models?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the difference between document embeddings and word
embeddings</li>
<li>Introduce the Gensim python library and its word embedding
functionality</li>
<li>Explore vector math with word embeddings using pretrained
models</li>
<li>Visualize word embeddings with the help of principal component
analysis (PCA)</li>
<li>Discuss word embedding use-cases</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="load-pre-trained-model-via-gensim">Load pre-trained model via Gensim<a class="anchor" aria-label="anchor" href="#load-pre-trained-model-via-gensim"></a>
</h2>
<hr class="half-width">
<p>First, load the Word2Vec embedding model. The Word2Vec model takes
3-10 minutes to load.</p>
<p>We’ll be using the Gensim library. The Gensim library comes with
several word embedding models including Word2Vec, GloVe, and fastText.
We’ll start by exploring one of the pre-trained Word2Vec models. We’ll
discuss the other options later in this lesson. Gensim is an older
library that is a little finnicky. We need to make sure we have the
right version of numpy to go along with it. This next cell will install
gensim and a compatible version of numpy.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="op">!</span>pip uninstall <span class="op">-</span>y numpy gensim</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="op">!</span>rm <span class="op">-</span>rf <span class="op">/</span>usr<span class="op">/</span>local<span class="op">/</span>lib<span class="op">/</span>python3<span class="fl">.11</span><span class="op">/</span>dist<span class="op">-</span>packages<span class="op">/</span>numpy<span class="op">*</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">--</span>no<span class="op">-</span>cache<span class="op">-</span><span class="bu">dir</span> numpy<span class="op">==</span><span class="fl">1.26.4</span> gensim<span class="op">==</span><span class="fl">4.3.3</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="load-google-news-model-word2vec">Load Google News model (Word2Vec)<a class="anchor" aria-label="anchor" href="#load-google-news-model-word2vec"></a>
</h3>
<p><strong>Note</strong>: You may need to restart the kernel after
installing gensim (above cell) for the import statement (below cell) to
work.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># RUN BEFORE INTRO LECTURE :)</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co"># api to load word2vec models</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co"># takes 3-10 minutes to load</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>wv <span class="op">=</span> api.load(<span class="st">'word2vec-google-news-300'</span>) <span class="co"># takes 3-10 minutes to load </span></span></code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="documentcorpus-embeddings-recap">Document/Corpus Embeddings Recap<a class="anchor" aria-label="anchor" href="#documentcorpus-embeddings-recap"></a>
</h2>
<hr class="half-width">
<p>So far, we’ve seen how word counts (bag of words), TF-IDF, and LSA
can help us embed a document or set of documents into useful vector
spaces that allow us to gain insights from text data. Let’s review the
embeddings covered thus far…</p>
<ul>
<li><p><strong>Word count embeddings</strong>: Word count embeddings are
a simple yet powerful method that represent text data as a sparse vector
where each dimension corresponds to a unique word in the vocabulary, and
the value in each dimension indicates the frequency of that word in the
document. This approach disregards word order and context, treating each
document as an unordered collection of words or tokens.</p></li>
<li><p><strong>TF-IDF embeddings:</strong> Term Frequency Inverse
Document Frequency (TF-IDF) is a fancier word-count method. It
emphasizes words that are both frequent within a specific document
<em>and</em> rare across the entire corpus.</p></li>
<li><p><strong>LSA embeddings:</strong> Latent Semantic Analysis (LSA)
is used to find the hidden topics represented by a group of documents.
It involves running singular-value decomposition (SVD) on a
document-term matrix (typically the TF-IDF matrix), producing a vector
representation of each document. This vector scores each document’s
representation in different topic/concept areas which are derived based
on word co-occurences (e.g., 45% topic A, 35% topic B, and 20% topic C).
Importantly, LSA is considered a <em>bag of words</em> method since the
order of words in a document is not considered.</p></li>
</ul>
<p>To get a high-level overview of the embedding methods covered thus
far, study the table below:</p>
<table class="table">
<colgroup>
<col width="21%">
<col width="11%">
<col width="13%">
<col width="16%">
<col width="25%">
<col width="11%">
</colgroup>
<thead><tr class="header">
<th align="center">Technique</th>
<th align="center">Input</th>
<th align="center">Embedding Structure</th>
<th align="center">Output Vector Dimensions</th>
<th align="center">Meaning Stored</th>
<th align="center">Order Dependency</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">Word Counts</td>
<td align="center">Raw text corpus</td>
<td align="center">Sparse vectors</td>
<td align="center">[1, Vocabulary Size] <br>(per document)</td>
<td align="center">Word presence in documents</td>
<td align="center">No <br>(bag of words)</td>
</tr>
<tr class="even">
<td align="center">TF-IDF</td>
<td align="center">Word Counts</td>
<td align="center">Sparse vectors</td>
<td align="center">[1, Vocabulary Size] <br>(per document)</td>
<td align="center">Importance of terms in documents</td>
<td align="center">No <br>(bag of words)</td>
</tr>
<tr class="odd">
<td align="center">Latent Semantic Analysis (LSA)</td>
<td align="center">TF-IDF or similar</td>
<td align="center">Dense vectors</td>
<td align="center">[1, Number of Topics] <br>(per document)</td>
<td align="center">Semantic topics present in documents</td>
<td align="center">No <br>(bag of words)</td>
</tr>
</tbody>
</table>
<div class="section level3">
<h3 id="bag-of-words-limitations">Bag of Words limitations<a class="anchor" aria-label="anchor" href="#bag-of-words-limitations"></a>
</h3>
<p>In all of these emebdding methods, notice how the order of words in
sentences does not matter. We are simply tossing all words in a corpus
into a bag (“bag of words”) and attempting to glean insights from this
bag of words. While such an approach can be effective for revealing
broad topics/concepts from text, additional features of language may be
revealed by zooming in on the context in which words appear throughout a
text.</p>
<p>For instance, maybe our bag of words contains the following: “cook”,
“I”, “family”, “my”, “to”, “dinner”, “love”, and “for”. Depending on how
these words are arranged, the meaning conveyed will change
drastically!</p>
<ul>
<li><em>I love to cook dinner for my family.</em></li>
<li><em>I love to cook family for my dinner.</em></li>
</ul>
</div>
<div class="section level3">
<h3 id="distributional-hypothesis-extracting-more-meaningful-representations-of-text">Distributional hypothesis: extracting more meaningful
representations of text<a class="anchor" aria-label="anchor" href="#distributional-hypothesis-extracting-more-meaningful-representations-of-text"></a>
</h3>
<p>To clarify whether our text is about a nice wholesome family or a
cannibal on the loose, we need to include context in our embeddings. As
the famous linguist JR Firth once said, “You shall know a word by the
company it keeps.” Firth is referring to the <em>distributional
hypothesis</em>, which states that words that repeatedly occur in
similar contexts probably have similar meanings. While the LSA
methodology is inspired by the distributional hypothesis, LSA ignores
the context of words as they appear in sentences and only pays attention
to global word co-occurence patterns across large chunks of texts. If we
want to truly know a word based on the company it keeps, we’ll need to
take into account how some words are more likely to appear before/after
other words in a sentence. We’ll explore how one of the most famous
embedding models, Word2Vec, does this in this episode.</p>
</div>
</section><section><h2 class="section-heading" id="word-embeddings-with-word2vec">Word embeddings with Word2Vec<a class="anchor" aria-label="anchor" href="#word-embeddings-with-word2vec"></a>
</h2>
<hr class="half-width">
<p>Word2vec is a famous <em>word embedding</em> method that was created
and published in the ancient year of 2013 by a team of researchers led
by Tomas Mikolov at Google over two papers, [<a href="https://arxiv.org/abs/1301.3781" class="external-link">1</a>, <a href="https://arxiv.org/abs/1310.4546" class="external-link">2</a>]. Unlike with TF-IDF and
LSA, which are typically used to produce document and corpus embeddings,
Word2Vec focuses on producing a single embedding for every word
encountered in a corpus. These embeddings, which are represented as
high-dimesional vectors, tend to look very similar for words that are
used in similar contexts. Adding this method to our overview table, we
get:</p>
<table class="table">
<colgroup>
<col width="21%">
<col width="11%">
<col width="13%">
<col width="16%">
<col width="25%">
<col width="11%">
</colgroup>
<thead><tr class="header">
<th align="center">Technique</th>
<th align="center">Input</th>
<th align="center">Embedding Structure</th>
<th align="center">Output Vector Dimensions</th>
<th align="center">Meaning Stored</th>
<th align="center">Order Dependency</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">Word Counts</td>
<td align="center">Raw text corpus</td>
<td align="center">Sparse vectors</td>
<td align="center">[1, Vocabulary Size] <br>(per document)</td>
<td align="center">Word presence in documents</td>
<td align="center">No <br>(bag of words)</td>
</tr>
<tr class="even">
<td align="center">TF-IDF</td>
<td align="center">Word Counts</td>
<td align="center">Sparse vectors</td>
<td align="center">[1, Vocabulary Size] <br>(per document)</td>
<td align="center">Importance of terms in documents</td>
<td align="center">No <br>(bag of words)</td>
</tr>
<tr class="odd">
<td align="center">Latent Semantic Analysis (LSA)</td>
<td align="center">TF-IDF or similar</td>
<td align="center">Dense vectors</td>
<td align="center">[1, Number of Topics] <br>(per document)</td>
<td align="center">Semantic topics present in documents</td>
<td align="center">No <br>(bag of words)</td>
</tr>
<tr class="even">
<td align="center">Word2Vec</td>
<td align="center">Raw text corpus</td>
<td align="center">Dense vectors</td>
<td align="center">[1, Embedding Dimension] <br>(per word)</td>
<td align="center">Semantic meaning of words</td>
<td align="center">Yes <br>(word order)</td>
</tr>
</tbody>
</table>
<p>The next <em>supplemental</em> episode unpacks the technology behind
Word2Vec — neural networks. In the interest of time, we will only cover
the key concepts and intuition. Please consider studying the next
episode if you are interested in learning more about the fascinating
world of neural networks and how they actually work. For now, it is
sufficient to be aware of few key insights.</p>
<div class="section level3">
<h3 id="neural-networks-have-an-exceptional-ability-to-learn-functions-that-can-map-a-set-of-input-features-to-some-output-">1. Neural networks have an exceptional ability to learn functions
that can map a set of input features to some output.<a class="anchor" aria-label="anchor" href="#neural-networks-have-an-exceptional-ability-to-learn-functions-that-can-map-a-set-of-input-features-to-some-output-"></a>
</h3>
<p>Because of this general capability, they can be used for a wide
assortment of tasks including…</p>
<ul>
<li>Predicting the weather tomorrow given historical weather
patterns</li>
<li>Classifying if an email is spam or not</li>
<li>Classifying if an image contains a person or not</li>
<li>Predicting a person’s weight based on their height, age, location,
etc.</li>
<li>Predicting commute times given traffic conditions</li>
<li>Predicting house prices given stock market prices</li>
</ul>
</div>
<div class="section level3">
<h3 id="neural-networks-learn-new-meaningful-features-from-the-input-data-">2. Neural networks <em>learn</em> new meaningful features from the
input data.<a class="anchor" aria-label="anchor" href="#neural-networks-learn-new-meaningful-features-from-the-input-data-"></a>
</h3>
<p>Specifically, the learned features will be features that are useful
for whatever task the model is assigned. With this consideration, we can
devise a language related task that allows a neural network model to
learn interesting features of words which can then be extracted from the
model as a word embedding representation (i.e., a vector).</p>
<p><strong>What task can we give a neural network to learn meaningful
word embeddings?</strong> Our friend RJ Firth gives us a hint when he
says, “You shall know a word by the company it keeps.” Using the
<em>distributional hypothesis</em> as motivation, which states that
words that repeatedly occur in similar contexts probably have similar
meanings, we can ask a neural network to predict the <em>context</em>
words that surround a given word in a sentence or, similarly, ask it to
predict the <em>center</em> word based on <em>context</em> words. Both
variants are shown below — Skip Gram and Continous Bag of Words
(CBOW).</p>
<figure><img src="fig/wordEmbed_NN-training-methods.png" alt="Skipgram" class="figure mx-auto d-block"></figure><div class="section level4">
<h4 id="learning-a-vector-representation-of-the-word-outside">Learning a vector representation of the word, “outside”<a class="anchor" aria-label="anchor" href="#learning-a-vector-representation-of-the-word-outside"></a>
</h4>
<p>Word2Vec is an neural network model that <em>learns</em>
high-dimensional (many features) vector representations of
<em>individual words</em> based on observing a word’s most likely
surrounding words in multiple sentences (dist. hypothesis). For
instance, suppose we want to learn a vector representation of the word
“outside”. For this, we would train the Word2Vec model on many sentences
containing the word, “outside”.</p>
<ul>
<li><em>It’s a beautiful day <strong>outside</strong>, perfect for a
picnic.</em></li>
<li><em>My cat loves to spend time <strong>outside</strong>, chasing
birds and bugs.</em></li>
<li><em>The noise <strong>outside</strong> woke me up early this
morning.</em></li>
<li><em>I always feel more relaxed after spending some time
<strong>outside</strong> in nature.</em></li>
<li><em>I can hear the rain pouring <strong>outside</strong>, it’s a
good day to stay indoors.</em></li>
<li><em>The sun is shining brightly <strong>outside</strong>, it’s time
to put on some sunscreen.</em></li>
<li><em>I saw a group of kids playing <strong>outside</strong> in the
park.</em></li>
<li><em>It’s not safe to leave your belongings <strong>outside</strong>
unattended.</em></li>
<li><em>I love to go for a walk <strong>outside</strong> after dinner to
help me digest.</em></li>
<li><em>The temperature <strong>outside</strong> is dropping, I need to
grab a jacket before I leave.</em></li>
</ul>
<p>In the process of training, the model’s weights learn to derive new
features (weight optimized perceptrons) associated with the input data
(single words). These new learned features will be conducive to
accurately predicting the context words for each word. In addition, the
features can be used as a information-rich vector representation of the
word, “outside”.</p>
<p><strong>Skip-gram versus Continuous Bag of Words</strong>: The
primary difference between these two approaches lies in how CBOW and
Skip-gram handle the context words for each target word. In CBOW, the
context words are averaged together to predict the target word, while in
Skip-gram, each context word is considered separately to predict the
target word. While both CBOW and Skip-gram consider each word-context
pair during training, Skip-gram often performs better with rare words
because it treats each occurrence of a word separately, generating more
training examples for rare words compared to CBOW. This can lead to
better representations of rare words in Skip-gram embeddings.</p>
</div>
</div>
<div class="section level3">
<h3 id="the-vectors-learned-by-the-model-are-a-reflection-of-the-models-past-experience-">3. The vectors learned by the model are a reflection of the model’s
past experience.<a class="anchor" aria-label="anchor" href="#the-vectors-learned-by-the-model-are-a-reflection-of-the-models-past-experience-"></a>
</h3>
<p>Past experience = the specific data the model was “trained” on. This
means that the vectors extracted from the model will reflect, on
average, how words are used in a specific text. For example, notice how
in the example sentences given above, the word “outside” tends to be
surrounded by words associated with the outdoors.</p>
</div>
<div class="section level3">
<h3 id="the-learned-features-or-vectors-are-black-boxes-lacking-direct-interpretability-">4. The learned features or vectors are <em>black boxes</em>, lacking
direct interpretability.<a class="anchor" aria-label="anchor" href="#the-learned-features-or-vectors-are-black-boxes-lacking-direct-interpretability-"></a>
</h3>
<p>The learned vectors create useful and meaningful representations of
words, capturing semantic relationships based on word co-occurrences.
However, these vectors represent abstract features learned from the
surrounding context of words in the training data, and are not directly
interpretable. Still, once we have language mapped to a numerical space,
we can compare things on a relative scale and ask a variety of reserach
questions.</p>
<div id="word2vec-applications" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="word2vec-applications" class="callout-inner">
<h3 class="callout-title">Word2Vec Applications</h3>
<div class="callout-content">
<p>Take a few minutes to think about different types of questions or
problems that could be addressed using Word2Vec and word embeddings.
Share your thoughts and suggestions with the class.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ul>
<li>
<strong>Semantic Change Over Time</strong>: How have the meanings of
words evolved over different historical periods? By training Word2Vec
models on texts from different time periods, researchers can analyze how
word embeddings change over time, revealing shifts in semantic
usage.</li>
<li>Authorship Attribution: Can Word2Vec be used to identify the authors
of anonymous texts or disputed authorship works? By comparing the word
embeddings of known authors’ works with unknown texts, researchers can
potentially attribute authorship based on stylistic similarities (e.g.,
<a href="https://arxiv.org/pdf/2209.11717.pdf" class="external-link">Agrawal et al., 2023</a>
and <a href="https://arxiv.org/pdf/1704.00177v1.pdf" class="external-link">Liu,
2017</a>).</li>
<li>
<strong>Authorship Attribution</strong>: Word2Vec has been applied
to authorship attribution tasks (e.g., <a href="https://arxiv.org/abs/2310.16972" class="external-link">Tripto and Ali, 2023</a>).</li>
<li>
<strong>Comparative Analysis of Multilingual Texts</strong>:
Word2Vec enables cross-lingual comparisons. Researchers have explored
multilingual embeddings to study semantic differences between languages
(e.g., <a href="https://arxiv.org/pdf/1912.10169.pdf" class="external-link">Heijden et al.,
2019</a>).</li>
<li>
<strong>Studying Cultural Concepts and Biases</strong>: Word2Vec
helps uncover cultural biases in language. Researchers have examined
biases related to race, religion, and colonialism (e.g., <a href="https://link.springer.com/article/10.1007/s00146-022-01443-w" class="external-link">Petreski
and Hashim, 2022</a>).</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="preliminary-considerations">Preliminary Considerations<a class="anchor" aria-label="anchor" href="#preliminary-considerations"></a>
</h2>
<hr class="half-width">
<p>In determining whether or not Word2Vec is a suitable embedding method
for your research, it’s important to consider the following:</p>
<ul>
<li>
<strong>Analysis Relevance</strong>: Does examining the
relationships and meanings among words serve as a guideline for your
research? Are you able to pinpoint specific terms or clusters of terms
that encapsulate the broader conceptual realms you are
investigating?</li>
<li>
<strong>Data Quality</strong>: Ensure that your text corpus is of
high quality. Garbage or noisy data can adversely affect Word2Vec
embeddings.</li>
<li>
<strong>Corpus Size</strong>: Word2Vec performs better with larger
corpora. Having substantial text data improves the quality of learned
word vectors.</li>
<li>
<strong>Domain-Specific Data Availability</strong>: Choose a dataset
relevant to your DH research. If you’re analyzing historical texts, use
historical documents. For sentiment analysis, domain-specific data
matters.</li>
</ul></section><section><h2 class="section-heading" id="exploring-word2vec-in-python">Exploring Word2Vec in Python<a class="anchor" aria-label="anchor" href="#exploring-word2vec-in-python"></a>
</h2>
<hr class="half-width">
<p>With that said, let’s see what we can do with meaningful word
vectors. The pre-trained model we loaded earlier was trained on a Google
News dataset (about 100 billion words). We loaded this model as the
variable <code>wv</code> earlier. Let’s check the type of this
object.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(wv))</span></code></pre>
</div>
<p>Gensim stores “KeyedVectors” representing the Word2Vec model. They’re
called keyed vectors because you can use words as keys to extract the
corresponding vectors. Let’s take a look at the vector representaton of
<em>whale</em>.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>wv[<span class="st">'whale'</span>] </span></code></pre>
</div>
<p>We can also check the shape of this vector with…</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="bu">print</span>(wv[<span class="st">'whale'</span>].shape) </span></code></pre>
</div>
<p>In this model, each word has a 300-dimensional representation. You
can think of these 300 dimensions as 300 different features that encode
a word’s meaning. Unlike LSA, which produces (somewhat) interpretable
features (i.e., topics) relevant to a text, the features produced by
Word2Vec will be treated as a black box. That is, we won’t actually know
what each dimension of the vector represents. However, if the vectors
have certain desirable properties (e.g., similar words produce similar
vectors), they can still be very useful. Let’s check this with the help
of the cosine similarity measure.</p>
<p><strong>Cosine Similarity (Review)</strong>: Recall from earlier in
the workshop that cosine similarity helps evaluate vector similarity in
terms of the angle that separates the two vectors, irrespective of
vector magnitude. It can take a value ranging from -1 to 1, with…</p>
<ul>
<li>1 indicating that the two vectors share the same angle</li>
<li>0 indicating that the two vectors are perpendicular or 90 degrees to
one another</li>
<li>-1 indicating that the two vectors are 180 degrees apart.</li>
</ul>
<p>Words that occur in similar contexts should have similar
vectors/embeddings. How similar are the word vectors representing
<em>whale</em> and <em>dolphin</em>?</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>wv.similarity(<span class="st">'whale'</span>,<span class="st">'dolphin'</span>)</span></code></pre>
</div>
<p>How about <em>whale</em> and <em>fish</em>?</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>wv.similarity(<span class="st">'whale'</span>,<span class="st">'fish'</span>)</span></code></pre>
</div>
<p>How about <em>whale</em> and… <em>potato</em>?</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>wv.similarity(<span class="st">'whale'</span>,<span class="st">'potato'</span>)</span></code></pre>
</div>
<p>Our similarity scale seems to be on the right track. We can also use
the similarity function to quickly extract the top N most similar words
to <em>whale</em>.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>wv.most_similar(positive<span class="op">=</span>[<span class="st">'whale'</span>], topn<span class="op">=</span><span class="dv">10</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('whales', 0.8474178910255432),
 ('humpback_whale', 0.7968777418136597),
 ('dolphin', 0.7711714506149292),
 ('humpback', 0.7535837292671204),
 ('minke_whale', 0.7365031838417053),
 ('humpback_whales', 0.7337379455566406),
 ('dolphins', 0.7213870882987976),
 ('humpbacks', 0.7138717174530029),
 ('shark', 0.7011443376541138),
 ('orca', 0.7007412314414978)]</code></pre>
</div>
<p>Based on our ability to recover similar words, it appears the
Word2Vec embedding method produces fairly good (i.e., semantically
meaningful) word representations.</p>
<div id="exploring-words-with-multiple-meanings" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="exploring-words-with-multiple-meanings" class="callout-inner">
<h3 class="callout-title">Exploring Words With Multiple Meanings</h3>
<div class="callout-content">
<p>Use Gensim’s <code>most_similar</code> function to find the top 10
most similar words to each of the following words (separately): “bark”,
“pitcher”, “park”. Note that all of these words have multiple meanings
depending on their context. Does Word2Vec capture the meaning of these
words well? Why or why not?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>wv.most_similar(positive<span class="op">=</span>[<span class="st">'bark'</span>], topn<span class="op">=</span><span class="dv">15</span>) <span class="co"># all seem to reflect tree bark</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>wv.most_similar(positive<span class="op">=</span>[<span class="st">'park'</span>], topn<span class="op">=</span><span class="dv">15</span>) <span class="co"># all seem to reflect outdoor parks</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>wv.most_similar(positive<span class="op">=</span>[<span class="st">'pitcher'</span>], topn<span class="op">=</span><span class="dv">15</span>) <span class="co"># all seem to reflect baseball pitching</span></span></code></pre>
</div>
<p>Based on these three lists, it looks like Word2Vec is biased towards
representing the predominant meaning or sense of a word. In fact, the
Word2Vec does not explicitly differentiate between multiple meanings of
a word during training. Instead, it treats each occurrence of a word in
the training corpus as a distinct symbol, regardless of its meaning. As
a result, resulting embeddings may be biased towards the most frequent
meaning or sense of a word. This is because the more frequent a word
sense appears in the training data, the more opportunities the algorithm
has to learn that particular meaning.</p>
<p>Note that while this can be a limitation of Word2Vec, there are some
techniques that can be applied to incorporate word sense disambiguation.
One common approach is to train multiple embeddings for a word, where
each embedding corresponds to a specific word sense. This can be done by
pre-processing the training corpus to annotate word senses, and then
training Word2Vec embeddings separately for each sense. This approach
allows Word2Vec to capture different word senses as separate vectors,
effectively representing the polysemy of the word.</p>
</div>
</div>
</div>
</div>
<div id="word2vec-applications-in-digital-humanities" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="word2vec-applications-in-digital-humanities" class="callout-inner">
<h3 class="callout-title">Word2Vec Applications in Digital
Humanities</h3>
<div class="callout-content">
<p>From the above exercise, we see that the vectors produced by Word2Vec
will reflect how words are typically used in a specific dataset. By
training Word2Vec on large corpora of text from historical documents,
literary works, or cultural artifacts, researchers can uncover semantic
relationships between words and analyze word usage patterns over time,
across genres, or within specific cultural contexts.</p>
<p>Taking this into consideration, what are some possible ways we could
make use of Word2Vec to explore newspaper articles from the years
1900-2000?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>One possible approach with this data is to investigate how the
meaning of certain words can evolve over time by training separate
models for different chunks of time (e.g., 1900-1950, 1951-2000, etc.).
A few words that have changed their meaning over time include:</p>
<ul>
<li>Nice: This word used to mean “silly, foolish, simple.”</li>
<li>Silly: In its earliest uses, it referred to things worthy or
blessed; from there it came to refer to the weak and vulnerable, and
more recently to those who are foolish.</li>
<li>Awful: Awful things used to be “worthy of awe”.</li>
</ul>
<p>We’ll explore how training a Word2Vec model on specific texts can
yield insights into those texts later in this lesson.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="adding-and-subtracting-vectors-king---man-woman-queen">Adding and Subtracting Vectors: King - Man + Woman = Queen<a class="anchor" aria-label="anchor" href="#adding-and-subtracting-vectors-king---man-woman-queen"></a>
</h3>
<p>We can also add and subtract word vectors to reveal latent meaning in
words. As a canonical example, let’s see what happens if we take the
word vector representing <em>King</em>, subtract the <em>Man</em> vector
from it, and then add the <em>Woman</em> vector to the result. We should
get a new vector that closely matches the word vector for
<em>Queen</em>. We can test this idea out in Gensim with:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="bu">print</span>(wv.most_similar(positive<span class="op">=</span>[<span class="st">'woman'</span>,<span class="st">'king'</span>], negative<span class="op">=</span>[<span class="st">'man'</span>], topn<span class="op">=</span><span class="dv">3</span>))</span></code></pre>
</div>
<p>Behind the scenes of the most_similar function, Gensim first unit
normalizes the <em>length</em> of all vectors included in the positive
and negative function arguments. This is done before adding/subtracting,
which prevents longer vectors from unjustly skewing the sum. Note that
length here refers to the linear algebraic definition of summing the
squared values of each element in a vector followed by taking the square
root of that sum.</p>
</div>
<div class="section level3">
<h3 id="visualizing-word-vectors-with-pca">Visualizing word vectors with PCA<a class="anchor" aria-label="anchor" href="#visualizing-word-vectors-with-pca"></a>
</h3>
<p>Similar to how we visualized our texts in the previous lesson to show
how they relate to one another, we can visualize how a sample of words
relate by plotting their respecitve word vectors.</p>
<p>Let’s start by extracting some word vectors from the pre-trained
Word2Vec model.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>words <span class="op">=</span> [<span class="st">'man'</span>,<span class="st">'woman'</span>,<span class="st">'boy'</span>,<span class="st">'girl'</span>,<span class="st">'king'</span>,<span class="st">'queen'</span>,<span class="st">'prince'</span>,<span class="st">'princess'</span>]</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>sample_vectors <span class="op">=</span> np.array([wv[word] <span class="cf">for</span> word <span class="kw">in</span> words])</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>sample_vectors.shape <span class="co"># 8 words, 300 dimensions </span></span></code></pre>
</div>
<p>Recall that each word vector has 300 dimensions that encode a word’s
meaning. Considering humans can only visualize up to 3 dimensions, this
dataset presents a plotting challenge. We could certainly try plotting
just the first 2 dimensions or perhaps the dimensions with the largest
amount of variability, but this would overlook a lot of the information
stored in the other dimensions/variables. Instead, we can use a
<em>dimensionality-reduction</em> technique known as Principal Component
Analysis (PCA) to allow us to capture most of the information in the
data with just 2 dimensions.</p>
<div class="section level4">
<h4 id="principal-component-analysis-pca">Principal Component Analysis (PCA)<a class="anchor" aria-label="anchor" href="#principal-component-analysis-pca"></a>
</h4>
<p>Principal Component Analysis (PCA) is a data transformation technique
that allows you to linearly combine a set of variables from a matrix
(<em>N</em> observations and <em>M</em> variables) into a smaller set of
variables called components. Specifically, it remaps the data onto new
dimensions that are strictly orthogonal to one another and can be
ordered according to the amount of information or variance they carry.
The allows you to easily visualize <em>most</em> of the variability in
the data with just a couple of dimensions.</p>
<p>We’ll use scikit-learn’s (a popular machine learning library) PCA
functionality to explore the power of PCA, and matplotlib as our
plotting library.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre>
</div>
<p>In the code below, we will assess how much variance is stored in each
dimension following PCA. The new dimensions are often referred to as
principal components or eigenvectors, which relates to the underlying
math behind this algorithm.</p>
<p>Notice how the first two dimensions capture around 70% of the
variability in the dataset.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>pca <span class="op">=</span> PCA() <span class="co"># init PCA object</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>pca.fit(sample_vectors) <span class="co"># the fit function determines the new dimensions or axes to represent the data -- the result is sent back to the pca object</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a><span class="co"># Calculate cumulative variance explained</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>cumulative_variance_explained <span class="op">=</span> np.cumsum(pca.explained_variance_ratio_)<span class="op">*</span><span class="dv">100</span></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="co"># Plot cumulative variance explained</span></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a>plt.figure()</span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(cumulative_variance_explained) <span class="op">+</span> <span class="dv">1</span>), cumulative_variance_explained, <span class="st">'-o'</span>)</span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of Principal Components"</span>)</span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a>plt.ylabel(<span class="st">"Cumulative Variance Explained (%)"</span>)</span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a>plt.title(<span class="st">"Cumulative Variance Explained by Principal Components"</span>)</span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/wordEmbeddings_word2vecPCA_cumulative_variance_explained.jpg" alt="PCA Variance Explained" class="figure mx-auto d-block"></figure><p>We can now use these new dimensions to transform the original
data.</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="co"># transform the data</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>result <span class="op">=</span> pca.transform(sample_vectors)</span></code></pre>
</div>
<p>Once transformed, we can plot the first two principal components
representing each word in our list:
<code>['man', 'woman', 'boy', 'girl', 'king', 'queen', 'prince', 'princess']</code></p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>plt.figure()</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>plt.scatter(result[:,<span class="dv">0</span>], result[:,<span class="dv">1</span>])</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>  plt.annotate(word, xy<span class="op">=</span>(result[i, <span class="dv">0</span>], result[i, <span class="dv">1</span>]))</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>plt.xlabel(<span class="st">"PC1"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>plt.ylabel(<span class="st">"PC2"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/wordEmbed_PCAviz.jpg" alt="Visualizing Word Embeddings with PCA" class="figure mx-auto d-block"></figure><p>Note how the principal component 1 seems to represent the royalty
dimension, while the principal component 2 seems to represent male vs
female.</p>
</div>
</div>
</section><section><h2 class="section-heading" id="recap">Recap<a class="anchor" aria-label="anchor" href="#recap"></a>
</h2>
<hr class="half-width">
<p>In summary, Word2Vec is a powerful text-embedding method that allows
researchers to explore how different words relate to one another based
on past observations (i.e., by being trained on a large list of
sentences). Unlike LSA, which produces topics as features of the text to
investigate, Word2Vec produces “black-box” features which have to be
compared relative to one another. By training Word2Vec text from
historical documents, literary works, or cultural artifacts, researchers
can uncover semantic relationships between words and analyze word usage
patterns over time, across genres, or within specific cultural
contexts.</p>
<p>In the next section, we’ll explore the technology behind Word2Vec
before training a Word2Vec model on some of the text data used in this
workshop.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Word emebddings can help us derive additional meaning stored in text
at the level of individual words</li>
<li>Word embeddings have many use-cases in text-analysis and NLP related
tasks</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-08-wordEmbed_word2vec-algorithm"><p>Content from <a href="08-wordEmbed_word2vec-algorithm.html">The Word2Vec Algorithm</a></p>
<hr>
<p>Last updated on 2025-05-01 |

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/episodes/08-wordEmbed_word2vec-algorithm.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How does the Word2Vec model produce meaningful word embeddings?</li>
<li>How is a Word2Vec model trained?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Introduce artificial neural networks and their structure.</li>
<li>Understand the two training methods employed by the Word2Vec, CBOW
and Skip-gram.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="related-carpentries-workshops">Related Carpentries workshops<a class="anchor" aria-label="anchor" href="#related-carpentries-workshops"></a>
</h2>
<hr class="half-width">
<p>We could spend an entire workshop on neural networks (see <a href="https://carpentries-incubator.github.io/machine-learning-novice-sklearn/06-neural-networks/index.html" class="external-link">here</a>
and <a href="https://carpentries-incubator.github.io/deep-learning-intro/" class="external-link">here</a>
for a couple of related lessons). Here, we will distill some of the most
important concepts needed to understand them in the context of
text-analysis.</p>
</section><section><h2 class="section-heading" id="mapping-inputs-to-outputs-using-neural-networks">Mapping inputs to outputs using neural networks<a class="anchor" aria-label="anchor" href="#mapping-inputs-to-outputs-using-neural-networks"></a>
</h2>
<hr class="half-width">
<p>How is it that Word2Vec is able to represent words in such a
semantically meaningful way? The key technology behind Word2Vec is an
<strong>artificial neural network</strong>. Neural networks are highly
prevalent in many fields now due to their exceptional ability to learn
functions that can map a set of input features to some output (e.g., a
label or predicted value for some target variable). Because of this
general capability, they can be used for a wide assortment of tasks
including…</p>
<ul>
<li>Predicting the weather tomorrow given historical weather
patterns</li>
<li>Classifying if an email is spam or not</li>
<li>Classifying if an image contains a person or not</li>
<li>Predicting a person’s weight based on their height</li>
<li>Predicting commute times given traffic conditions</li>
<li>Predicting house prices given stock market prices</li>
</ul>
<div class="section level3">
<h3 id="supervised-learning">Supervised learning<a class="anchor" aria-label="anchor" href="#supervised-learning"></a>
</h3>
<p>Most machine learning systems “learn” by taking tabular input data
with N observations (rows), M features (cols), and an associated output
(e.g., a class label or predicted value for some target variable), and
using it to form a model. The maths behind the machine learning doesn’t
care what the data is as long as it can represented numerically or
categorised. When the model learns this function based on observed data,
we call this “training” the model.</p>
<div class="section level4">
<h4 id="training-dataset-example">Training Dataset Example<a class="anchor" aria-label="anchor" href="#training-dataset-example"></a>
</h4>
<p>As a example, maybe we have recorded tail lengths, weights, and snout
lengths from a disorganized vet clinic database that is missing some of
the animals’ labels (e.g., cat vs dog). For simplicity, let’s say that
this vet clinic only treats cats and dogs. With the help of neural
networks, we could use a labelled dataset to learn a function mapping
from tail length, weight, and snout length to the animal’s species label
(i.e., a cat or a dog).</p>
<table class="table">
<colgroup>
<col width="26%">
<col width="36%">
<col width="28%">
<col width="8%">
</colgroup>
<thead><tr class="header">
<th>Tail length (in)</th>
<th>Weight (lbs)</th>
<th>Snout length (in)</th>
<th>Label</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>12.2</td>
<td>10.1</td>
<td>1.1</td>
<td>cat</td>
</tr>
<tr class="even">
<td>11.6</td>
<td>9.8</td>
<td>.82</td>
<td>cat</td>
</tr>
<tr class="odd">
<td>9.5</td>
<td>61.2</td>
<td>2.6</td>
<td>dog</td>
</tr>
<tr class="even">
<td>9.1</td>
<td>65.7</td>
<td>2.9</td>
<td>dog</td>
</tr>
<tr class="odd">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="even">
<td>11.2</td>
<td>12.1</td>
<td>.91</td>
<td>cat</td>
</tr>
</tbody>
</table>
<p>In the above table used to train a neural network model, the model
learns how best to map the observed features (tail length, weight, and
snout length) to their assigned classes. After the model is trained, it
can be used to infer the labels of unlabelled samples (so long as they
hae tail length, weight, and snouth length recorded).</p>
</div>
</div>
</section><section><h2 class="section-heading" id="the-perceptron">The Perceptron<a class="anchor" aria-label="anchor" href="#the-perceptron"></a>
</h2>
<hr class="half-width">
<figure><img src="fig/wordEmbed_NN-perceptron.svg" alt="Single artificial neuron" class="figure mx-auto d-block"></figure><p>The diagram above shows a perceptron — the computational unit that
makes up artificial neural networks. Perceptrons are inspired by real
biological neurons. From the diagram, we can see that the
perceptron…</p>
<ul>
<li>
<strong>Input features</strong>: Receives multiple input features
and returns a single output</li>
<li>
<strong>Weights connecting features</strong>: Has adjustable weights
which scale the impact of individual inputs</li>
<li>
<strong>Nonlinear activation function</strong>: Has a nonlinear
activation function which takes as input, the weighted sum of inputs. If
the sum is above some threshold, the neuron “fires” a signal (outputs -1
or 1 which represents two different class labels)</li>
</ul>
<p>The goal then is to determine what specific weight values will allow
us to separate the two classes based on the input features (e.g., shown
below).</p>
<figure><img src="fig/wordEmbed_NN-cats-dogs-linear-boundary.png" alt="Linear Decision Boundary" class="figure mx-auto d-block"></figure><p><a href="https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/" class="external-link">Image
Source</a></p>
<p>In order to determine the optimal weights, we will need to “train”
the model on a labelled “training” dataset. As we pass each observation
in the training data to the model, the model is able to adjust its
weights in a direction that leads better performance. By training the
model on many observations, we can derive weights that can accurately
classify cats and dogs based on the observed input features. More
explicitly, its training method can be outlined as follows:</p>
<div class="section level3">
<h3 id="training-algorithm">Training algorithm<a class="anchor" aria-label="anchor" href="#training-algorithm"></a>
</h3>
<ol style="list-style-type: decimal">
<li><p><strong>Initialize weights</strong>: The perceptron model starts
with randomly initialized weights. These weights are the
parameters/coefficients that the model will learn during training to
make accurate predictions.</p></li>
<li><p><strong>Input data</strong>: The perceptron model takes in the
input data, which consists of feature vectors representing the input
samples, and their corresponding labels or target values.</p></li>
<li><p><strong>Compute weighted sum</strong>: The model computes the
weighted sum of the input features by multiplying the feature values
with their corresponding weights, and summing them up. This is followed
by adding the bias term.</p></li>
<li><p><strong>Activation function</strong>: The perceptron model
applies an activation function, typically a step function or a threshold
function, to the computed weighted sum. The activation function
determines the output of the perceptron, usually producing a binary
output of 0 or 1.</p></li>
<li><p><strong>Compare with target label</strong>: The output of the
perceptron is compared with the target label of the input sample to
determine the prediction error. If the prediction is correct, no weight
updates are made. If the prediction is incorrect, the weights and bias
are updated to minimize the error.</p></li>
<li><p><strong>Update weights</strong>: The perceptron model updates the
weights based on a learning rate and the prediction error. The learning
rate determines the step size of the weight updates, and it is a
hyperparameter that needs to be tuned. The weights are updated using the
formula:</p></li>
</ol>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>weight_new <span class="op">=</span> weight_old <span class="op">+</span> learning_rate <span class="op">*</span> (target <span class="op">-</span> prediction) <span class="op">*</span> feature</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="perceptron-limitations">Perceptron limitations<a class="anchor" aria-label="anchor" href="#perceptron-limitations"></a>
</h3>
<p>A single perceptron cannot solve any function that is not linearly
separable, meaning that we need to be able to divide the classes of
inputs and outputs with a straight line. To overcome this key limitation
of the perceptron (a single aritifical neuron), we need to stack
together multiple perceptrons in a hierarchical fashion. Such models are
referred to as <em>multilayer perceptrons</em> or simply <em>neural
networks</em></p>
</div>
</section><section><h2 class="section-heading" id="the-multilayer-perceptron-mlp">The multilayer perceptron (MLP)<a class="anchor" aria-label="anchor" href="#the-multilayer-perceptron-mlp"></a>
</h2>
<hr class="half-width">
<p>To overcome the limitation of the perceptron, we can stack together
multiple perceptrons in a multilayer neural network (shown below) called
a multilayer perceptron (MLP). An MLP refers to a type of artificial
neural network (ANN) that consists of multiple layers of interconnected
nodes (neurons) organized in a feedforward manner. It typically has one
or more hidden layers between the input and output layers, with each
hidden layer applying an activation function to the weighted sum of its
inputs. By stacking together layers of perceptrons, the MLP model can
learn complex non-linear relationships in the data and make predictions
based on those learned patterns.</p>
<figure><img src="fig/wordEmbed_NN-MLP.svg" alt="Multilayer neural network" class="figure mx-auto d-block"></figure><p>In the diagram above, the general structure of a multilayer neural
network is shown with…</p>
<ul>
<li>
<strong>Input Layer</strong>: The input layer is the first layer of
the MLP and consists of input nodes that receive the features of the
input data. Each node in the input layer represents a feature or
attribute of the input data. The input layer is not involved in any
computation or activation; it simply passes the input features to the
next layer.</li>
<li>
<strong>Hidden Layer(s)</strong>: The hidden layers are the
intermediate layers between the input and output layers. In the above
diagram, there is only 1 hidden layer, but MLPs often have more. They
are called “hidden” because their outputs are not directly visible in
the input or output data. Each hidden layer consists of multiple nodes
(neurons) that compute a weighted sum of the inputs from the previous
layer, followed by an activation function. The number of hidden layers
and the number of nodes in each hidden layer are hyperparameters that
can be tuned during model design and training.</li>
<li>
<strong>Weighted Connections</strong>: Each connection between nodes
in adjacent layers has a weight associated with it. These weights are
the parameters that the model learns during training to determine the
strength of the connections. The weighted sum of inputs to a node is
computed by multiplying the input values with their corresponding
weights and summing them up. Also referred to as “weights” for
short.</li>
<li>
<strong>Weights</strong>: The weights of each neuron send its
(weighted) output to each neuron in the subsequent layer</li>
<li>
<strong>Output Layer</strong>: The output layer is the last layer of
the MLP and produces the final output of the model. It typically
consists of one or more nodes, depending on the specific task. For
binary classification, a single output node with a sigmoid activation
function is commonly used. For multi-class classification, multiple
output nodes with a softmax activation function are used. For regression
tasks, a single output node with a linear activation function is often
used.</li>
</ul>
<div class="section level3">
<h3 id="training-algorithm-1">Training algorithm<a class="anchor" aria-label="anchor" href="#training-algorithm-1"></a>
</h3>
<p>Similar to the perceptron, the MLP is trained using a supervised
learning algorithm that updates the weights iteratively based on the
prediction error of each training sample.</p>
<ol style="list-style-type: decimal">
<li>
<strong>Initialization</strong>: The network’s weights are randomly
initialized.</li>
<li>
<strong>Forward Propagation</strong>: Input data is fed through the
network from input nodes to output nodes, with weights applied at each
connection, and the output is computed.</li>
<li>
<strong>Error Calculation</strong>: The difference between the
predicted output and the actual output (target) is calculated as the
error.</li>
<li>
<strong>Backpropagation</strong>: The error is propagated backward
through the network, and the weights are adjusted to minimize the
error.</li>
<li>
<strong>Iterative Process</strong>: Steps 2-4 are repeated for
multiple iterations or epochs, with input data fed through the network
and weights updated until the network’s performance converges to a
satisfactory level.</li>
<li>
<strong>Function Mapping</strong>: Once the network is trained, it
can be used to map new input data to corresponding outputs, leveraging
the learned weights.</li>
</ol>
</div>
</section><section><h2 class="section-heading" id="deriving-new-features-from-neural-networks">Deriving New Features from Neural Networks<a class="anchor" aria-label="anchor" href="#deriving-new-features-from-neural-networks"></a>
</h2>
<hr class="half-width">
<p>After training a neural network, the neural weights encode new
features of the data that are conducive to performing well on whatever
task the neural network is given. This is due to the feedforward
processing built into the network — the outputs of previous layers are
sent to subsequent layers, and the so additional transformations get
applied to the original inputs as they transcend the network.</p>
<p>Generally speaking, the deeper the neural network is, the more
complicated/abstract these features can become. We call this a
<strong>hierarchical feature representation</strong>. For example, in
deep convolutional neural networks (a special kind of neural network
designed for image processing), the features in each layer look
something like the image shown below when the model is trained on a
facial recognition task.</p>
<figure><img src="fig/wordEmbed_NN-hierarchical-features.png" alt="Hierarchical Feature Representations - Face Detection" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="training-word2vec-to-learn-word-embeddings">Training Word2Vec to Learn Word Embeddings<a class="anchor" aria-label="anchor" href="#training-word2vec-to-learn-word-embeddings"></a>
</h2>
<hr class="half-width">
<p>Recall that the ultimate goal of the Word2Vec method is to output
meaningful word embeddings/vectors. How can we train a neural network
for such a task? We could try to tediously hand-craft a large list of
word vectors that have the properties we seek (e.g., similar words have
similar vectors), and then train a neural network to learn this mapping
before applying it to new words. However, crafting a list of vectors
manually would be an arudous task. Furthermore, it is not immediately
clear what kind of vector representation would be best.</p>
<p>Instead, we can capitalize on the fact that neural networks are well
posed to learn new features from the input data. Specifically, the new
features will be features that are useful for whatever task the model is
assigned. With this consideration, we can devise a language related task
that allows a neural network model to learn interesting features of
words which can then be extracted from the model as a word embedding
representation (i.e., a vector). We’ll unpack how the embedding gets
extracted from the trained model shortly. For now, let’s focus on what
kind of language-related task to give the model.</p>
<div class="section level3">
<h3 id="predicting-context-words">Predicting context words<a class="anchor" aria-label="anchor" href="#predicting-context-words"></a>
</h3>
<p>What task can we give a neural network to learn meaningful word
embeddings? Our friend RJ Firth gives us a hint when he says, “You shall
know a word by the company it keeps.” Using the <em>distributional
hypothesis</em> as motivation, which states that words that repeatedly
occur in similar contexts probably have similar meanings, we can ask a
neural network to predict the <em>context</em> words that surround a
given word in a sentence. The Skip-gram algorithm shown on the right
side of the below diagram does just that.</p>
<figure><img src="fig/wordEmbed_NN-training-methods.png" alt="Skipgram" class="figure mx-auto d-block"></figure><div class="section level4">
<h4 id="sentence-processing-with-skip-gram">Sentence Processing With Skip-Gram<a class="anchor" aria-label="anchor" href="#sentence-processing-with-skip-gram"></a>
</h4>
<p>The Skip-gram version takes as input each word in a sentence, and
tries to guess the most likely surrounding context words associated with
that word. It does this for all sentences and words in a corpus in order
to learn a function that can map each word to its most likely context
words.</p>
<p><em>Have a very nice day.</em></p>
<table class="table">
<thead><tr class="header">
<th>Input</th>
<th>Output (context words)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Have</td>
<td>a, very</td>
</tr>
<tr class="even">
<td>a</td>
<td>Have, very, nice</td>
</tr>
<tr class="odd">
<td>very</td>
<td>Have, a, nice, day</td>
</tr>
<tr class="even">
<td>nice</td>
<td>a, very, day</td>
</tr>
<tr class="odd">
<td>day</td>
<td>very, nice</td>
</tr>
</tbody>
</table>
<p>In the process of training, the model’s weights learn to derive new
features (weight optimized perceptrons) associated with the input data
(single words). These new learned features will be conducive to
accurately predicting the context words for each word. We will see next
how we can extract these features as word vectors.</p>
</div>
</div>
<div class="section level3">
<h3 id="extracting-word-embeddings-from-the-model">Extracting Word Embeddings From the Model<a class="anchor" aria-label="anchor" href="#extracting-word-embeddings-from-the-model"></a>
</h3>
<p>With a model trained to predict context words, how can we extract the
model’s learned features as word embeddings? For this, we need a set of
model weights associated with each word fed into the model. We can
achieve this property by:</p>
<ol style="list-style-type: decimal">
<li>Converting each input word into a one-hot encoded vector
representation. The vector dimensionality will be equal to the size of
the vocabularly contained in the training data.</li>
<li>Connecting each element of the one-hot encoded vector to each
node/neuron in the subsequent hidden layer of neurons</li>
</ol>
<p>These steps can be visualized in the Word2Vec model diagram shown
below, with Sigmas representing individual neurons and their ability to
integrate input from previous layers.</p>
<figure><img src="fig/wordEmbed_NN-SG-model-architecture.png" alt="Word2Vec Model Architecture (Skip-gram)" class="figure mx-auto d-block"></figure><p><a href="https://israelg99.github.io/2017-03-23-Word2Vec-Explained/#:~:text=How%20does%20Word2Vec%20produce%20word,to%20reduce%20a%20loss%20function." class="external-link">Image
Source</a></p>
<p>In the above digram, we can see…</p>
<ul>
<li>The input layer has 10,000 dimensions representing 10,000 words in
this model’s vocabulary</li>
<li>The hidden layer of the model has 300 neurons. Note that this number
also corresponds to the dimensionality of the word vectors extracted
from the model.</li>
<li>The output layer has one neuron for each possible word in the
model’s vocabulary</li>
</ul>
<p>The word vectors, themselves, are stored in the weights connecting
the input layer to the hidden layer of neurons. Each word will have its
own set of learned weights which we call word vectors. You can think of
each element of the word vectors as encoding different features which
are relevant to the prediction task at hand — predicting context
words.</p>
<div class="section level4">
<h4 id="continuous-bag-of-words-cbow">
<strong>Continuous Bag-of-Words (CBOW)</strong><a class="anchor" aria-label="anchor" href="#continuous-bag-of-words-cbow"></a>
</h4>
<figure><img src="fig/wordEmbed_NN-training-methods.png" alt="Image from Word2Vec research paper, by Mikolov et al" class="figure mx-auto d-block"></figure><p>Before wrapping up with the mechanisms underlying the Word2Vec model,
it is important to mention that the Skip-gram algorithm is not the only
way to train word embeddings using Word2Vec. A similar method known as
the Continuous Bag-of-Words (CBOW) takes as an input the context words
surrounding a target word, and tries to guess the target word based on
those words. Thus, it flips the prediction task faced by Skip-gram. The
CBOW algorithm does not care how far away different context words are
from the target word, which is why it is called a bag-of-words method.
With this task setup, the neural network will learn a function that can
map the surrounding context words to a target word. Similar to
Skip-gram, the CBOW method will generate word vectors stored as weights
of the neural network. However, given the slight adjustment in task, the
weights extracted from CBOW are the ones that connect the hidden layer
of neurons to the output layer.</p>
</div>
</div>
<div class="section level3">
<h3 id="cbow-vs-skip-gram">CBOW vs Skip-gram<a class="anchor" aria-label="anchor" href="#cbow-vs-skip-gram"></a>
</h3>
<p>Since there are two popular word2vec training methods, how should we
decide which one to pick? Like with many things in machine learning, the
best course of action is typically to take a data-driven approach to see
which one works better for your specific application. However, as
general guidelines according to Mikolov et al.,</p>
<ol style="list-style-type: decimal">
<li>Skip-Gram works well with smaller datasets and has been found to
perform better in terms of its ability to represent rarer words</li>
<li>CBOW trains several times faster than Skip-gram and has slightly
better accuracy for more frequent words</li>
</ol>
</div>
</section><section><h2 class="section-heading" id="recap">Recap<a class="anchor" aria-label="anchor" href="#recap"></a>
</h2>
<hr class="half-width">
<p>Artificial neural networks are powerful machine learning models that
can learn to map input data containing features to a predicted label or
continuous value. In addition, neural networks learn to encode the input
data as hierarchical features of the text during training. The Word2Vec
model exploits this capability, and trains the model on a word
prediction task in order to generate features of words which are
conducive to the prediction task at hand.</p>
<p>In the next episode, we’ll train a Word2Vec model using both training
methods and empirically evaluate the performance of each. We’ll also see
how training Word2Vec models from scratch (rather than using a
pretrained model) can be beneficial in some circumstances.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Artificial neural networks (ANNs) are powerful models that can
approximate any function given sufficient training data.</li>
<li>The best method to decide between training methods (CBOW and
Skip-gram) is to try both methods and see which one works best for your
specific application.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-09-wordEmbed_train-word2vec"><p>Content from <a href="09-wordEmbed_train-word2vec.html">Training Word2Vec</a></p>
<hr>
<p>Last updated on 2025-05-01 |

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/episodes/09-wordEmbed_train-word2vec.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can we train a Word2Vec model?</li>
<li>When is it beneficial to train a Word2Vec model on a specific
dataset?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the benefits of training a Word2Vec model on your own
data rather than using a pre-trained model</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="colab-setup">Colab Setup<a class="anchor" aria-label="anchor" href="#colab-setup"></a>
</h2>
<hr class="half-width">
<p>Run this code to enable helper functions and read data back in.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># Run this cell to mount your Google Drive.</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>drive.mount(<span class="st">'/content/drive'</span>)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># Show existing colab notebooks and helpers.py file</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> os <span class="im">import</span> listdir</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>wksp_dir <span class="op">=</span> <span class="st">'/content/drive/My Drive/Colab Notebooks/text-analysis/code'</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="bu">print</span>(listdir(wksp_dir))</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co"># Add folder to colab's path so we can import the helper functions</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>sys.path.insert(<span class="dv">0</span>, wksp_dir)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># pip install necessary to access parse module (called from helpers.py)</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="op">!</span>pip install parse</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="install-and-import-gensim">Install and import Gensim<a class="anchor" aria-label="anchor" href="#install-and-import-gensim"></a>
</h2>
<hr class="half-width">
<p>Install gensim again.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="op">!</span>pip uninstall <span class="op">-</span>y numpy gensim</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="op">!</span>rm <span class="op">-</span>rf <span class="op">/</span>usr<span class="op">/</span>local<span class="op">/</span>lib<span class="op">/</span>python3<span class="fl">.11</span><span class="op">/</span>dist<span class="op">-</span>packages<span class="op">/</span>numpy<span class="op">*</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">--</span>no<span class="op">-</span>cache<span class="op">-</span><span class="bu">dir</span> numpy<span class="op">==</span><span class="fl">1.26.4</span> gensim<span class="op">==</span><span class="fl">4.3.3</span></span></code></pre>
</div>
<p>Test to make sure we can import Word2Vec from gensim. You may need to
restart the kernel after running the above cell before gensim will
successfully import.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># import gensim's Word2Vec module</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span></code></pre>
</div>
<div class="section level3">
<h3 id="load-in-the-data">Load in the data<a class="anchor" aria-label="anchor" href="#load-in-the-data"></a>
</h3>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Read the data back in.</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> read_csv</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>data <span class="op">=</span> read_csv(<span class="st">"/content/drive/My Drive/Colab Notebooks/text-analysis/data/data.csv"</span>)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>data.head()</span></code></pre>
</div>
<p>Create list of files we’ll use for our analysis. We’ll start by
fitting a word2vec model to just one of the books in our list — Moby
Dick.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>single_file <span class="op">=</span> data.loc[data[<span class="st">'title'</span>] <span class="op">==</span> <span class="st">'moby_dick'</span>,<span class="st">'File'</span>].item()</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>single_file</span></code></pre>
</div>
<p>Let’s preview the file contents to make sure our code and directory
setup is working correctly.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># open and read file</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>f <span class="op">=</span> <span class="bu">open</span>(single_file,<span class="st">'r'</span>)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>file_contents <span class="op">=</span> f.read()</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>f.close()</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="co"># preview file contents</span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>preview_len <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="bu">print</span>(file_contents[<span class="dv">0</span>:preview_len])</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>file_contents[<span class="dv">0</span>:preview_len] <span class="co"># Note that \n are still present in actual string (print() processes these as new lines)</span></span></code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="preprocessing-steps">Preprocessing steps<a class="anchor" aria-label="anchor" href="#preprocessing-steps"></a>
</h2>
<hr class="half-width">
<ol style="list-style-type: decimal">
<li>Split text into sentences</li>
<li>Tokenize the text</li>
<li>Lemmatize and lowercase all tokens</li>
<li>Remove stop words</li>
</ol>
<div class="section level3">
<h3 id="convert-text-to-list-of-sentences">1. Convert text to list of sentences<a class="anchor" aria-label="anchor" href="#convert-text-to-list-of-sentences"></a>
</h3>
<p>Remember that we are using the sequence of words in a sentence to
learn meaningful word embeddings. The last word of one sentence does not
always relate to the first word of the next sentence. For this reason,
we will split the text into individual sentences before going
further.</p>
<div class="section level4">
<h4 id="punkt-sentence-tokenizer">Punkt Sentence Tokenizer<a class="anchor" aria-label="anchor" href="#punkt-sentence-tokenizer"></a>
</h4>
<p>NLTK’s sentence tokenizer (‘punkt’) works well in most cases, but it
may not correctly detect sentences when there is a complex paragraph
that contains many punctuation marks, exclamation marks, abbreviations,
or repetitive symbols. It is not possible to define a standard way to
overcome these issues. If you want to ensure every “sentence” you use to
train the Word2Vec is truly a sentence, you would need to write some
additional (and highly data-dependent) code that uses regex and string
manipulation to overcome rare errors.</p>
<p>For our purposes, we’re willing to overlook a few sentence
tokenization errors. If this work were being published, it would be
worthwhile to double-check the work of punkt.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>nltk.download(<span class="st">'punkt'</span>) <span class="co"># dependency of sent_tokenize function</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>nltk.download(<span class="st">'punkt_tab'</span>)  <span class="co"># needed for punkt in certain environments</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>sentences <span class="op">=</span> nltk.sent_tokenize(file_contents)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>sentences[<span class="dv">300</span>:<span class="dv">305</span>]</span></code></pre>
</div>
</div>
</div>
<div class="section level3">
<h3 id="tokenize-lemmatize-and-remove-stop-words">2-4: Tokenize, lemmatize, and remove stop words<a class="anchor" aria-label="anchor" href="#tokenize-lemmatize-and-remove-stop-words"></a>
</h3>
<p>Pull up preprocess text helper function and unpack the code…</p>
<ul>
<li>We’ll run this function on each sentence</li>
<li>Lemmatization, tokenization, lowercase and stopwords are all
review</li>
<li>For the lemmatization step, we’ll use NLTK’s lemmatizer which runs
very quickly</li>
<li>We’ll also use NLTK’s stop word lists and its tokenization function.
Recall that stop words are usually thought of as the most common words
in a language. By removing them, we can let the Word2Vec model focus on
sequences of meaningful words, only.</li>
</ul>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">from</span> helpers <span class="im">import</span> preprocess_text</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># test function</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>string <span class="op">=</span> <span class="st">'It is not down on any map; true places never are.'</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>tokens <span class="op">=</span> preprocess_text(string, </span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>                         remove_stopwords<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>                         verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Result'</span>, tokens)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="co"># convert list of sentences to pandas series so we can use the apply functionality</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>sentences_series <span class="op">=</span> pd.Series(sentences)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>tokens_cleaned <span class="op">=</span> sentences_series.<span class="bu">apply</span>(preprocess_text, </span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>                                        remove_stopwords<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>                                        verbose<span class="op">=</span><span class="va">False</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="co"># view sentences before clearning</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>sentences[<span class="dv">300</span>:<span class="dv">305</span>]</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="co"># view sentences after cleaning</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>tokens_cleaned[<span class="dv">300</span>:<span class="dv">305</span>]</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>tokens_cleaned.shape <span class="co"># 9852 sentences</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="co"># remove empty sentences and 1-word sentences (all stop words)</span></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>tokens_cleaned <span class="op">=</span> tokens_cleaned[tokens_cleaned.<span class="bu">apply</span>(<span class="bu">len</span>) <span class="op">&gt;</span> <span class="dv">1</span>]</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>tokens_cleaned.shape</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="train-word2vec-model-using-tokenized-text">Train Word2Vec model using tokenized text<a class="anchor" aria-label="anchor" href="#train-word2vec-model-using-tokenized-text"></a>
</h3>
<p>We can now use this data to train a word2vec model. We’ll start by
importing the Word2Vec module from gensim. We’ll then hand the Word2Vec
function our list of tokenized sentences and set sg=0 (“skip-gram”) to
use the continuous bag of words (CBOW) training method.</p>
<p><strong>Set seed and workers for a fully deterministic run</strong>:
Next we’ll set some parameters for reproducibility. We’ll set the seed
so that our vectors get randomly initialized the same way each time this
code is run. For a fully deterministically-reproducible run, we’ll also
limit the model to a single worker thread (workers=1), to eliminate
ordering jitter from OS thread scheduling — noted in <a href="https://radimrehurek.com/gensim/models/word2vec.html" class="external-link">gensim’s
documentation</a></p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="co"># import gensim's Word2Vec module</span></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a><span class="co"># train the word2vec model with our cleaned data</span></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(sentences<span class="op">=</span>tokens_cleaned, seed<span class="op">=</span><span class="dv">0</span>, workers<span class="op">=</span><span class="dv">1</span>, sg<span class="op">=</span><span class="dv">0</span>)</span></code></pre>
</div>
<p>Gensim’s implementation is based on the original <a href="%22https://arxiv.org/pdf/1301.3781.pdf%22" class="external-link">Tomas Mikolov’s
original model of word2vec</a>, which downsamples all frequent words
automatically based on frequency. The downsampling saves time when
training the model.</p>
</div>
<div class="section level3">
<h3 id="next-steps-word-embedding-use-cases">Next steps: word embedding use-cases<a class="anchor" aria-label="anchor" href="#next-steps-word-embedding-use-cases"></a>
</h3>
<p>We now have a vector representation for all the (lemmatized and
non-stop words) words referenced throughout Moby Dick. Let’s see how we
can use these vectors to gain insights from our text data.</p>
</div>
<div class="section level3">
<h3 id="most-similar-words">Most similar words<a class="anchor" aria-label="anchor" href="#most-similar-words"></a>
</h3>
<p>Just like with the pretrained word2vec models, we can use the
most_similar function to find words that meaningfully relate to a
queried word.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="co"># default</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>model.wv.most_similar(positive<span class="op">=</span>[<span class="st">'whale'</span>], topn<span class="op">=</span><span class="dv">10</span>)</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="vocabulary-limits">Vocabulary limits<a class="anchor" aria-label="anchor" href="#vocabulary-limits"></a>
</h3>
<p>Note that Word2Vec can only produce vector representations for words
encountered in the data used to train the model.</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>model.wv.most_similar(positive<span class="op">=</span>[<span class="st">'orca'</span>],topn<span class="op">=</span><span class="dv">30</span>) <span class="co"># KeyError: "Key 'orca' not present in vocabulary"</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="fasttext-solves-oov-issue">fastText solves OOV issue<a class="anchor" aria-label="anchor" href="#fasttext-solves-oov-issue"></a>
</h3>
<p>If you need to obtain word vectors for out of vocabulary (OOV) words,
you can use the fastText word embedding model, instead (also provided
from Gensim). The fastText model can obtain vectors even for
out-of-vocabulary (OOV) words, by summing up vectors for its component
char-ngrams, provided at least one of the char-ngrams was present in the
training data.</p>
</div>
<div class="section level3">
<h3 id="word2vec-for-named-entity-recognition">Word2Vec for Named Entity Recognition<a class="anchor" aria-label="anchor" href="#word2vec-for-named-entity-recognition"></a>
</h3>
<p>What can we do with this most similar functionality? One way we can
use it is to construct a list of similar words to represent some sort of
category. For example, maybe we want to know what other sea creatures
are referenced throughout Moby Dick. We can use gensim’s most_smilar
function to begin constructing a list of words that, on average,
represent a “sea creature” category.</p>
<p>We’ll use the following procedure: 1. Initialize a small list of
words that represent the category, sea creatures. 2. Calculate the
average vector representation of this list of words 3. Use this average
vector to find the top N most similar vectors (words) 4. Review similar
words and update the sea creatures list 5. Repeat steps 1-4 until no
additional sea creatures can be found</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="co"># start with a small list of words that represent sea creatures </span></span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>sea_creatures <span class="op">=</span> [<span class="st">'whale'</span>,<span class="st">'fish'</span>,<span class="st">'creature'</span>,<span class="st">'animal'</span>]</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a><span class="co"># The below code will calculate an average vector of the words in our list, </span></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a><span class="co"># and find the vectors/words that are most similar to this average vector</span></span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a>model.wv.most_similar(positive<span class="op">=</span>sea_creatures, topn<span class="op">=</span><span class="dv">30</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="co"># we can add shark to our list</span></span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>model.wv.most_similar(positive<span class="op">=</span>[<span class="st">'whale'</span>,<span class="st">'fish'</span>,<span class="st">'creature'</span>,<span class="st">'animal'</span>,<span class="st">'shark'</span>],topn<span class="op">=</span><span class="dv">30</span>) </span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="co"># add leviathan (sea serpent) to our list</span></span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>model.wv.most_similar(positive<span class="op">=</span>[<span class="st">'whale'</span>,<span class="st">'fish'</span>,<span class="st">'creature'</span>,<span class="st">'animal'</span>,<span class="st">'shark'</span>,<span class="st">'leviathan'</span>],topn<span class="op">=</span><span class="dv">30</span>) </span></code></pre>
</div>
<p>No additional sea creatures. It appears we have our list of sea
creatures recovered using Word2Vec</p>
<div class="section level4">
<h4 id="limitations">Limitations<a class="anchor" aria-label="anchor" href="#limitations"></a>
</h4>
<p>There is at least one sea creature missing from our list — a giant
squid. The giant squid is only mentioned a handful of times throughout
Moby Dick, and therefore it could be that our word2vec model was not
able to train a good representation of the word “squid”. Neural networks
only work well when you have lots of data</p>
<div id="exploring-the-skip-gram-algorithm" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="exploring-the-skip-gram-algorithm" class="callout-inner">
<h3 class="callout-title">Exploring the skip-gram algorithm</h3>
<div class="callout-content">
<p>The skip-gram algoritmm sometimes performs better in terms of its
ability to capture meaning of rarer words encountered in the training
data. Train a new Word2Vec model using the skip-gram algorithm, and see
if you can repeat the above categorical search task to find the word,
“squid”.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="co"># import gensim's Word2Vec module</span></span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a><span class="co"># train the word2vec model with our cleaned data</span></span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(sentences<span class="op">=</span>tokens_cleaned, seed<span class="op">=</span><span class="dv">0</span>, workers<span class="op">=</span><span class="dv">1</span>, sg<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a>model.wv.most_similar(positive<span class="op">=</span>[<span class="st">'whale'</span>,<span class="st">'fish'</span>,<span class="st">'creature'</span>,<span class="st">'animal'</span>,<span class="st">'shark'</span>,<span class="st">'leviathan'</span>],topn<span class="op">=</span><span class="dv">100</span>) <span class="co"># still no sight of squid </span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('whalemen', 0.9931729435920715),
 ('specie', 0.9919217824935913),
 ('bulk', 0.9917919635772705),
 ('ground', 0.9913252592086792),
 ('skeleton', 0.9905602931976318),
 ('among', 0.9898401498794556),
 ('small', 0.9887762665748596),
 ('full', 0.9885162115097046),
 ('captured', 0.9883950352668762),
 ('found', 0.9883666634559631),
 ('sometimes', 0.9882548451423645),
 ('snow', 0.9880553483963013),
 ('magnitude', 0.9880378842353821),
 ('various', 0.9878063201904297),
 ('hump', 0.9876748919487),
 ('cuvier', 0.9875931739807129),
 ('fisherman', 0.9874721765518188),
 ('general', 0.9873012900352478),
 ('living', 0.9872495532035828),
 ('wholly', 0.9872384667396545),
 ('bone', 0.987160861492157),
 ('mouth', 0.9867696762084961),
 ('natural', 0.9867129921913147),
 ('monster', 0.9865870475769043),
 ('blubber', 0.9865683317184448),
 ('indeed', 0.9864518046379089),
 ('teeth', 0.9862186908721924),
 ('entire', 0.9861844182014465),
 ('latter', 0.9859246015548706),
 ('book', 0.9858523607254028)]</code></pre>
</div>
<p><strong>Discuss Exercise Result</strong>: When using Word2Vec to
reveal items from a category, you risk missing items that are rarely
mentioned. This is true even when we use the Skip-gram training method,
which has been found to have better performance on rarer words. For this
reason, it’s sometimes better to save this task for larger text
corpuses. In a later lesson, we will explore how large language models
(LLMs) can yield better performance on Named Entity Recognition related
tasks.</p>
</div>
</div>
</div>
</div>
<div id="entity-recognition-applications" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="entity-recognition-applications" class="callout-inner">
<h3 class="callout-title">Entity Recognition Applications</h3>
<div class="callout-content">
<p>How else might you exploit this kind of analysis in your research?
Share your ideas with the group.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p><strong>Example</strong>: Train a model on newspaper articles from
the 19th century, and collect a list of foods (the topic chosen)
referenced throughout the corpus. Do the same for 20th century newspaper
articles and compare to see how popular foods have changed over
time.</p>
</div>
</div>
</div>
</div>
<div id="comparing-vector-representations-across-authors" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="comparing-vector-representations-across-authors" class="callout-inner">
<h3 class="callout-title">Comparing Vector Representations Across
Authors</h3>
<div class="callout-content">
<p>Recall that the Word2Vec model learns to encode a word’s
meaning/representation based on that word’s most common surrounding
context words. By training two separate Word2Vec models on, e.g., books
collected from two different authors (one model for each author), we can
compare how the different authors tend to use words differently. What
are some research questions or words that we could investigate with this
kind of approach?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>As one possible approach, we could compare how authors tend to
represent different genders. It could be that older (outdated!) books
tend to produce word vectors for man and women that are further apart
from one another than newer books due to historic gender norms.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="other-word-embedding-models">Other word embedding models<a class="anchor" aria-label="anchor" href="#other-word-embedding-models"></a>
</h3>
<p>While Word2Vec is a famous model that is still used throughout many
NLP applications today, there are a few other word embedding models that
you might also want to consider exploring. GloVe and fastText are among
the two most popular choices to date.</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="co"># Preview other word embedding models available</span></span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>(api.info()[<span class="st">'models'</span>].keys()))</span></code></pre>
</div>
<div class="section level4">
<h4 id="similarities">Similarities<a class="anchor" aria-label="anchor" href="#similarities"></a>
</h4>
<ul>
<li>All three algorithms generate vector representations of words in a
high-dimensional space.</li>
<li>They can be used to solve a wide range of natural language
processing tasks.</li>
<li>They are all open-source and widely used in the research
community.</li>
</ul>
</div>
<div class="section level4">
<h4 id="differences">Differences<a class="anchor" aria-label="anchor" href="#differences"></a>
</h4>
<ul>
<li>Word2Vec focuses on generating embeddings by predicting the context
of a word given its surrounding words in a sentence, while GloVe and
fastText generate embeddings by predicting the probability of a word
co-occurring with another word in a corpus.</li>
<li>fastText also includes character n-grams, allowing it to generate
embeddings for words not seen during training, making it particularly
useful for handling out-of-vocabulary words.</li>
<li>In general, fastText is considered to be the fastest to train among
the three embedding techniques (GloVe, fastText, and Word2Vec). This is
because fastText uses subword information, which reduces the vocabulary
size and allows the model to handle out-of-vocabulary words.
Additionally, fastText uses a hierarchical softmax for training, which
is faster than the traditional softmax used by Word2Vec. Finally,
fastText can be trained on multiple threads, further speeding up the
training process.</li>
</ul>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>As an alternative to using a pre-trained model, training a Word2Vec
model on a specific dataset allows you use Word2Vec for NER-related
tasks.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section></section><section id="aio-10-finetuning-transformers"><p>Content from <a href="10-finetuning-transformers.html">Finetuning LLMs</a></p>
<hr>
<p>Last updated on 2025-05-01 |

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/episodes/10-finetuning-transformers.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can I fine-tune preexisting LLMs for my own research?</li>
<li>How do I pick the right data format?</li>
<li>How do I create my own labels?</li>
<li>How do I put my data into a model for finetuning?</li>
<li>How do I evaluate success at my task?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Examine CONLL2003 data.</li>
<li>Learn about Label Studio.</li>
<li>Learn about finetuning a BERT model.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<hr class="half-width">
<p>If you are running this lesson on Google Colab, it is strongly
recommended that you enable GPU acceleration. If you are running locally
without CUDA, you should be able to run most of the commands, but
training will take a long time and you will want to use the pretrained
model when using it.</p>
<p>To enable GPU, click “Edit &gt; Notebook settings” and select GPU. If
enabled, this command will return a status window and not an error:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="op">!</span>nvidia<span class="op">-</span>smi</span></code></pre>
</div>
<pre><code>Thu Mar 28 20:50:47 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |
| N/A   64C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+</code></pre>
<p>These installation commands will take time to run. Begin them
now.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="op">!</span>pip install transformers datasets evaluate seqeval</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="finetuning-llms">Finetuning LLMs<a class="anchor" aria-label="anchor" href="#finetuning-llms"></a>
</h2>
<hr class="half-width">
<p>In 2017, a revolutionary breakthrough for NLP occurred. A new type of
hidden layer for neural networks called Transfomers were invented.
Transformers made processing huge amounts of data feasible for the first
time.</p>
<p>Large Language Models, or LLMs, were the result. LLMs are the current
state of the art when it comes to many tasks, and although LLMs can
differ, they are mostly based on a similar architecture to one another.
We will be looking at an influential LLM called BERT.</p>
<figure><img src="fig/10-bert-fine-tune.png" alt="BERT fine-tune" class="figure mx-auto d-block"></figure><p>Training these models from scratch requires a huge amount of data and
compute power. The majority of work is done for the many hidden layers
of the model. However, by tweaking only the output layer, BERT can
effectively perform many tasks with a minimal amount of data. This
process of adapting an LLM is called <strong>fine-tuning</strong>.</p>
<p>Because of this, we will not be writing the code for this lesson from
scratch. Rather, this lesson will focus on creating our own data,
adapting existing code and modifying it to achieve the task we want to
accomplish.</p>
</section><section><h2 class="section-heading" id="using-existing-model--distilbert">Using Existing Model- DistilBERT<a class="anchor" aria-label="anchor" href="#using-existing-model--distilbert"></a>
</h2>
<hr class="half-width">
<p>We will be using a miniture LLM called DistilBERT for this lesson. We
are using the “uncased” version of distilbert, which removes
capitalization.</p>
<p>Much like many of our models, DistilBERT is available through
HuggingFace. <a href="https://huggingface.co/docs/transformers/model_doc/distilbert" class="external-link uri">https://huggingface.co/docs/transformers/model_doc/distilbert</a></p>
<p>Let’s start by importing the library, and importing both the
pretrained model and the tokenizer that BERT uses.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForTokenClassification</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"Davlan/distilbert-base-multilingual-cased-ner-hrl"</span>)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>model <span class="op">=</span> AutoModelForTokenClassification.from_pretrained(<span class="st">"Davlan/distilbert-base-multilingual-cased-ner-hrl"</span>)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co">#The aggregation strategy combines all of the tokens with a given label. Useful when our tokenizer uses subword tokens.</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>nlp <span class="op">=</span> pipeline(<span class="st">"ner"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer, aggregation_strategy<span class="op">=</span><span class="st">'simple'</span>)</span></code></pre>
</div>
<p>Next, we’ll use the tokenizer to preprocess our example sentence.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>example <span class="op">=</span> <span class="st">"Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute."</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>ner_results <span class="op">=</span> nlp(example)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="cf">for</span> result <span class="kw">in</span> ner_results:</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>  <span class="bu">print</span>(result)</span></code></pre>
</div>
<p>LLMs are highly performant at not just one, but a variety of tasks.
And there are many versions of LLMs, designed to perform well on a
variety of tasks available on HuggingFace.</p>
<p>We could use this existing model for research purposes as is. We
might use an existing NER model to find examples of the most common
locations in a set of fiction. You could categorize product reviews as
positive or negative automatically using sentiment analysis. You could
automatically translate documents from one language to another.</p>
<p>There are many possible tasks that LLMs can handle!</p>
<div class="section level3">
<h3 id="why-fine-tune">Why Fine Tune?<a class="anchor" aria-label="anchor" href="#why-fine-tune"></a>
</h3>
<p>Given that there are many many prebuilt models for BERT, why would
you want to go through the trouble of fine tuning your own?</p>
<p>LLM’s are very robust. They aren’t just capable of doing tasks other
people have already trained them for. LLMs can also do specific and
novel tasks you might want to accomplish as part of research!</p>
<p>Imagine using a LLM to classify a group of documents using training
data you create. Or imagine an LLM pulling out specific types of words
based on examples you provide. LLM’s can be trained to do these specific
tasks fairly well, without needing terabytes of data to do so.</p>
<p>Let’s take a look on how we fine tune an LLM on a novel task by
walking through an example.</p>
</div>
</section><section><h2 class="section-heading" id="the-interpretive-loop">The Interpretive Loop<a class="anchor" aria-label="anchor" href="#the-interpretive-loop"></a>
</h2>
<hr class="half-width">
<p>To fine-tune, we will walk through all of the steps of our
interpretive loop diagram. Let’s take a look at our diagram once
more:</p>
<figure><img src="fig/01-Interpretive_Loop.JPG" alt="BERT fine-tune" class="figure mx-auto d-block"></figure><p>If no existing model does a given task, we can fine-tune a LLM to do
it. How do we start? We’re going to create versions of all the items
listed in our diagram.</p>
<p>We need the following: 1. A task, so we can find a model and LLM
pipeline to finetune. 2. A dataset for our task, properly formatted in a
way BERT can interpret. 3. A tokenizer and helpers to preprocess our
data in a way BERT expects. 4. A model that has been pretrained on
millions of documents for us. We will only fine-tune this model, not
recreate it from scratch. 5. A trainer to fine-tune our model to perform
our task. 6. A set of metrics so that we can evaluate how well our model
performs.</p>
<p>The final product of all this work will be a fine-tuned model that
classifies all the elements of reviews that we want. Let’s get
started!</p>
</section><section><h2 class="section-heading" id="nlp-task">NLP task<a class="anchor" aria-label="anchor" href="#nlp-task"></a>
</h2>
<hr class="half-width">
<p>The first thing we can do is identify our task. Suppose our research
question is to look carefully at different elements of restaurant
reviews. We want to classify different elements of restaurant reviews,
such as amenities, locations, ratings, cuisine types and so on using an
LLM.</p>
<p>Our task here is Token Classification, or more specifically, Named
Entity Recognition. Classifying tokens will enable us to pull out
categories that are of interest to us.</p>
<p>The standard set of Named Entity Recognition labels is designed to be
broad: people, organizations and so on. However, it doesn’t have to be.
We can define our own entities of interest and have our model search for
them.</p>
<p>Now that we have an idea of what we’re aiming to do, lets look at
some of the LLMs provided by HuggingFace that perform this activity.
HuggingFace hosts many instructional Colab notebooks available at: <a href="https://huggingface.co/docs/transformers/notebooks" class="external-link uri">https://huggingface.co/docs/transformers/notebooks</a>.</p>
<p>We can find an example of Token Classification using PyTorch there.
This document will be the basis for our code.</p>
</section><section><h2 class="section-heading" id="examining-working-example">Examining Working Example<a class="anchor" aria-label="anchor" href="#examining-working-example"></a>
</h2>
<hr class="half-width">
<p>Looking at the notebook, we can get an idea of how it functions and
adapt it for our own purposes.</p>
<ol style="list-style-type: decimal">
<li>The existing model it uses is a compressed version of BERT,
“distilbert.” While not as accurate as the full BERT model, it is
smaller and easier to fine tune. We’ll use this model as well.</li>
<li>The existing dataset for our task is something called “conll2003”.
We will want to look at this and replace it with our own data, taking
care to copy the formatting of existing data.</li>
<li>The existing tokenizer requires a special helper method called an
aligner. We will copy this directly.</li>
<li>The existing model that we will tweak to accomplish our task.</li>
<li>A trainer, which will largely use existing parameters. We will need
to tweak our output labels for our new data.</li>
<li>The existing metrics will be fine, but we have to feed them into our
trainer.</li>
</ol></section><section><h2 class="section-heading" id="creating-training-data">Creating training data<a class="anchor" aria-label="anchor" href="#creating-training-data"></a>
</h2>
<hr class="half-width">
<p>It’s a good idea to pattern your data output based on what the model
is expecting. You will need to make adjustments, but if you have
selected a model that is appropriate to the task you can reuse most of
the code already in place. We’ll start by installing our
dependencies.</p>
<p>Now, let’s take a look at the example data from the dataset used in
the example. The dataset used is called the CoNLL2003 dataset.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>ds <span class="op">=</span> load_dataset(<span class="st">"conll2003"</span>, trust_remote_code<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="bu">print</span>(ds)</span></code></pre>
</div>
<p>We can see that the CONLL dataset is split into three sets- training
data, validation data, and test data. Training data should make up about
80% of your corpus and is fed into the model to fine tune it. Validation
data should be about 10%, and is used to check how the training progress
is going as the model is trained. The test data is about 10% withheld
until the model is fully trained and ready for testing, so you can see
how it handles new documents that the model has never seen before.</p>
<p>Let’s take a closer look at a record in the train set so we can get
an idea of what our data should look like. The NER tags are the ones we
are interested in, so lets print them out and take a look. We’ll also
select the dataset and then an index for the document to look at an
example.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>traindoc <span class="op">=</span> ds[<span class="st">"train"</span>][<span class="dv">0</span>]</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>conll_tags <span class="op">=</span> ds[<span class="st">"train"</span>].features[<span class="ss">f"ner_tags"</span>].feature.names</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="bu">print</span>(traindoc[<span class="st">'tokens'</span>])</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="bu">print</span>(traindoc[<span class="st">'ner_tags'</span>])</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="bu">print</span>(conll_tags)</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="cf">for</span> token, ner_tag <span class="kw">in</span> <span class="bu">zip</span>(traindoc[<span class="st">'tokens'</span>], traindoc[<span class="st">'ner_tags'</span>]):</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>  <span class="bu">print</span>(token<span class="op">+</span><span class="st">" "</span><span class="op">+</span>conll_tags[ner_tag])</span></code></pre>
</div>
<p>Each document has it’s own ID number. We can see that the tokens are
a list of words in the document. For each word in the tokens, there are
a series of numbers. Those numbers correspond to the labels in the
database. Based on this, we can see that the EU is recognized as an ORG
and the terms “German” and “British” are labelled as MISC.</p>
<p>These datasets are loaded using specially written loading scripts. We
can look at this script by searching for the ‘conll2003’ in huggingface
and selecting “Files”. The loading script is always named after the
dataset. In this case it is “conll2003.py”.</p>
<p><a href="https://huggingface.co/datasets/conll2003/blob/main/conll2003.py" class="external-link uri">https://huggingface.co/datasets/conll2003/blob/main/conll2003.py</a></p>
<p>Opening this file up, we can see that a zip file is downloaded and
text files are extracted. We can manually download this ourselves if we
would really like to take a closer look. For the sake of convienence,
the example we looked just looked at is reproduced below:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="co">-DOCSTART- -X- -X- O</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co">EU NNP B-NP B-ORG</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="co">rejects VBZ B-VP O</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a><span class="co">German JJ B-NP B-MISC</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="co">call NN I-NP O</span></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a><span class="co">to TO B-VP O</span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a><span class="co">boycott VB I-VP O</span></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a><span class="co">British JJ B-NP B-MISC</span></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a><span class="co">lamb NN I-NP O</span></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a><span class="co">. . O O</span></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a><span class="co">"""</span></span></code></pre>
</div>
<pre><code><span><span class="st">'\n-DOCSTART- -X- -X- O\n\nEU NNP B-NP B-ORG\nrejects VBZ B-VP O\nGerman JJ B-NP B-MISC\ncall NN I-NP O\nto TO B-VP O\nboycott VB I-VP O\nBritish JJ B-NP B-MISC\nlamb NN I-NP O\n. . O O\n'</span></span></code></pre>
<p>This is a simple format, similar to a CSV. Each document is seperated
by a blank line. The token we look at is first, then space seperated
tags for POS, chunk_tags and NER tags. Many of the token classifications
use BIO tagging, which specifies that “B” is the beginning of a tag, “I”
is inside a tag, and “O” means that the token outside of our tagging
schema.</p>
<p>So, now that we have an idea of what the HuggingFace models expect,
let’s start thinking about how we can create our own set of data and
labels.</p>
</section><section><h2 class="section-heading" id="tagging-a-dataset">Tagging a dataset<a class="anchor" aria-label="anchor" href="#tagging-a-dataset"></a>
</h2>
<hr class="half-width">
<p>Most of the human time spent training a model will be spent
pre-processing and labelling data. If we expect our model to label data
with an arbitrary set of labels, we need to give it some idea of what to
look for. We want to make sure we have enough data for the model to
perform at a good enough degree of accuracy for our purpose. Of course,
this number will vary based on what level of performance is “good
enough” and the difficulty of the task. While there’s no set number, a
set of approximately 100,000 tokens is enough to train many NER
tasks.</p>
<p>Fortunately, software exists to help streamline the tagging process.
One open source example of tagging software is Label Studio. However,
it’s not the only option, so feel free to select a data labelling
software that matches your preferences or needs for a given project. An
online demo of Label Studio is available here: <a href="https://labelstud.io/playground" class="external-link uri">https://labelstud.io/playground</a>. It’s also possible to
install locally.</p>
<p>Select “Named Entity Recognition” as the task to see what the
interface would look like if we were doing our own tagging. We can
define our own labels by copying in the following code (minus the
quotations):</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="co">&lt;View&gt;</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="co">  &lt;Labels name="label" toName="text"&gt;</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a><span class="co">    &lt;Label value="Amenity" background="red"/&gt;</span></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="co">    &lt;Label value="Cuisine" background="darkorange"/&gt;</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="co">    &lt;Label value="Dish" background="orange"/&gt;</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="co">    &lt;Label value="Hours" background="green"/&gt;</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a><span class="co">    &lt;Label value="Location" background="darkblue"/&gt;</span></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="co">    &lt;Label value="Price" background="blue"/&gt;</span></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a><span class="co">    &lt;Label value="Rating" background="purple"/&gt;</span></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a><span class="co">    &lt;Label value="Restaurant_Name" background="#842"/&gt;</span></span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a><span class="co">  &lt;/Labels&gt;</span></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a><span class="co">  &lt;Text name="text" value="$text"/&gt;</span></span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a><span class="co">&lt;/View&gt;</span></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a><span class="co">"""</span></span></code></pre>
</div>
<p>In Label Studio, labels can be applied by hitting a number on your
keyboard and highlighting the relevant part of the document. Try doing
so on our example text and looking at the output.</p>
<p>Once done, we will have to export our files for use in our model.
Label Studio supports a number of different types of labelling tasks, so
you may want to use it for tasks other than just NER.</p>
<p>One additional note: There is a github project for direct integration
between label studio and HuggingFace available as well. Given that the
task selected may vary on the model and you may not opt to use Label
Studio for a given project, we will simply point to this project as a
possible resource (<a href="https://github.com/heartexlabs/label-studio-transformers" class="external-link uri">https://github.com/heartexlabs/label-studio-transformers</a>)
rather than use it in this lesson.</p>
</section><section><h2 class="section-heading" id="export-to-desired-format">Export to desired format<a class="anchor" aria-label="anchor" href="#export-to-desired-format"></a>
</h2>
<hr class="half-width">
<p>So, let’s say you’ve finished your tagging project. How do we get
these labels out of label studio and into our model?</p>
<p>Label Studio supports export into many formats, including one called
CoNLL2003. This is the format our test dataset is in. It’s a space
seperated CSV, with words and their tags.</p>
<p>We’ll skip the export step as well, as we already have a prelabeled
set of tags in a similar format published by MIT. For more details about
supported export formats consult the help page for Label Studio here: <a href="https://labelstud.io/guide/export.html" class="external-link uri">https://labelstud.io/guide/export.html</a></p>
<p>At this point, we’ve got all the labelled data we want. We now need
to load our dataset into HuggingFace and then train our model. The
following code will be largely based on the example code from
HuggingFace, substituting in our data for the CoNLL data.</p>
</section><section><h2 class="section-heading" id="loading-our-custom-dataset">Loading our custom dataset<a class="anchor" aria-label="anchor" href="#loading-our-custom-dataset"></a>
</h2>
<hr class="half-width">
<p>Let’s import our carpentries files and helper methods first, as they
contain our data and a loading script.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="co"># Run this cell to mount your Google Drive.</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>drive.mount(<span class="st">'/content/drive'</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># pip install necessary to access parse module (called from helpers.py)</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="op">!</span>pip install parse</span></code></pre>
</div>
<p>Finally, lets make our own tweaks to the HuggingFace colab notebook.
We’ll start by importing some key metrics.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">import</span> datasets</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset, Features</span></code></pre>
</div>
<p>The HuggingFace example uses <a href="https://www.aclweb.org/anthology/W03-0419.pdf" class="external-link">CONLL 2003
dataset</a>.</p>
<p>All datasets from huggingface are loaded using scripts. Datasets can
be defined from a JSON or csv file (see the <a href="https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files" class="external-link">Datasets
documentation</a>) but selecting CSV will by default create a new
document for every token and NER tag and will not load the documents
correctly. So we will use a tweaked version of the Conll loading script
instead. Let’s take a look at the regular Conll script first:</p>
<p><a href="https://huggingface.co/datasets/conll2003/tree/main" class="external-link uri">https://huggingface.co/datasets/conll2003/tree/main</a></p>
<p>The loading script is the python file. Usually the loading script is
named after the dataset in question. There are a couple of things we
want to change-</p>
<ol style="list-style-type: decimal">
<li>We want to tweak the metadata with citations to reflect where we got
our data. If you created the data, you can add in your own citation
here.</li>
<li>We want to define our own categories for NER_TAGS, to reflect our
new named entities.</li>
<li>The order for our tokens and NER tags is flipped in our data
files.</li>
<li>Delimiters for our data files are tabs instead of spaces.</li>
<li>We will replace the method names with ones appropriate for our
dataset.</li>
</ol>
<p>Those modifications have been made in our mit_restaurants.py file.
Let’s briefly take a look at that file before we proceed with the
huggingface script. Again, these are modifications, not working from
scratch.</p>
</section><section><h2 class="section-heading" id="huggingface-code">HuggingFace Code<a class="anchor" aria-label="anchor" href="#huggingface-code"></a>
</h2>
<hr class="half-width">
<p>Now that we have a modified huggingface script, let’s load our
data.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>ds <span class="op">=</span> load_dataset(<span class="st">"/content/drive/MyDrive/Colab Notebooks/text-analysis/code/mit_restaurants.py"</span>, trust_remote_code<span class="op">=</span><span class="va">True</span>)</span></code></pre>
</div>
<p>How does our dataset compare to the CONLL dataset? Let’s look at a
record and compare.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>ds</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>label_list <span class="op">=</span> ds[<span class="st">"train"</span>].features[<span class="ss">f"ner_tags"</span>].feature.names</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>label_list</span></code></pre>
</div>
<p>Our data looks pretty similar to the CONLL data now. This is good
since we can now reuse many of the methods listed by HuggingFace in
their Colab notebook.</p>
</section><section><h2 class="section-heading" id="preprocessing-the-data">Preprocessing the data<a class="anchor" aria-label="anchor" href="#preprocessing-the-data"></a>
</h2>
<hr class="half-width">
<p>We start by defining some variables that HuggingFace uses later
on.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>task <span class="op">=</span> <span class="st">"ner"</span> <span class="co"># Should be one of "ner", "pos" or "chunk"</span></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>model_checkpoint <span class="op">=</span> <span class="st">"distilbert-base-uncased"</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span></code></pre>
</div>
<p>Next, we create our special BERT tokenizer.</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_checkpoint)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>example <span class="op">=</span> ds[<span class="st">"train"</span>][<span class="dv">4</span>]</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>tokenized_input <span class="op">=</span> tokenizer(example[<span class="st">"tokens"</span>], is_split_into_words<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(tokenized_input[<span class="st">"input_ids"</span>])</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a><span class="bu">print</span>(tokens)</span></code></pre>
</div>
<p>Since our words are broken into just words, and the BERT tokenizer
sometimes breaks words into subwords, we need to retokenize our words.
We also need to make sure that when we do this, the labels we created
don’t get misaligned. More details on these methods are available
through HuggingFace, but we will simply use their code to do this.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>word_ids <span class="op">=</span> tokenized_input.word_ids()</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>aligned_labels <span class="op">=</span> [<span class="op">-</span><span class="dv">100</span> <span class="cf">if</span> i <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> example[<span class="ss">f"</span><span class="sc">{</span>task<span class="sc">}</span><span class="ss">_tags"</span>][i] <span class="cf">for</span> i <span class="kw">in</span> word_ids]</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>label_all_tokens <span class="op">=</span> <span class="va">True</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="kw">def</span> tokenize_and_align_labels(examples):</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>    tokenized_inputs <span class="op">=</span> tokenizer(examples[<span class="st">"tokens"</span>], truncation<span class="op">=</span><span class="va">True</span>, is_split_into_words<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>    labels <span class="op">=</span> []</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a>    <span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(examples[<span class="ss">f"</span><span class="sc">{</span>task<span class="sc">}</span><span class="ss">_tags"</span>]):</span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a>        word_ids <span class="op">=</span> tokenized_inputs.word_ids(batch_index<span class="op">=</span>i)</span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a>        previous_word_idx <span class="op">=</span> <span class="va">None</span></span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a>        label_ids <span class="op">=</span> []</span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>        <span class="cf">for</span> word_idx <span class="kw">in</span> word_ids:</span>
<span id="cb21-10"><a href="#cb21-10" tabindex="-1"></a>            <span class="co"># Special tokens have a word id that is None. We set the label to -100 so they are automatically</span></span>
<span id="cb21-11"><a href="#cb21-11" tabindex="-1"></a>            <span class="co"># ignored in the loss function.</span></span>
<span id="cb21-12"><a href="#cb21-12" tabindex="-1"></a>            <span class="cf">if</span> word_idx <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb21-13"><a href="#cb21-13" tabindex="-1"></a>                label_ids.append(<span class="op">-</span><span class="dv">100</span>)</span>
<span id="cb21-14"><a href="#cb21-14" tabindex="-1"></a>            <span class="co"># We set the label for the first token of each word.</span></span>
<span id="cb21-15"><a href="#cb21-15" tabindex="-1"></a>            <span class="cf">elif</span> word_idx <span class="op">!=</span> previous_word_idx:</span>
<span id="cb21-16"><a href="#cb21-16" tabindex="-1"></a>                label_ids.append(label[word_idx])</span>
<span id="cb21-17"><a href="#cb21-17" tabindex="-1"></a>            <span class="co"># For the other tokens in a word, we set the label to either the current label or -100, depending on</span></span>
<span id="cb21-18"><a href="#cb21-18" tabindex="-1"></a>            <span class="co"># the label_all_tokens flag.</span></span>
<span id="cb21-19"><a href="#cb21-19" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb21-20"><a href="#cb21-20" tabindex="-1"></a>                label_ids.append(label[word_idx] <span class="cf">if</span> label_all_tokens <span class="cf">else</span> <span class="op">-</span><span class="dv">100</span>)</span>
<span id="cb21-21"><a href="#cb21-21" tabindex="-1"></a>            previous_word_idx <span class="op">=</span> word_idx</span>
<span id="cb21-22"><a href="#cb21-22" tabindex="-1"></a></span>
<span id="cb21-23"><a href="#cb21-23" tabindex="-1"></a>        labels.append(label_ids)</span>
<span id="cb21-24"><a href="#cb21-24" tabindex="-1"></a></span>
<span id="cb21-25"><a href="#cb21-25" tabindex="-1"></a>    tokenized_inputs[<span class="st">"labels"</span>] <span class="op">=</span> labels</span>
<span id="cb21-26"><a href="#cb21-26" tabindex="-1"></a>    <span class="cf">return</span> tokenized_inputs</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>tokenized_datasets <span class="op">=</span> ds.<span class="bu">map</span>(tokenize_and_align_labels, batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a><span class="bu">print</span>(tokenized_datasets)</span></code></pre>
</div>
<p>The preprocessed features we’ve just added will be the ones used to
actually train the model.</p>
</section><section><h2 class="section-heading" id="fine-tuning-the-model">Fine-tuning the model<a class="anchor" aria-label="anchor" href="#fine-tuning-the-model"></a>
</h2>
<hr class="half-width">
<p>Now that our data is ready, we can download the pretrained LLM model.
Since our task is token classification, we use the
<code>AutoModelForTokenClassification</code> class. Before we do though,
we want to specify the mapping for ids and labels to our model so it
does not simply return CLASS_1, CLASS_2 and so on.</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>id2label <span class="op">=</span> {</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>    <span class="dv">0</span>: <span class="st">"O"</span>,</span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>    <span class="dv">1</span>: <span class="st">"B-Amenity"</span>,</span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a>    <span class="dv">2</span>: <span class="st">"I-Amenity"</span>,</span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a>    <span class="dv">3</span>: <span class="st">"B-Cuisine"</span>,</span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a>    <span class="dv">4</span>: <span class="st">"I-Cuisine"</span>,</span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a>    <span class="dv">5</span>: <span class="st">"B-Dish"</span>,</span>
<span id="cb23-8"><a href="#cb23-8" tabindex="-1"></a>    <span class="dv">6</span>: <span class="st">"I-Dish"</span>,</span>
<span id="cb23-9"><a href="#cb23-9" tabindex="-1"></a>    <span class="dv">7</span>: <span class="st">"B-Hours"</span>,</span>
<span id="cb23-10"><a href="#cb23-10" tabindex="-1"></a>    <span class="dv">8</span>: <span class="st">"I-Hours"</span>,</span>
<span id="cb23-11"><a href="#cb23-11" tabindex="-1"></a>    <span class="dv">9</span>: <span class="st">"B-Location"</span>,</span>
<span id="cb23-12"><a href="#cb23-12" tabindex="-1"></a>    <span class="dv">10</span>: <span class="st">"I-Location"</span>,</span>
<span id="cb23-13"><a href="#cb23-13" tabindex="-1"></a>    <span class="dv">11</span>: <span class="st">"B-Price"</span>,</span>
<span id="cb23-14"><a href="#cb23-14" tabindex="-1"></a>    <span class="dv">12</span>: <span class="st">"I-Price"</span>,</span>
<span id="cb23-15"><a href="#cb23-15" tabindex="-1"></a>    <span class="dv">13</span>: <span class="st">"B-Rating"</span>,</span>
<span id="cb23-16"><a href="#cb23-16" tabindex="-1"></a>    <span class="dv">14</span>: <span class="st">"I-Rating"</span>,</span>
<span id="cb23-17"><a href="#cb23-17" tabindex="-1"></a>    <span class="dv">15</span>: <span class="st">"B-Restaurant_Name"</span>,</span>
<span id="cb23-18"><a href="#cb23-18" tabindex="-1"></a>    <span class="dv">16</span>: <span class="st">"I-Restaurant_Name"</span>,</span>
<span id="cb23-19"><a href="#cb23-19" tabindex="-1"></a>}</span>
<span id="cb23-20"><a href="#cb23-20" tabindex="-1"></a></span>
<span id="cb23-21"><a href="#cb23-21" tabindex="-1"></a>label2id <span class="op">=</span> {</span>
<span id="cb23-22"><a href="#cb23-22" tabindex="-1"></a>    <span class="st">"O"</span>: <span class="dv">0</span>,</span>
<span id="cb23-23"><a href="#cb23-23" tabindex="-1"></a>    <span class="st">"B-Amenity"</span>: <span class="dv">1</span>,</span>
<span id="cb23-24"><a href="#cb23-24" tabindex="-1"></a>    <span class="st">"I-Amenity"</span>: <span class="dv">2</span>,</span>
<span id="cb23-25"><a href="#cb23-25" tabindex="-1"></a>    <span class="st">"B-Cuisine"</span>: <span class="dv">3</span>,</span>
<span id="cb23-26"><a href="#cb23-26" tabindex="-1"></a>    <span class="st">"I-Cuisine"</span>: <span class="dv">4</span>,</span>
<span id="cb23-27"><a href="#cb23-27" tabindex="-1"></a>    <span class="st">"B-Dish"</span>: <span class="dv">5</span>,</span>
<span id="cb23-28"><a href="#cb23-28" tabindex="-1"></a>    <span class="st">"I-Dish"</span>: <span class="dv">6</span>,</span>
<span id="cb23-29"><a href="#cb23-29" tabindex="-1"></a>    <span class="st">"B-Hours"</span>: <span class="dv">7</span>,</span>
<span id="cb23-30"><a href="#cb23-30" tabindex="-1"></a>    <span class="st">"I-Hours"</span>: <span class="dv">8</span>,</span>
<span id="cb23-31"><a href="#cb23-31" tabindex="-1"></a>    <span class="st">"B-Location"</span>: <span class="dv">9</span>,</span>
<span id="cb23-32"><a href="#cb23-32" tabindex="-1"></a>    <span class="st">"I-Location"</span>: <span class="dv">10</span>,</span>
<span id="cb23-33"><a href="#cb23-33" tabindex="-1"></a>    <span class="st">"B-Price"</span>: <span class="dv">11</span>,</span>
<span id="cb23-34"><a href="#cb23-34" tabindex="-1"></a>    <span class="st">"I-Price"</span>: <span class="dv">12</span>,</span>
<span id="cb23-35"><a href="#cb23-35" tabindex="-1"></a>    <span class="st">"B-Rating"</span>: <span class="dv">13</span>,</span>
<span id="cb23-36"><a href="#cb23-36" tabindex="-1"></a>    <span class="st">"I-Rating"</span>: <span class="dv">14</span>,</span>
<span id="cb23-37"><a href="#cb23-37" tabindex="-1"></a>    <span class="st">"B-Restaurant_Name"</span>: <span class="dv">15</span>,</span>
<span id="cb23-38"><a href="#cb23-38" tabindex="-1"></a>    <span class="st">"I-Restaurant_Name"</span>: <span class="dv">16</span>,</span>
<span id="cb23-39"><a href="#cb23-39" tabindex="-1"></a>}</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForTokenClassification, TrainingArguments, Trainer</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a>model <span class="op">=</span> AutoModelForTokenClassification.from_pretrained(model_checkpoint, id2label<span class="op">=</span>id2label, label2id<span class="op">=</span>label2id, num_labels<span class="op">=</span><span class="bu">len</span>(label_list)).to(device)</span></code></pre>
</div>
<pre><code>Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
<p>The warning is telling us we are throwing away some weights. We’re
training our model, so we should be fine.</p>
<p>##Configuration Arguments</p>
<p>Next, we configure our trainer. The are lots of settings here but the
defaults are fine. More detailed documentation on what each of these
mean are available through Huggingface: <a href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments" class="external-link"><code>TrainingArguments</code></a>,</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>model_name <span class="op">=</span> model_checkpoint.split(<span class="st">"/"</span>)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>args <span class="op">=</span> TrainingArguments(</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>    <span class="co">#f"{model_name}-finetuned-{task}",</span></span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">-carpentries-restaurant-ner"</span>,</span>
<span id="cb26-5"><a href="#cb26-5" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-5</span>,</span>
<span id="cb26-6"><a href="#cb26-6" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span>batch_size,</span>
<span id="cb26-7"><a href="#cb26-7" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span>batch_size,</span>
<span id="cb26-8"><a href="#cb26-8" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb26-9"><a href="#cb26-9" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb26-10"><a href="#cb26-10" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb26-11"><a href="#cb26-11" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb26-12"><a href="#cb26-12" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb26-13"><a href="#cb26-13" tabindex="-1"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb26-14"><a href="#cb26-14" tabindex="-1"></a>    push_to_hub<span class="op">=</span><span class="va">False</span>, <span class="co">#You can have your model automatically pushed to HF if you uncomment this and log in.</span></span>
<span id="cb26-15"><a href="#cb26-15" tabindex="-1"></a>)</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="collator">Collator<a class="anchor" aria-label="anchor" href="#collator"></a>
</h2>
<hr class="half-width">
<p>One finicky aspect of the model is that all of the inputs have to be
the same size. When the sizes do not match, something called a data
collator is used to batch our processed examples together and pad them
to the same size.</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DataCollatorForTokenClassification</span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a>data_collator <span class="op">=</span> DataCollatorForTokenClassification(tokenizer)</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="metrics">Metrics<a class="anchor" aria-label="anchor" href="#metrics"></a>
</h2>
<hr class="half-width">
<p>The last thing we want to define is the metric by which we evaluate
how our model did. We will use <a href="https://github.com/chakki-works/seqeval" class="external-link"><code>seqeval</code></a>.
The metric used will vary based on the task- make sure to check the
huggingface notebooks for the appropriate metric for a given task.</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="im">import</span> evaluate</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>seqeval <span class="op">=</span> evaluate.load(<span class="st">"seqeval"</span>)</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="post-processing">Post Processing<a class="anchor" aria-label="anchor" href="#post-processing"></a>
</h2>
<hr class="half-width">
<p>Per HuggingFace- we need to do a bit of post-processing on our
predictions. The following function and description is taken directly
from HuggingFace. The function does the following: - Selected the
predicted index (with the maximum logit) for each token - Converts it to
its string label - Ignore everywhere we set a label of -100</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a><span class="kw">def</span> compute_metrics(p):</span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a>    predictions, labels <span class="op">=</span> p</span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a>    predictions <span class="op">=</span> np.argmax(predictions, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" tabindex="-1"></a>    <span class="co"># Remove ignored index (special tokens)</span></span>
<span id="cb29-8"><a href="#cb29-8" tabindex="-1"></a>    true_predictions <span class="op">=</span> [</span>
<span id="cb29-9"><a href="#cb29-9" tabindex="-1"></a>        [label_list[p] <span class="cf">for</span> (p, l) <span class="kw">in</span> <span class="bu">zip</span>(prediction, label) <span class="cf">if</span> l <span class="op">!=</span> <span class="op">-</span><span class="dv">100</span>]</span>
<span id="cb29-10"><a href="#cb29-10" tabindex="-1"></a>        <span class="cf">for</span> prediction, label <span class="kw">in</span> <span class="bu">zip</span>(predictions, labels)</span>
<span id="cb29-11"><a href="#cb29-11" tabindex="-1"></a>    ]</span>
<span id="cb29-12"><a href="#cb29-12" tabindex="-1"></a>    true_labels <span class="op">=</span> [</span>
<span id="cb29-13"><a href="#cb29-13" tabindex="-1"></a>        [label_list[l] <span class="cf">for</span> (p, l) <span class="kw">in</span> <span class="bu">zip</span>(prediction, label) <span class="cf">if</span> l <span class="op">!=</span> <span class="op">-</span><span class="dv">100</span>]</span>
<span id="cb29-14"><a href="#cb29-14" tabindex="-1"></a>        <span class="cf">for</span> prediction, label <span class="kw">in</span> <span class="bu">zip</span>(predictions, labels)</span>
<span id="cb29-15"><a href="#cb29-15" tabindex="-1"></a>    ]</span>
<span id="cb29-16"><a href="#cb29-16" tabindex="-1"></a></span>
<span id="cb29-17"><a href="#cb29-17" tabindex="-1"></a>    results <span class="op">=</span> metric.compute(predictions<span class="op">=</span>true_predictions, references<span class="op">=</span>true_labels)</span>
<span id="cb29-18"><a href="#cb29-18" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb29-19"><a href="#cb29-19" tabindex="-1"></a>        <span class="st">"precision"</span>: results[<span class="st">"overall_precision"</span>],</span>
<span id="cb29-20"><a href="#cb29-20" tabindex="-1"></a>        <span class="st">"recall"</span>: results[<span class="st">"overall_recall"</span>],</span>
<span id="cb29-21"><a href="#cb29-21" tabindex="-1"></a>        <span class="st">"f1"</span>: results[<span class="st">"overall_f1"</span>],</span>
<span id="cb29-22"><a href="#cb29-22" tabindex="-1"></a>        <span class="st">"accuracy"</span>: results[<span class="st">"overall_accuracy"</span>],</span>
<span id="cb29-23"><a href="#cb29-23" tabindex="-1"></a>    }</span></code></pre>
</div>
<p>Finally, after all of the preparation we’ve done, we’re ready to
create a Trainer to train our model.</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a>    model,</span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a>    args,</span>
<span id="cb30-4"><a href="#cb30-4" tabindex="-1"></a>    train_dataset<span class="op">=</span>tokenized_datasets[<span class="st">"train"</span>],</span>
<span id="cb30-5"><a href="#cb30-5" tabindex="-1"></a>    eval_dataset<span class="op">=</span>tokenized_datasets[<span class="st">"validation"</span>],</span>
<span id="cb30-6"><a href="#cb30-6" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collator,</span>
<span id="cb30-7"><a href="#cb30-7" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb30-8"><a href="#cb30-8" tabindex="-1"></a>    compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb30-9"><a href="#cb30-9" tabindex="-1"></a>)</span></code></pre>
</div>
<p>We can now finetune our model by just calling the <code>train</code>
method. Note that this step will take about 5 minutes if you are running
it on a GPU, and 4+ hours if you are not.</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training starts NOW"</span>)</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a>trainer.train()</span></code></pre>
</div>
<p>We’ve done it! We’ve fine-tuned the model for our task. Now that it’s
trained, we want to save our work so that we can reuse the model
whenever we wish. A saved version of this model has also been published
through huggingface, so if you are using a CPU, skip the remaining
evaluation steps and launch a new terminal so you can participate in
the</p>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a>trainer.save_model(<span class="st">"/content/drive/MyDrive/Colab Notebooks/text-analysis/ft-model"</span>)</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="evaluation-metrics-for-ner">Evaluation Metrics for NER<a class="anchor" aria-label="anchor" href="#evaluation-metrics-for-ner"></a>
</h2>
<hr class="half-width">
<p>We have some NER evaluation metrics, so let’s discuss what they mean.
Accuracy is the most obvious metric for NER. Accuracy is the number of
correctly labelled entities divided by the number of total entities. The
problem with this metric can be illustrated by supposing we want a model
to identify a needle in a haystack. A model that identifies everything
as hay would be highly accurate, as most of the entities in a haystack
ARE hay, but it wouldn’t allow us to find the rare needles we’re looking
for. Similarly, our named entities will likely not make up most of our
documents, so accuracy is not a good metric.</p>
<p>We can classify recommendations made by a model into four categories-
true positive, true negative, false positive and false negative.</p>
<table class="table">
<colgroup>
<col width="38%">
<col width="28%">
<col width="32%">
</colgroup>
<thead><tr class="header">
<th></th>
<th>Document is in our category</th>
<th>Document is not in our category</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Model predicts it is in our category</td>
<td>True Positive (TP)</td>
<td>False Positive (FP)</td>
</tr>
<tr class="even">
<td>Model predicts it is not in category</td>
<td>False Negative (FN)</td>
<td>True Negative (TN)</td>
</tr>
</tbody>
</table>
<p><strong>Precision</strong> is TP / TP + FP. It measures how correct
your model’s labels were among the set of entities the model predicted
were part of the class. This measure could be gamed, however, by being
very conservative about making positive labels and only doing so when
the model was absolutely certain, possibly missing relevant
entities.</p>
<p><strong>Recall</strong> is TP / TP + FN. It measures how correct your
model’s labels are among the set of every entity actually belonging to
the class. Recall could be trivally gamed by simply classify all
documents as being part of the class.</p>
<p>The <strong>F1</strong> score is a harmonic mean between the two,
ensuring the model is neither too conservative or too prone to
overclassification.</p>
<p>Now let’s see how our model did. We’ll run a more detailed evaluation
step from HuggingFace if desired, to see how well our model performed.
It is likely a good idea to have these metrics so that you can compare
your performance to more generic models for the task.</p>
<div class="codewrapper sourceCode" id="cb33">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a><span class="im">from</span> evaluate <span class="im">import</span> evaluator</span>
<span id="cb33-2"><a href="#cb33-2" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" tabindex="-1"></a>task_evaluator <span class="op">=</span> evaluator(<span class="st">"ner"</span>)</span>
<span id="cb33-4"><a href="#cb33-4" tabindex="-1"></a>data<span class="op">=</span> load_dataset(<span class="st">"/content/drive/MyDrive/Colab Notebooks/text-analysis/code/mit_restaurants.py"</span>, split<span class="op">=</span><span class="st">"test"</span>, trust_remote_code<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-5"><a href="#cb33-5" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" tabindex="-1"></a>eval_results <span class="op">=</span> task_evaluator.compute(</span>
<span id="cb33-7"><a href="#cb33-7" tabindex="-1"></a>    model_or_pipeline<span class="op">=</span><span class="st">"/content/drive/MyDrive/Colab Notebooks/text-analysis/ft-model"</span>,</span>
<span id="cb33-8"><a href="#cb33-8" tabindex="-1"></a>    data<span class="op">=</span>data,</span>
<span id="cb33-9"><a href="#cb33-9" tabindex="-1"></a>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a><span class="cf">for</span> r <span class="kw">in</span> eval_results:</span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a>  <span class="bu">print</span>(r, eval_results[r])</span></code></pre>
</div>
<p>Whether a F1 score of .779 is ‘good enough’ depends on the
performance of other models, how difficult the task is, and so on. It
may be good enough for our needs, or we may want to collect more data,
train on a bigger model, or adjust our parameters. For the purposes of
the workshop, we will say that this is fine.</p>
</section><section><h2 class="section-heading" id="using-our-model">Using our Model<a class="anchor" aria-label="anchor" href="#using-our-model"></a>
</h2>
<hr class="half-width">
<p>Now that we’ve created our model, we can run it just like we did the
pretrained models. The code below should do just that. Feel free to
compose your own example and see how well the model performs!</p>
<div class="codewrapper sourceCode" id="cb35">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb35-2"><a href="#cb35-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForTokenClassification</span>
<span id="cb35-3"><a href="#cb35-3" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb35-4"><a href="#cb35-4" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TokenClassificationPipeline</span>
<span id="cb35-5"><a href="#cb35-5" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb35-6"><a href="#cb35-6" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" tabindex="-1"></a><span class="co">#Colab code</span></span>
<span id="cb35-8"><a href="#cb35-8" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"distilbert-base-uncased"</span>)</span>
<span id="cb35-9"><a href="#cb35-9" tabindex="-1"></a>model <span class="op">=</span> AutoModelForTokenClassification.from_pretrained(<span class="st">"/content/drive/MyDrive/Colab Notebooks/text-analysis/ft-model"</span>)</span>
<span id="cb35-10"><a href="#cb35-10" tabindex="-1"></a>nlp <span class="op">=</span> pipeline(<span class="st">"ner"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer, aggregation_strategy<span class="op">=</span><span class="st">"first"</span>)</span>
<span id="cb35-11"><a href="#cb35-11" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" tabindex="-1"></a><span class="co">#This code imports this model, which I've uploaded to HuggingFace.</span></span>
<span id="cb35-13"><a href="#cb35-13" tabindex="-1"></a><span class="co">#tokenizer = AutoTokenizer.from_pretrained("karlholten/distilbert-carpentries-restaurant-ner")</span></span>
<span id="cb35-14"><a href="#cb35-14" tabindex="-1"></a><span class="co">#model = AutoModelForTokenClassification.from_pretrained("karlholten/distilbert-carpentries-restaurant-ner")</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb36">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a>EXAMPLE <span class="op">=</span> <span class="st">"where is a four star restaurant in milwaukee with tapas"</span></span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a>ner_results <span class="op">=</span> nlp(EXAMPLE)</span>
<span id="cb36-3"><a href="#cb36-3" tabindex="-1"></a><span class="cf">for</span> entity <span class="kw">in</span> ner_results:</span>
<span id="cb36-4"><a href="#cb36-4" tabindex="-1"></a>  <span class="bu">print</span>(entity)</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="outro">Outro<a class="anchor" aria-label="anchor" href="#outro"></a>
</h2>
<hr class="half-width">
<p>That’s it! Let’s review briefly what we have done. We’ve discussed
how to select a task. We used a HuggingFace example to help decide on a
data format, and looked over it to get an idea of what the model
expects. We went over Label Studio, one way to label your own data. We
retokenized our example data and fine-tuned a model. Then we went over
the results of our model.</p>
<p>LLM’s are the state-of-the-art for many types of task, and now you
have an idea of how to use and even fine tune them in your own research.
Our next lesson will discuss the ethics and implications of text
analysis.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>HuggingFace has many examples of LLMs you can fine-tune.</li>
<li>Examine preexisting examples to get an idea of what your model
expects.</li>
<li>Label Studio and other tagging software allows you to easily tag
your own data.</li>
<li>Looking at common metrics used and other models performance in your
subject area will give you an idea of how your model did.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-11-ethics"><p>Content from <a href="11-ethics.html">Ethics and Text Analysis</a></p>
<hr>
<p>Last updated on 2025-05-01 |

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/episodes/11-ethics.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>Is text analysis artificial intelligence?</li>
<li>How can training data influence results?</li>
<li>What are the risk zones to consider when using text analysis for
research?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand how text analysis fits into the larger picture of
artificial intelligence</li>
<li>Be able to consider the tool against your research objectives</li>
<li>Consider the drawbacks and inherent biases that may be present in
large language models</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="is-text-analysis-artificial-intelligence">Is text analysis artificial intelligence?<a class="anchor" aria-label="anchor" href="#is-text-analysis-artificial-intelligence"></a>
</h2>
<hr class="half-width">
<p>Artificial intelligence is loosely defined as the ability for
computer systems to perform tasks that have traditionally required human
reasoning and perception.</p>
<ul>
<li><p>To the extent that text analysis performs a task that resembles
reading, understanding, and analyzing meaning, it can be understood to
be part of the definition of artificial intelligence.</p></li>
<li><p>The methods in this lesson all demonstrate models that learn from
data - specifically, from text corpora that are not structured to
explicitly tell the machine anything other than, perhaps, title, author,
date, and body of text.</p></li>
<li>
<p>As a method and a tool, it is important to understand the tasks
to which it is best suited, and to understand the process well enough to
be able to interpret the results, including:</p>
<ol style="list-style-type: decimal">
<li>whether the results are relevant or meaningful</li>
<li>whether the results have been overly influenced by the model or
training data</li>
<li>how to responsibly use the results</li>
</ol>
</li>
</ul>
<p>We can describe these as commitments to ethical research methods.</p>
</section><section><h2 class="section-heading" id="relevance-or-meaningfulness">Relevance or meaningfulness<a class="anchor" aria-label="anchor" href="#relevance-or-meaningfulness"></a>
</h2>
<hr class="half-width">
<p>As with any research, the relevance or meaningfulness of your results
is relative to the research question itself. However, when you have a
particular research question (or a particular set of research
interests), it can be hard to connect the results of these models back
to your bigger picture aims. It can feel like trying to write a book
report but all you were given were the table of contents. One reason for
this difficulty is that the dimensions of the model are atheoretical.
That is, regardless of what research questions you are asking, the
models always start from the same starting point: the words of the text,
with no understanding of what those words mean to you. Our job is to
interpret the meaning of the model’s results, or the qualitative work
that follows.</p>
<p>The model is making a statistical determination based on the training
data it has been fed, and on the training itself, as well as the methods
you have used to parse the data set you’re analyzing. If you are using a
tool like ChatGPT, you may have access only to your own methods, and
will need to make an educated guess about the training data and training
methods. That doesn’t mean you can’t use that tool, but it does mean you
need to keep what is known and what is obscured about your methods at
the forefront as you conduct your research.</p>
<blockquote>
<p><em>Exercise</em>: You use LSA as a method to identify important
topics that are common across a set of popular 19th century English
novels, and conclude that X is most common. How might you explain this
result and why you used LSA?</p>
</blockquote>
</section><section><h2 class="section-heading" id="training-data-can-influence-results">Training data can influence results<a class="anchor" aria-label="anchor" href="#training-data-can-influence-results"></a>
</h2>
<hr class="half-width">
<p>There are numerous examples of how training data - or the language
model, ultimately - can negatively influence results. Reproducing bias
in the data is probably one of the most discussed negative outcomes.
Let’s look at one real world example:</p>
<p>In 2016, ProPublica published an investigative report that exposed
the clear bias against Black people in computer programs used to
determine the likelihood of defendants committing crimes in the future.
That bias was built into the tool because the training data that it
relied on included historical data about crime statistics, which
reflected - and then reproduced - existing racist bias in
sentencing.</p>
<blockquote>
<p><em>Exercise</em>: How might a researcher avoid introducing bias into
their methodology when using pre-trained data to conduct text
analysis?</p>
</blockquote>
</section><section><h2 class="section-heading" id="using-your-research">Using your research<a class="anchor" aria-label="anchor" href="#using-your-research"></a>
</h2>
<hr class="half-width">
<p>Rarely will results from topic modeling, text analysis, etc. stand on
their own as evidence of anything. Researchers should be able to explain
their method and how they got their results, and be able to talk about
the data sets and training models used. As discussed above, though, the
nature of the large language models that may underlie the methods used
to do LSA topic modeling, identify relationships between words using
Word2Vec, or summarize themes using BERT, is that they contain vast
numbers of parameters that cannot be reverse engineered or described.
The tool can still be part of the explanation, and any results that may
change due to the randommness of the LLM can be called out, for
example.</p>
</section><section><h2 class="section-heading" id="risk-zones">Risk zones<a class="anchor" aria-label="anchor" href="#risk-zones"></a>
</h2>
<hr class="half-width">
<p>Another area to consider when using any technology are the risk zones
that are introduced. We’re talking about unintended consequences, for
the most part, but consequences nonethless.</p>
<p>Let’s say you were using BERT to help summarize a large body of texts
to understand broad themes and relationships. Could this same method be
used to distort the contents of those texts to spread misinformation?
How can we mitigate that risk?</p>
<p>In the case of the LLMs that underlie many of the text analysis
methods you learned in this workshop, is there a chance that the results
could reinforce existing biases because of existing biases in the
training data? Consider this example:</p>
<blockquote>
<p><em>Exercise</em>: You are identifying topics across a large number
of archival texts from hundreds of 20th century collections documenting
LGBTQ organizations. You are using a LLM where the training data is
petabytes of data collected over a decade of web crawling, starting in
2013. What risks are introduced by this method and how might they be
anticipated and mitigated?</p>
</blockquote>
</section><section><h2 class="section-heading" id="hype-cycles-and-ai">Hype cycles and AI<a class="anchor" aria-label="anchor" href="#hype-cycles-and-ai"></a>
</h2>
<hr class="half-width">
<p>Because this workshop is being introduced shortly after the release
of ChatGPT3 by OpenAI, we want to address how AI and tech hype cycles
can influence tool selection and use of tech. The inscrutability of
LLMs, the ability of chatbots to output coherent and meaningful text on
a seemingly infinite variety of topics, and the rhetoric of the tech
industry can make these tools seem magical and unfathomable. They aren’t
magical, though the black box nature of the training data and the
parameters does lend itself to unfathomability. Regardless, the output
of any of the methods described in this workshop, and by LLMs to come,
is the product of mathematical processes and statistical weights. That
is why learning some of the methodology behind text analysis is
important, even if it takes much longer to become fluent in LSA or
Word2Vec. We all will use tools based on these methods in the years to
come, whether for our research or for more mundane administrative tasks.
Understanding something about how these tools work helps hold tech
accountable, and enables better use of these tools for apprpriate tasks.
Regrdless of the sophistication of the tool, it is humans who attribute
meaning to the results and not the machine.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>Text analysis is a tool and can’t assign meaning to results</li>
<li>As researchers we are responsible for understanding and explaining
our methods and results</li>
</ul>
</div>
</div>
</div>
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries-incubator/python-text-analysis/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/python-text-analysis/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/python-text-analysis/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://carpentries-incubator.github.io/python-text-analysis/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/python-text-analysis/aio.html",
  "identifier": "https://carpentries-incubator.github.io/python-text-analysis/aio.html",
  "dateCreated": "2020-10-12",
  "dateModified": "2025-10-28",
  "datePublished": "2025-10-28"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo --><script>
          var _paq = window._paq = window._paq || [];
          /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
          _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
          _paq.push(["setDomains", ["*.lessons.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
          _paq.push(["setDoNotTrack", true]);
          _paq.push(["disableCookies"]);
          _paq.push(["trackPageView"]);
          _paq.push(["enableLinkTracking"]);
          (function() {
              var u="https://matomo.carpentries.org/";
              _paq.push(["setTrackerUrl", u+"matomo.php"]);
              _paq.push(["setSiteId", "1"]);
              var d=document, g=d.createElement("script"), s=d.getElementsByTagName("script")[0];
              g.async=true; g.src="https://matomo.carpentries.org/matomo.js"; s.parentNode.insertBefore(g,s);
          })();
        </script><!-- End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

