<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Text Analysis in Python: Training Word2Vec</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css"><script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/cp/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="favicons/cp/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="favicons/cp/favicon-16x16.png"><link rel="manifest" href="favicons/cp/site.webmanifest"><link rel="mask-icon" href="favicons/cp/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav carpentries"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="cp-logo" alt="Lesson Description" src="assets/images/carpentries-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/09-wordEmbed_train-word2vec.html';">Instructor View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav carpentries" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="assets/images/carpentries-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Text Analysis in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Text Analysis in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><li><a class="dropdown-item" href="discuss.html">Discussion</a></li><li><a class="dropdown-item" href="reference.html">Glossary</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Text Analysis in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 54%" class="percentage">
    54%
  </div>
  <div class="progress carpentries">
    <div class="progress-bar carpentries" role="progressbar" style="width: 54%" aria-valuenow="54" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/09-wordEmbed_train-word2vec.html">Instructor View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-basicConcepts.html">1. Introduction to Natural Language Processing</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-apis.html">2. Corpus Development- Text Data Collection</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-preprocessing.html">3. Preparing and Preprocessing Your Data</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-vectorSpace.html">4. Vector Space and Distance</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-tf-idf-documentEmbeddings.html">5. Document Embeddings and TF-IDF</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-lsa.html">6. Latent Semantic Analysis</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-wordEmbed_intro.html">7. Intro to Word Embeddings</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="08-wordEmbed_word2vec-algorithm.html">8. The Word2Vec Algorithm</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        9. Training Word2Vec
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#colab-setup">Colab Setup</a></li>
<li><a href="#preprocessing-steps">Preprocessing steps</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush11">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading11">
        <a href="10-finetuning-transformers.html">10. Finetuning LLMs</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush12">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading12">
        <a href="11-ethics.html">11. Ethics and Text Analysis</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="discuss.html">Discussion</a></li><li><a href="reference.html">Glossary</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="08-wordEmbed_word2vec-algorithm.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="10-finetuning-transformers.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="08-wordEmbed_word2vec-algorithm.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: The Word2Vec
        </a>
        <a class="chapter-link float-end" href="10-finetuning-transformers.html" rel="next">
          Next: Finetuning LLMs...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Training Word2Vec</h1>
        <p>Last updated on 2025-03-21 |

        <a href="https://github.com/josenino95/python-text-analysis/edit/main/episodes/09-wordEmbed_train-word2vec.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>How can we train a Word2Vec model?</li>
<li>When is it beneficial to train a Word2Vec model on a specific
dataset?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Understand the benefits of training a Word2Vec model on your own
data rather than using a pre-trained model</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="colab-setup">Colab Setup<a class="anchor" aria-label="anchor" href="#colab-setup"></a></h2>
<hr class="half-width"><p>Run this code to enable helper functions and read data back in.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># Run this cell to mount your Google Drive.</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>drive.mount(<span class="st">'/content/drive'</span>)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># Show existing colab notebooks and helpers.py file</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> os <span class="im">import</span> listdir</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>wksp_dir <span class="op">=</span> <span class="st">'/content/drive/My Drive/Colab Notebooks/text-analysis/code'</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="bu">print</span>(listdir(wksp_dir))</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co"># Add folder to colab's path so we can import the helper functions</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>sys.path.insert(<span class="dv">0</span>, wksp_dir)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Mounted at /content/drive
['analysis.py',
 'pyldavis.py',
 '.gitkeep',
 'helpers.py',
 'preprocessing.py',
 'attentionviz.py',
 'mit_restaurants.py',
 'plotfrequency.py',
 '__pycache__']</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># pip install necessary to access parse module (called from helpers.py)</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="op">!</span>pip install parse</span></code></pre>
</div>
<div class="section level3">
<h3 id="load-in-the-data">Load in the data<a class="anchor" aria-label="anchor" href="#load-in-the-data"></a></h3>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># Read the data back in.</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> read_csv</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>data <span class="op">=</span> read_csv(<span class="st">"/content/drive/My Drive/Colab Notebooks/text-analysis/data/data.csv"</span>)</span></code></pre>
</div>
<p>Create list of files we’ll use for our analysis. We’ll start by
fitting a word2vec model to just one of the books in our list — Moby
Dick.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>single_file <span class="op">=</span> data.loc[data[<span class="st">'Title'</span>] <span class="op">==</span> <span class="st">'moby_dick'</span>,<span class="st">'File'</span>].item()</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>single_file</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="st">'/content/drive/My Drive/Colab Notebooks/text-analysis/data/melville-moby_dick.txt'</span></span></code></pre>
</div>
<p>Let’s preview the file contents to make sure our code and directory
setup is working correctly.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># open and read file</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>f <span class="op">=</span> <span class="bu">open</span>(single_file,<span class="st">'r'</span>)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>file_contents <span class="op">=</span> f.read()</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>f.close()</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="co"># preview file contents</span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>preview_len <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="bu">print</span>(file_contents[<span class="dv">0</span>:preview_len])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[Moby Dick by Herman Melville 1851]


ETYMOLOGY.

(Supplied by a Late Consumptive Usher to a Grammar School)

The pale Usher--threadbare in coat, heart, body, and brain; I see him
now.  He was ever dusting his old lexicons and grammars, with a queer
handkerchief, mockingly embellished with all the gay flags of all the
known nations of the world.  He loved to dust his old grammars; it
somehow mildly reminded him of his mortality.

"While you take in hand to school others, and to teach them by wha</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>file_contents[<span class="dv">0</span>:preview_len] <span class="co"># Note that \n are still present in actual string (print() processes these as new lines)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="st">'[Moby Dick by Herman Melville 1851]\n\n\nETYMOLOGY.\n\n(Supplied by a Late Consumptive Usher to a Grammar School)\n\nThe pale Usher--threadbare in coat, heart, body, and brain; I see him\nnow.  He was ever dusting his old lexicons and grammars, with a queer\nhandkerchief, mockingly embellished with all the gay flags of all the\nknown nations of the world.  He loved to dust his old grammars; it\nsomehow mildly reminded him of his mortality.\n\n"While you take in hand to school others, and to teach them by wha'</span></span></code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="preprocessing-steps">Preprocessing steps<a class="anchor" aria-label="anchor" href="#preprocessing-steps"></a></h2>
<hr class="half-width"><ol style="list-style-type: decimal"><li>Split text into sentences</li>
<li>Tokenize the text</li>
<li>Lemmatize and lowercase all tokens</li>
<li>Remove stop words</li>
</ol><div class="section level3">
<h3 id="convert-text-to-list-of-sentences">1. Convert text to list of sentences<a class="anchor" aria-label="anchor" href="#convert-text-to-list-of-sentences"></a></h3>
<p>Remember that we are using the sequence of words in a sentence to
learn meaningful word embeddings. The last word of one sentence does not
always relate to the first word of the next sentence. For this reason,
we will split the text into individual sentences before going
further.</p>
<div class="section level4">
<h4 id="punkt-sentence-tokenizer">Punkt Sentence Tokenizer<a class="anchor" aria-label="anchor" href="#punkt-sentence-tokenizer"></a></h4>
<p>NLTK’s sentence tokenizer (‘punkt’) works well in most cases, but it
may not correctly detect sentences when there is a complex paragraph
that contains many punctuation marks, exclamation marks, abbreviations,
or repetitive symbols. It is not possible to define a standard way to
overcome these issues. If you want to ensure every “sentence” you use to
train the Word2Vec is truly a sentence, you would need to write some
additional (and highly data-dependent) code that uses regex and string
manipulation to overcome rare errors.</p>
<p>For our purposes, we’re willing to overlook a few sentence
tokenization errors. If this work were being published, it would be
worthwhile to double-check the work of punkt.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>nltk.download(<span class="st">'punkt'</span>) <span class="co"># dependency of sent_tokenize function</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>sentences <span class="op">=</span> nltk.sent_tokenize(file_contents)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>sentences[<span class="dv">300</span>:<span class="dv">305</span>]</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['How then is this?',
 'Are the green fields gone?',
 'What do they\nhere?',
 'But look!',
 'here come more crowds, pacing straight for the water, and\nseemingly bound for a dive.']</code></pre>
</div>
</div>
</div>
<div class="section level3">
<h3 id="tokenize-lemmatize-and-remove-stop-words">2-4: Tokenize, lemmatize, and remove stop words<a class="anchor" aria-label="anchor" href="#tokenize-lemmatize-and-remove-stop-words"></a></h3>
<p>Pull up preprocess text helper function and unpack the code…</p>
<ul><li>We’ll run this function on each sentence</li>
<li>Lemmatization, tokenization, lowercase and stopwords are all
review</li>
<li>For the lemmatization step, we’ll use NLTK’s lemmatizer which runs
very quickly</li>
<li>We’ll also use NLTK’s stop word lists and its tokenization function.
Recall that stop words are usually thought of as the most common words
in a language. By removing them, we can let the Word2Vec model focus on
sequences of meaningful words, only.</li>
</ul><div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="im">from</span> helpers <span class="im">import</span> preprocess_text</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="co"># test function</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>string <span class="op">=</span> <span class="st">'It is not down on any map; true places never are.'</span></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>tokens <span class="op">=</span> preprocess_text(string, </span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>                         remove_stopwords<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>                         verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Result'</span>, tokens)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="va">Tokens</span> <span class="op">[</span><span class="st">'It'</span>, <span class="st">'is'</span>, <span class="st">'not'</span>, <span class="st">'down'</span>, <span class="st">'on'</span>, <span class="st">'any'</span>, <span class="st">'map'</span>, <span class="st">'true'</span>, <span class="st">'places'</span>, <span class="st">'never'</span>, <span class="st">'are'</span><span class="op">]</span></span>
<span><span class="va">Lowercase</span> <span class="op">[</span><span class="st">'it'</span>, <span class="st">'is'</span>, <span class="st">'not'</span>, <span class="st">'down'</span>, <span class="st">'on'</span>, <span class="st">'any'</span>, <span class="st">'map'</span>, <span class="st">'true'</span>, <span class="st">'places'</span>, <span class="st">'never'</span>, <span class="st">'are'</span><span class="op">]</span></span>
<span><span class="va">Lemmas</span> <span class="op">[</span><span class="st">'it'</span>, <span class="st">'is'</span>, <span class="st">'not'</span>, <span class="st">'down'</span>, <span class="st">'on'</span>, <span class="st">'any'</span>, <span class="st">'map'</span>, <span class="st">'true'</span>, <span class="st">'place'</span>, <span class="st">'never'</span>, <span class="st">'are'</span><span class="op">]</span></span>
<span><span class="va">StopRemoved</span> <span class="op">[</span><span class="st">'map'</span>, <span class="st">'true'</span>, <span class="st">'place'</span>, <span class="st">'never'</span><span class="op">]</span></span>
<span><span class="va">Result</span> <span class="op">[</span><span class="st">'map'</span>, <span class="st">'true'</span>, <span class="st">'place'</span>, <span class="st">'never'</span><span class="op">]</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="co"># convert list of sentences to pandas series so we can use the apply functionality</span></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>sentences_series <span class="op">=</span> pd.Series(sentences)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>tokens_cleaned <span class="op">=</span> sentences_series.<span class="bu">apply</span>(preprocess_text, </span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>                                        remove_stopwords<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>                                        verbose<span class="op">=</span><span class="va">False</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="co"># view sentences before clearning</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>sentences[<span class="dv">300</span>:<span class="dv">305</span>]</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['How then is this?',
 'Are the green fields gone?',
 'What do they\nhere?',
 'But look!',
 'here come more crowds, pacing straight for the water, and\nseemingly bound for a dive.']</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="co"># view sentences after cleaning</span></span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>tokens_cleaned[<span class="dv">300</span>:<span class="dv">305</span>]</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>    300                                                   []
    301                                 [green, field, gone]
    302                                                   []
    303                                               [look]
    304    [come, crowd, pacing, straight, water, seeming...
    dtype: object</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>tokens_cleaned.shape <span class="co"># 9852 sentences</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(9852,)</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="co"># remove empty sentences and 1-word sentences (all stop words)</span></span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>tokens_cleaned <span class="op">=</span> tokens_cleaned[tokens_cleaned.<span class="bu">apply</span>(<span class="bu">len</span>) <span class="op">&gt;</span> <span class="dv">1</span>]</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>tokens_cleaned.shape</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(9007,)</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="train-word2vec-model-using-tokenized-text">Train Word2Vec model using tokenized text<a class="anchor" aria-label="anchor" href="#train-word2vec-model-using-tokenized-text"></a></h3>
<p>We can now use this data to train a word2vec model. We’ll start by
importing the Word2Vec module from gensim. We’ll then hand the Word2Vec
function our list of tokenized sentences and set sg=0 (“skip-gram”) to
use the continuous bag of words (CBOW) training method.</p>
<p><strong>Set seed and workers for a fully deterministic run</strong>:
Next we’ll set some parameters for reproducibility. We’ll set the seed
so that our vectors get randomly initialized the same way each time this
code is run. For a fully deterministically-reproducible run, we’ll also
limit the model to a single worker thread (workers=1), to eliminate
ordering jitter from OS thread scheduling — noted in <a href="https://radimrehurek.com/gensim/models/word2vec.html" class="external-link">gensim’s
documentation</a></p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="co"># import gensim's Word2Vec module</span></span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a><span class="co"># train the word2vec model with our cleaned data</span></span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(sentences<span class="op">=</span>tokens_cleaned, seed<span class="op">=</span><span class="dv">0</span>, workers<span class="op">=</span><span class="dv">1</span>, sg<span class="op">=</span><span class="dv">0</span>)</span></code></pre>
</div>
<p>Gensim’s implementation is based on the original <a href="%22https://arxiv.org/pdf/1301.3781.pdf%22" class="external-link">Tomas Mikolov’s
original model of word2vec</a>, which downsamples all frequent words
automatically based on frequency. The downsampling saves time when
training the model.</p>
</div>
<div class="section level3">
<h3 id="next-steps-word-embedding-use-cases">Next steps: word embedding use-cases<a class="anchor" aria-label="anchor" href="#next-steps-word-embedding-use-cases"></a></h3>
<p>We now have a vector representation for all the (lemmatized and
non-stop words) words referenced throughout Moby Dick. Let’s see how we
can use these vectors to gain insights from our text data.</p>
</div>
<div class="section level3">
<h3 id="most-similar-words">Most similar words<a class="anchor" aria-label="anchor" href="#most-similar-words"></a></h3>
<p>Just like with the pretrained word2vec models, we can use the
most_similar function to find words that meaningfully relate to a
queried word.</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="co"># default</span></span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a>model.wv.most_similar(positive<span class="op">=</span>[<span class="st">'whale'</span>], topn<span class="op">=</span><span class="dv">10</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('great', 0.9986481070518494),
 ('white', 0.9984517097473145),
 ('fishery', 0.9984385371208191),
 ('sperm', 0.9984176158905029),
 ('among', 0.9983417987823486),
 ('right', 0.9983320832252502),
 ('three', 0.9983301758766174),
 ('day', 0.9983181357383728),
 ('length', 0.9983041882514954),
 ('seen', 0.998255729675293)]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="vocabulary-limits">Vocabulary limits<a class="anchor" aria-label="anchor" href="#vocabulary-limits"></a></h3>
<p>Note that Word2Vec can only produce vector representations for words
encountered in the data used to train the model.</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a>model.wv.most_similar(positive<span class="op">=</span>[<span class="st">'orca'</span>],topn<span class="op">=</span><span class="dv">30</span>) </span></code></pre>
</div>
<pre><code><span><span class="va">KeyError</span><span class="op">:</span> <span class="st">"Key 'orca' not present in vocabulary"</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="fasttext-solves-oov-issue">fastText solves OOV issue<a class="anchor" aria-label="anchor" href="#fasttext-solves-oov-issue"></a></h3>
<p>If you need to obtain word vectors for out of vocabulary (OOV) words,
you can use the fastText word embedding model, instead (also provided
from Gensim). The fastText model can obtain vectors even for
out-of-vocabulary (OOV) words, by summing up vectors for its component
char-ngrams, provided at least one of the char-ngrams was present in the
training data.</p>
</div>
<div class="section level3">
<h3 id="word2vec-for-named-entity-recognition">Word2Vec for Named Entity Recognition<a class="anchor" aria-label="anchor" href="#word2vec-for-named-entity-recognition"></a></h3>
<p>What can we do with this most similar functionality? One way we can
use it is to construct a list of similar words to represent some sort of
category. For example, maybe we want to know what other sea creatures
are referenced throughout Moby Dick. We can use gensim’s most_smilar
function to begin constructing a list of words that, on average,
represent a “sea creature” category.</p>
<p>We’ll use the following procedure:</p>
<ol style="list-style-type: decimal"><li>Initialize a small list of words that represent the category, sea
creatures.</li>
<li>Calculate the average vector representation of this list of
words</li>
<li>Use this average vector to find the top N most similar vectors
(words)</li>
<li>Review similar words and update the sea creatures list</li>
<li>Repeat steps 1-4 until no additional sea creatures can be found</li>
</ol><div class="codewrapper sourceCode" id="cb33">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a><span class="co"># start with a small list of words that represent sea creatures </span></span>
<span id="cb33-2"><a href="#cb33-2" tabindex="-1"></a>sea_creatures <span class="op">=</span> [<span class="st">'whale'</span>,<span class="st">'fish'</span>,<span class="st">'creature'</span>,<span class="st">'animal'</span>]</span>
<span id="cb33-3"><a href="#cb33-3" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" tabindex="-1"></a><span class="co"># The below code will calculate an average vector of the words in our list, </span></span>
<span id="cb33-5"><a href="#cb33-5" tabindex="-1"></a><span class="co"># and find the vectors/words that are most similar to this average vector</span></span>
<span id="cb33-6"><a href="#cb33-6" tabindex="-1"></a>model.wv.most_similar(positive<span class="op">=</span>sea_creatures, topn<span class="op">=</span><span class="dv">30</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('great', 0.9997826814651489),
 ('part', 0.9997532963752747),
 ('though', 0.9997507333755493),
 ('full', 0.999735951423645),
 ('small', 0.9997267127037048),
 ('among', 0.9997209906578064),
 ('case', 0.9997204542160034),
 ('like', 0.9997190833091736),
 ('many', 0.9997131824493408),
 ('fishery', 0.9997081756591797),
 ('present', 0.9997068643569946),
 ('body', 0.9997056722640991),
 ('almost', 0.9997050166130066),
 ('found', 0.9997038245201111),
 ('whole', 0.9997023940086365),
 ('water', 0.9996949434280396),
 ('even', 0.9996913075447083),
 ('time', 0.9996898174285889),
 ('two', 0.9996897578239441),
 ('air', 0.9996871948242188),
 ('length', 0.9996850490570068),
 ('vast', 0.9996834397315979),
 ('line', 0.9996828436851501),
 ('made', 0.9996813535690308),
 ('upon', 0.9996812343597412),
 ('large', 0.9996775984764099),
 ('known', 0.9996767640113831),
 ('harpooneer', 0.9996761679649353),
 ('sea', 0.9996750354766846),
 ('shark', 0.9996744990348816)]</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb35">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a><span class="co"># we can add shark to our list</span></span>
<span id="cb35-2"><a href="#cb35-2" tabindex="-1"></a>model.wv.most_similar(positive<span class="op">=</span>[<span class="st">'whale'</span>,<span class="st">'fish'</span>,<span class="st">'creature'</span>,<span class="st">'animal'</span>,<span class="st">'shark'</span>],topn<span class="op">=</span><span class="dv">30</span>) </span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('great', 0.9997999668121338),
 ('though', 0.9997922778129578),
 ('part', 0.999788761138916),
 ('full', 0.999781608581543),
 ('small', 0.9997766017913818),
 ('like', 0.9997683763504028),
 ('among', 0.9997652769088745),
 ('many', 0.9997631311416626),
 ('case', 0.9997614622116089),
 ('even', 0.9997515678405762),
 ('body', 0.9997514486312866),
 ('almost', 0.9997509717941284),
 ('present', 0.9997479319572449),
 ('found', 0.999747633934021),
 ('water', 0.9997465014457703),
 ('made', 0.9997431635856628),
 ('air', 0.9997406601905823),
 ('whole', 0.9997400641441345),
 ('fishery', 0.9997299909591675),
 ('harpooneer', 0.9997295141220093),
 ('time', 0.9997290372848511),
 ('two', 0.9997289776802063),
 ('sea', 0.9997265934944153),
 ('strange', 0.9997244477272034),
 ('large', 0.999722421169281),
 ('place', 0.9997209906578064),
 ('dead', 0.9997198581695557),
 ('leviathan', 0.9997192025184631),
 ('sometimes', 0.9997178316116333),
 ('high', 0.9997177720069885)]</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb37">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a><span class="co"># add leviathan (sea serpent) to our list</span></span>
<span id="cb37-2"><a href="#cb37-2" tabindex="-1"></a>model.wv.most_similar(positive<span class="op">=</span>[<span class="st">'whale'</span>,<span class="st">'fish'</span>,<span class="st">'creature'</span>,<span class="st">'animal'</span>,<span class="st">'shark'</span>,<span class="st">'leviathan'</span>],topn<span class="op">=</span><span class="dv">30</span>) </span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('though', 0.9998274445533752),
 ('part', 0.9998168349266052),
 ('full', 0.9998133182525635),
 ('small', 0.9998107552528381),
 ('great', 0.9998067021369934),
 ('like', 0.9998064041137695),
 ('even', 0.9997999668121338),
 ('many', 0.9997966885566711),
 ('body', 0.9997950196266174),
 ('among', 0.999794602394104),
 ('found', 0.9997929334640503),
 ('case', 0.9997885823249817),
 ('almost', 0.9997871518135071),
 ('made', 0.9997868537902832),
 ('air', 0.999786376953125),
 ('water', 0.9997802972793579),
 ('whole', 0.9997780919075012),
 ('present', 0.9997757077217102),
 ('harpooneer', 0.999768853187561),
 ('place', 0.9997684955596924),
 ('much', 0.9997658729553223),
 ('time', 0.999765157699585),
 ('sea', 0.999765157699585),
 ('dead', 0.999764621257782),
 ('strange', 0.9997624158859253),
 ('high', 0.9997615218162537),
 ('two', 0.999760091304779),
 ('sometimes', 0.9997592568397522),
 ('half', 0.9997562170028687),
 ('vast', 0.9997541904449463)]</code></pre>
</div>
<p>No additional sea creatures. It appears we have our list of sea
creatures recovered using Word2Vec</p>
<div class="section level4">
<h4 id="limitations">Limitations<a class="anchor" aria-label="anchor" href="#limitations"></a></h4>
<p>There is at least one sea creature missing from our list — a giant
squid. The giant squid is only mentioned a handful of times throughout
Moby Dick, and therefore it could be that our word2vec model was not
able to train a good representation of the word “squid”. Neural networks
only work well when you have lots of data</p>
<div id="exploring-the-skip-gram-algorithm" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exploring-the-skip-gram-algorithm" class="callout-inner">
<h3 class="callout-title">Exploring the skip-gram algorithm</h3>
<div class="callout-content">
<p>The skip-gram algoritmm sometimes performs better in terms of its
ability to capture meaning of rarer words encountered in the training
data. Train a new Word2Vec model using the skip-gram algorithm, and see
if you can repeat the above categorical search task to find the word,
“squid”.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb39">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" tabindex="-1"></a><span class="co"># import gensim's Word2Vec module</span></span>
<span id="cb39-2"><a href="#cb39-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb39-3"><a href="#cb39-3" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" tabindex="-1"></a><span class="co"># train the word2vec model with our cleaned data</span></span>
<span id="cb39-5"><a href="#cb39-5" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(sentences<span class="op">=</span>tokens_cleaned, seed<span class="op">=</span><span class="dv">0</span>, workers<span class="op">=</span><span class="dv">1</span>, sg<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb39-6"><a href="#cb39-6" tabindex="-1"></a>model.wv.most_similar(positive<span class="op">=</span>[<span class="st">'whale'</span>,<span class="st">'fish'</span>,<span class="st">'creature'</span>,<span class="st">'animal'</span>,<span class="st">'shark'</span>,<span class="st">'leviathan'</span>],topn<span class="op">=</span><span class="dv">100</span>) <span class="co"># still no sight of squid </span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('whalemen', 0.9931729435920715),
 ('specie', 0.9919217824935913),
 ('bulk', 0.9917919635772705),
 ('ground', 0.9913252592086792),
 ('skeleton', 0.9905602931976318),
 ('among', 0.9898401498794556),
 ('small', 0.9887762665748596),
 ('full', 0.9885162115097046),
 ('captured', 0.9883950352668762),
 ('found', 0.9883666634559631),
 ('sometimes', 0.9882548451423645),
 ('snow', 0.9880553483963013),
 ('magnitude', 0.9880378842353821),
 ('various', 0.9878063201904297),
 ('hump', 0.9876748919487),
 ('cuvier', 0.9875931739807129),
 ('fisherman', 0.9874721765518188),
 ('general', 0.9873012900352478),
 ('living', 0.9872495532035828),
 ('wholly', 0.9872384667396545),
 ('bone', 0.987160861492157),
 ('mouth', 0.9867696762084961),
 ('natural', 0.9867129921913147),
 ('monster', 0.9865870475769043),
 ('blubber', 0.9865683317184448),
 ('indeed', 0.9864518046379089),
 ('teeth', 0.9862186908721924),
 ('entire', 0.9861844182014465),
 ('latter', 0.9859246015548706),
 ('book', 0.9858523607254028)]</code></pre>
</div>
<p><strong>Discuss Exercise Result</strong>: When using Word2Vec to
reveal items from a category, you risk missing items that are rarely
mentioned. This is true even when we use the Skip-gram training method,
which has been found to have better performance on rarer words. For this
reason, it’s sometimes better to save this task for larger text
corpuses. In a later lesson, we will explore how large language models
(LLMs) can yield better performance on Named Entity Recognition related
tasks.</p>
</div>
</div>
</div>
</div>
<div id="entity-recognition-applications" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="entity-recognition-applications" class="callout-inner">
<h3 class="callout-title">Entity Recognition Applications</h3>
<div class="callout-content">
<p>How else might you exploit this kind of analysis in your research?
Share your ideas with the group.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p><strong>Example</strong>: Train a model on newspaper articles from
the 19th century, and collect a list of foods (the topic chosen)
referenced throughout the corpus. Do the same for 20th century newspaper
articles and compare to see how popular foods have changed over
time.</p>
</div>
</div>
</div>
</div>
<div id="comparing-vector-representations-across-authors" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="comparing-vector-representations-across-authors" class="callout-inner">
<h3 class="callout-title">Comparing Vector Representations Across Authors</h3>
<div class="callout-content">
<p>Recall that the Word2Vec model learns to encode a word’s
meaning/representation based on that word’s most common surrounding
context words. By training two separate Word2Vec models on, e.g., books
collected from two different authors (one model for each author), we can
compare how the different authors tend to use words differently. What
are some research questions or words that we could investigate with this
kind of approach?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>As one possible approach, we could compare how authors tend to
represent different genders. It could be that older (outdated!) books
tend to produce word vectors for man and women that are further apart
from one another than newer books due to historic gender norms.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="other-word-embedding-models">Other word embedding models<a class="anchor" aria-label="anchor" href="#other-word-embedding-models"></a></h3>
<p>While Word2Vec is a famous model that is still used throughout many
NLP applications today, there are a few other word embedding models that
you might also want to consider exploring. GloVe and fastText are among
the two most popular choices to date.</p>
<div class="codewrapper sourceCode" id="cb41">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" tabindex="-1"></a><span class="co"># Preview other word embedding models available</span></span>
<span id="cb41-2"><a href="#cb41-2" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>(api.info()[<span class="st">'models'</span>].keys()))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']</code></pre>
</div>
<div class="section level4">
<h4 id="similarities">Similarities<a class="anchor" aria-label="anchor" href="#similarities"></a></h4>
<ul><li>All three algorithms generate vector representations of words in a
high-dimensional space.</li>
<li>They can be used to solve a wide range of natural language
processing tasks.</li>
<li>They are all open-source and widely used in the research
community.</li>
</ul></div>
<div class="section level4">
<h4 id="differences">Differences<a class="anchor" aria-label="anchor" href="#differences"></a></h4>
<ul><li>Word2Vec focuses on generating embeddings by predicting the context
of a word given its surrounding words in a sentence, while GloVe and
fastText generate embeddings by predicting the probability of a word
co-occurring with another word in a corpus.</li>
<li>fastText also includes character n-grams, allowing it to generate
embeddings for words not seen during training, making it particularly
useful for handling out-of-vocabulary words.</li>
<li>In general, fastText is considered to be the fastest to train among
the three embedding techniques (GloVe, fastText, and Word2Vec). This is
because fastText uses subword information, which reduces the vocabulary
size and allows the model to handle out-of-vocabulary words.
Additionally, fastText uses a hierarchical softmax for training, which
is faster than the traditional softmax used by Word2Vec. Finally,
fastText can be trained on multiple threads, further speeding up the
training process.</li>
</ul><div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul><li>As an alternative to using a pre-trained model, training a Word2Vec
model on a specific dataset allows you use Word2Vec for NER-related
tasks.</li>
</ul></div>
</div>
</div>
</div>
</div>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="08-wordEmbed_word2vec-algorithm.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="10-finetuning-transformers.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="08-wordEmbed_word2vec-algorithm.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: The Word2Vec
        </a>
        <a class="chapter-link float-end" href="10-finetuning-transformers.html" rel="next">
          Next: Finetuning LLMs...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/josenino95/python-text-analysis/edit/main/episodes/09-wordEmbed_train-word2vec.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/josenino95/python-text-analysis/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/josenino95/python-text-analysis/" class="external-link">Source</a></p>
				<p><a href="https://github.com/josenino95/python-text-analysis/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.11" class="external-link">sandpaper (0.16.11)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.5" class="external-link">varnish (1.0.5)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://josenino95.github.io/python-text-analysis/09-wordEmbed_train-word2vec.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "Training Word2Vec",
  "creativeWorkStatus": "active",
  "url": "https://josenino95.github.io/python-text-analysis/09-wordEmbed_train-word2vec.html",
  "identifier": "https://josenino95.github.io/python-text-analysis/09-wordEmbed_train-word2vec.html",
  "dateCreated": "2020-10-12",
  "dateModified": "2025-03-21",
  "datePublished": "2025-04-29"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo --><script>
          var _paq = window._paq = window._paq || [];
          /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
          _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
          _paq.push(["setDomains", ["*.lessons.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
          _paq.push(["setDoNotTrack", true]);
          _paq.push(["disableCookies"]);
          _paq.push(["trackPageView"]);
          _paq.push(["enableLinkTracking"]);
          (function() {
              var u="https://matomo.carpentries.org/";
              _paq.push(["setTrackerUrl", u+"matomo.php"]);
              _paq.push(["setSiteId", "1"]);
              var d=document, g=d.createElement("script"), s=d.getElementsByTagName("script")[0];
              g.async=true; g.src="https://matomo.carpentries.org/matomo.js"; s.parentNode.insertBefore(g,s);
          })();
        </script><!-- End Matomo Code --></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

