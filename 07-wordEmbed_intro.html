<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Text Analysis in Python: Intro to Word Embeddings</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css"><script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/cp/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="favicons/cp/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="favicons/cp/favicon-16x16.png"><link rel="manifest" href="favicons/cp/site.webmanifest"><link rel="mask-icon" href="favicons/cp/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav carpentries"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="cp-logo" alt="Lesson Description" src="assets/images/carpentries-logo.svg"><span class="badge text-bg-info">
          <abbr title="This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-circle" style="border-radius: 5px"></i>
              Beta
            </a>
            <span class="visually-hidden">This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>


      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/07-wordEmbed_intro.html';">Instructor View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav carpentries" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="assets/images/carpentries-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Text Analysis in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Text Analysis in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><li><a class="dropdown-item" href="discuss.html">Discussion</a></li><li><a class="dropdown-item" href="reference.html">Glossary</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Text Analysis in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 35%" class="percentage">
    35%
  </div>
  <div class="progress carpentries">
    <div class="progress-bar carpentries" role="progressbar" style="width: 35%" aria-valuenow="35" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/07-wordEmbed_intro.html">Instructor View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-basicConcepts.html">1. Introduction to Natural Language Processing</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-apis.html">2. Corpus Development- Text Data Collection</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-preprocessing.html">3. Preparing and Preprocessing Your Data</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-vectorSpace.html">4. Vector Space and Distance</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-tf-idf-documentEmbeddings.html">5. Document Embeddings and TF-IDF</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-lsa.html">6. Latent Semantic Analysis</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        7. Intro to Word Embeddings
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#load-pre-trained-model-via-gensim">Load pre-trained model via Gensim</a></li>
<li><a href="#documentcorpus-embeddings-recap">Document/Corpus Embeddings Recap</a></li>
<li><a href="#word-embeddings-with-word2vec">Word embeddings with Word2Vec</a></li>
<li><a href="#preliminary-considerations">Preliminary Considerations</a></li>
<li><a href="#exploring-word2vec-in-python">Exploring Word2Vec in Python</a></li>
<li><a href="#recap">Recap</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="08-wordEmbed_word2vec-algorithm.html">8. The Word2Vec Algorithm</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush10">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading10">
        <a href="09-wordEmbed_train-word2vec.html">9. Training Word2Vec</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush11">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading11">
        <a href="10-finetuning-transformers.html">10. Finetuning LLMs</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush12">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading12">
        <a href="11-ethics.html">11. Ethics and Text Analysis</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="discuss.html">Discussion</a></li><li><a href="reference.html">Glossary</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="06-lsa.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="08-wordEmbed_word2vec-algorithm.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="06-lsa.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Latent Semantic
        </a>
        <a class="chapter-link float-end" href="08-wordEmbed_word2vec-algorithm.html" rel="next">
          Next: The Word2Vec...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Intro to Word Embeddings</h1>
        <p>Last updated on 2025-05-01 |

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/episodes/07-wordEmbed_intro.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>How can we extract vector representations of individual words rather
than documents?</li>
<li>What sort of research questions can be answered with word embedding
models?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Understand the difference between document embeddings and word
embeddings</li>
<li>Introduce the Gensim python library and its word embedding
functionality</li>
<li>Explore vector math with word embeddings using pretrained
models</li>
<li>Visualize word embeddings with the help of principal component
analysis (PCA)</li>
<li>Discuss word embedding use-cases</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="load-pre-trained-model-via-gensim">Load pre-trained model via Gensim<a class="anchor" aria-label="anchor" href="#load-pre-trained-model-via-gensim"></a></h2>
<hr class="half-width"><p>First, load the Word2Vec embedding model. The Word2Vec model takes
3-10 minutes to load.</p>
<p>We’ll be using the Gensim library. The Gensim library comes with
several word embedding models including Word2Vec, GloVe, and fastText.
We’ll start by exploring one of the pre-trained Word2Vec models. We’ll
discuss the other options later in this lesson. Gensim is an older
library that is a little finnicky. We need to make sure we have the
right version of numpy to go along with it. This next cell will install
gensim and a compatible version of numpy.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="op">!</span>pip uninstall <span class="op">-</span>y numpy gensim</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="op">!</span>rm <span class="op">-</span>rf <span class="op">/</span>usr<span class="op">/</span>local<span class="op">/</span>lib<span class="op">/</span>python3<span class="fl">.11</span><span class="op">/</span>dist<span class="op">-</span>packages<span class="op">/</span>numpy<span class="op">*</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">--</span>no<span class="op">-</span>cache<span class="op">-</span><span class="bu">dir</span> numpy<span class="op">==</span><span class="fl">1.26.4</span> gensim<span class="op">==</span><span class="fl">4.3.3</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="load-google-news-model-word2vec">Load Google News model (Word2Vec)<a class="anchor" aria-label="anchor" href="#load-google-news-model-word2vec"></a></h3>
<p><strong>Note</strong>: You may need to restart the kernel after
installing gensim (above cell) for the import statement (below cell) to
work.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># RUN BEFORE INTRO LECTURE :)</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co"># api to load word2vec models</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co"># takes 3-10 minutes to load</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>wv <span class="op">=</span> api.load(<span class="st">'word2vec-google-news-300'</span>) <span class="co"># takes 3-10 minutes to load </span></span></code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="documentcorpus-embeddings-recap">Document/Corpus Embeddings Recap<a class="anchor" aria-label="anchor" href="#documentcorpus-embeddings-recap"></a></h2>
<hr class="half-width"><p>So far, we’ve seen how word counts (bag of words), TF-IDF, and LSA
can help us embed a document or set of documents into useful vector
spaces that allow us to gain insights from text data. Let’s review the
embeddings covered thus far…</p>
<ul><li><p><strong>Word count embeddings</strong>: Word count embeddings are
a simple yet powerful method that represent text data as a sparse vector
where each dimension corresponds to a unique word in the vocabulary, and
the value in each dimension indicates the frequency of that word in the
document. This approach disregards word order and context, treating each
document as an unordered collection of words or tokens.</p></li>
<li><p><strong>TF-IDF embeddings:</strong> Term Frequency Inverse
Document Frequency (TF-IDF) is a fancier word-count method. It
emphasizes words that are both frequent within a specific document
<em>and</em> rare across the entire corpus.</p></li>
<li><p><strong>LSA embeddings:</strong> Latent Semantic Analysis (LSA)
is used to find the hidden topics represented by a group of documents.
It involves running singular-value decomposition (SVD) on a
document-term matrix (typically the TF-IDF matrix), producing a vector
representation of each document. This vector scores each document’s
representation in different topic/concept areas which are derived based
on word co-occurences (e.g., 45% topic A, 35% topic B, and 20% topic C).
Importantly, LSA is considered a <em>bag of words</em> method since the
order of words in a document is not considered.</p></li>
</ul><p>To get a high-level overview of the embedding methods covered thus
far, study the table below:</p>
<table class="table"><colgroup><col width="21%"><col width="11%"><col width="13%"><col width="16%"><col width="25%"><col width="11%"></colgroup><thead><tr class="header"><th align="center">Technique</th>
<th align="center">Input</th>
<th align="center">Embedding Structure</th>
<th align="center">Output Vector Dimensions</th>
<th align="center">Meaning Stored</th>
<th align="center">Order Dependency</th>
</tr></thead><tbody><tr class="odd"><td align="center">Word Counts</td>
<td align="center">Raw text corpus</td>
<td align="center">Sparse vectors</td>
<td align="center">[1, Vocabulary Size] <br>(per document)</td>
<td align="center">Word presence in documents</td>
<td align="center">No <br>(bag of words)</td>
</tr><tr class="even"><td align="center">TF-IDF</td>
<td align="center">Word Counts</td>
<td align="center">Sparse vectors</td>
<td align="center">[1, Vocabulary Size] <br>(per document)</td>
<td align="center">Importance of terms in documents</td>
<td align="center">No <br>(bag of words)</td>
</tr><tr class="odd"><td align="center">Latent Semantic Analysis (LSA)</td>
<td align="center">TF-IDF or similar</td>
<td align="center">Dense vectors</td>
<td align="center">[1, Number of Topics] <br>(per document)</td>
<td align="center">Semantic topics present in documents</td>
<td align="center">No <br>(bag of words)</td>
</tr></tbody></table><div class="section level3">
<h3 id="bag-of-words-limitations">Bag of Words limitations<a class="anchor" aria-label="anchor" href="#bag-of-words-limitations"></a></h3>
<p>In all of these emebdding methods, notice how the order of words in
sentences does not matter. We are simply tossing all words in a corpus
into a bag (“bag of words”) and attempting to glean insights from this
bag of words. While such an approach can be effective for revealing
broad topics/concepts from text, additional features of language may be
revealed by zooming in on the context in which words appear throughout a
text.</p>
<p>For instance, maybe our bag of words contains the following: “cook”,
“I”, “family”, “my”, “to”, “dinner”, “love”, and “for”. Depending on how
these words are arranged, the meaning conveyed will change
drastically!</p>
<ul><li><em>I love to cook dinner for my family.</em></li>
<li><em>I love to cook family for my dinner.</em></li>
</ul></div>
<div class="section level3">
<h3 id="distributional-hypothesis-extracting-more-meaningful-representations-of-text">Distributional hypothesis: extracting more meaningful
representations of text<a class="anchor" aria-label="anchor" href="#distributional-hypothesis-extracting-more-meaningful-representations-of-text"></a></h3>
<p>To clarify whether our text is about a nice wholesome family or a
cannibal on the loose, we need to include context in our embeddings. As
the famous linguist JR Firth once said, “You shall know a word by the
company it keeps.” Firth is referring to the <em>distributional
hypothesis</em>, which states that words that repeatedly occur in
similar contexts probably have similar meanings. While the LSA
methodology is inspired by the distributional hypothesis, LSA ignores
the context of words as they appear in sentences and only pays attention
to global word co-occurence patterns across large chunks of texts. If we
want to truly know a word based on the company it keeps, we’ll need to
take into account how some words are more likely to appear before/after
other words in a sentence. We’ll explore how one of the most famous
embedding models, Word2Vec, does this in this episode.</p>
</div>
</section><section><h2 class="section-heading" id="word-embeddings-with-word2vec">Word embeddings with Word2Vec<a class="anchor" aria-label="anchor" href="#word-embeddings-with-word2vec"></a></h2>
<hr class="half-width"><p>Word2vec is a famous <em>word embedding</em> method that was created
and published in the ancient year of 2013 by a team of researchers led
by Tomas Mikolov at Google over two papers, [<a href="https://arxiv.org/abs/1301.3781" class="external-link">1</a>, <a href="https://arxiv.org/abs/1310.4546" class="external-link">2</a>]. Unlike with TF-IDF and
LSA, which are typically used to produce document and corpus embeddings,
Word2Vec focuses on producing a single embedding for every word
encountered in a corpus. These embeddings, which are represented as
high-dimesional vectors, tend to look very similar for words that are
used in similar contexts. Adding this method to our overview table, we
get:</p>
<table class="table"><colgroup><col width="21%"><col width="11%"><col width="13%"><col width="16%"><col width="25%"><col width="11%"></colgroup><thead><tr class="header"><th align="center">Technique</th>
<th align="center">Input</th>
<th align="center">Embedding Structure</th>
<th align="center">Output Vector Dimensions</th>
<th align="center">Meaning Stored</th>
<th align="center">Order Dependency</th>
</tr></thead><tbody><tr class="odd"><td align="center">Word Counts</td>
<td align="center">Raw text corpus</td>
<td align="center">Sparse vectors</td>
<td align="center">[1, Vocabulary Size] <br>(per document)</td>
<td align="center">Word presence in documents</td>
<td align="center">No <br>(bag of words)</td>
</tr><tr class="even"><td align="center">TF-IDF</td>
<td align="center">Word Counts</td>
<td align="center">Sparse vectors</td>
<td align="center">[1, Vocabulary Size] <br>(per document)</td>
<td align="center">Importance of terms in documents</td>
<td align="center">No <br>(bag of words)</td>
</tr><tr class="odd"><td align="center">Latent Semantic Analysis (LSA)</td>
<td align="center">TF-IDF or similar</td>
<td align="center">Dense vectors</td>
<td align="center">[1, Number of Topics] <br>(per document)</td>
<td align="center">Semantic topics present in documents</td>
<td align="center">No <br>(bag of words)</td>
</tr><tr class="even"><td align="center">Word2Vec</td>
<td align="center">Raw text corpus</td>
<td align="center">Dense vectors</td>
<td align="center">[1, Embedding Dimension] <br>(per word)</td>
<td align="center">Semantic meaning of words</td>
<td align="center">Yes <br>(word order)</td>
</tr></tbody></table><p>The next <em>supplemental</em> episode unpacks the technology behind
Word2Vec — neural networks. In the interest of time, we will only cover
the key concepts and intuition. Please consider studying the next
episode if you are interested in learning more about the fascinating
world of neural networks and how they actually work. For now, it is
sufficient to be aware of few key insights.</p>
<div class="section level3">
<h3 id="neural-networks-have-an-exceptional-ability-to-learn-functions-that-can-map-a-set-of-input-features-to-some-output-">1. Neural networks have an exceptional ability to learn functions
that can map a set of input features to some output.<a class="anchor" aria-label="anchor" href="#neural-networks-have-an-exceptional-ability-to-learn-functions-that-can-map-a-set-of-input-features-to-some-output-"></a></h3>
<p>Because of this general capability, they can be used for a wide
assortment of tasks including…</p>
<ul><li>Predicting the weather tomorrow given historical weather
patterns</li>
<li>Classifying if an email is spam or not</li>
<li>Classifying if an image contains a person or not</li>
<li>Predicting a person’s weight based on their height, age, location,
etc.</li>
<li>Predicting commute times given traffic conditions</li>
<li>Predicting house prices given stock market prices</li>
</ul></div>
<div class="section level3">
<h3 id="neural-networks-learn-new-meaningful-features-from-the-input-data-">2. Neural networks <em>learn</em> new meaningful features from the
input data.<a class="anchor" aria-label="anchor" href="#neural-networks-learn-new-meaningful-features-from-the-input-data-"></a></h3>
<p>Specifically, the learned features will be features that are useful
for whatever task the model is assigned. With this consideration, we can
devise a language related task that allows a neural network model to
learn interesting features of words which can then be extracted from the
model as a word embedding representation (i.e., a vector).</p>
<p><strong>What task can we give a neural network to learn meaningful
word embeddings?</strong> Our friend RJ Firth gives us a hint when he
says, “You shall know a word by the company it keeps.” Using the
<em>distributional hypothesis</em> as motivation, which states that
words that repeatedly occur in similar contexts probably have similar
meanings, we can ask a neural network to predict the <em>context</em>
words that surround a given word in a sentence or, similarly, ask it to
predict the <em>center</em> word based on <em>context</em> words. Both
variants are shown below — Skip Gram and Continous Bag of Words
(CBOW).</p>
<figure><img src="fig/wordEmbed_NN-training-methods.png" alt="Skipgram" class="figure mx-auto d-block"></figure><div class="section level4">
<h4 id="learning-a-vector-representation-of-the-word-outside">Learning a vector representation of the word, “outside”<a class="anchor" aria-label="anchor" href="#learning-a-vector-representation-of-the-word-outside"></a></h4>
<p>Word2Vec is an neural network model that <em>learns</em>
high-dimensional (many features) vector representations of
<em>individual words</em> based on observing a word’s most likely
surrounding words in multiple sentences (dist. hypothesis). For
instance, suppose we want to learn a vector representation of the word
“outside”. For this, we would train the Word2Vec model on many sentences
containing the word, “outside”.</p>
<ul><li><em>It’s a beautiful day <strong>outside</strong>, perfect for a
picnic.</em></li>
<li><em>My cat loves to spend time <strong>outside</strong>, chasing
birds and bugs.</em></li>
<li><em>The noise <strong>outside</strong> woke me up early this
morning.</em></li>
<li><em>I always feel more relaxed after spending some time
<strong>outside</strong> in nature.</em></li>
<li><em>I can hear the rain pouring <strong>outside</strong>, it’s a
good day to stay indoors.</em></li>
<li><em>The sun is shining brightly <strong>outside</strong>, it’s time
to put on some sunscreen.</em></li>
<li><em>I saw a group of kids playing <strong>outside</strong> in the
park.</em></li>
<li><em>It’s not safe to leave your belongings <strong>outside</strong>
unattended.</em></li>
<li><em>I love to go for a walk <strong>outside</strong> after dinner to
help me digest.</em></li>
<li><em>The temperature <strong>outside</strong> is dropping, I need to
grab a jacket before I leave.</em></li>
</ul><p>In the process of training, the model’s weights learn to derive new
features (weight optimized perceptrons) associated with the input data
(single words). These new learned features will be conducive to
accurately predicting the context words for each word. In addition, the
features can be used as a information-rich vector representation of the
word, “outside”.</p>
<p><strong>Skip-gram versus Continuous Bag of Words</strong>: The
primary difference between these two approaches lies in how CBOW and
Skip-gram handle the context words for each target word. In CBOW, the
context words are averaged together to predict the target word, while in
Skip-gram, each context word is considered separately to predict the
target word. While both CBOW and Skip-gram consider each word-context
pair during training, Skip-gram often performs better with rare words
because it treats each occurrence of a word separately, generating more
training examples for rare words compared to CBOW. This can lead to
better representations of rare words in Skip-gram embeddings.</p>
</div>
</div>
<div class="section level3">
<h3 id="the-vectors-learned-by-the-model-are-a-reflection-of-the-models-past-experience-">3. The vectors learned by the model are a reflection of the model’s
past experience.<a class="anchor" aria-label="anchor" href="#the-vectors-learned-by-the-model-are-a-reflection-of-the-models-past-experience-"></a></h3>
<p>Past experience = the specific data the model was “trained” on. This
means that the vectors extracted from the model will reflect, on
average, how words are used in a specific text. For example, notice how
in the example sentences given above, the word “outside” tends to be
surrounded by words associated with the outdoors.</p>
</div>
<div class="section level3">
<h3 id="the-learned-features-or-vectors-are-black-boxes-lacking-direct-interpretability-">4. The learned features or vectors are <em>black boxes</em>, lacking
direct interpretability.<a class="anchor" aria-label="anchor" href="#the-learned-features-or-vectors-are-black-boxes-lacking-direct-interpretability-"></a></h3>
<p>The learned vectors create useful and meaningful representations of
words, capturing semantic relationships based on word co-occurrences.
However, these vectors represent abstract features learned from the
surrounding context of words in the training data, and are not directly
interpretable. Still, once we have language mapped to a numerical space,
we can compare things on a relative scale and ask a variety of reserach
questions.</p>
<div id="word2vec-applications" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="word2vec-applications" class="callout-inner">
<h3 class="callout-title">Word2Vec Applications</h3>
<div class="callout-content">
<p>Take a few minutes to think about different types of questions or
problems that could be addressed using Word2Vec and word embeddings.
Share your thoughts and suggestions with the class.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ul><li>
<strong>Semantic Change Over Time</strong>: How have the meanings of
words evolved over different historical periods? By training Word2Vec
models on texts from different time periods, researchers can analyze how
word embeddings change over time, revealing shifts in semantic
usage.</li>
<li>Authorship Attribution: Can Word2Vec be used to identify the authors
of anonymous texts or disputed authorship works? By comparing the word
embeddings of known authors’ works with unknown texts, researchers can
potentially attribute authorship based on stylistic similarities (e.g.,
<a href="https://arxiv.org/pdf/2209.11717.pdf" class="external-link">Agrawal et al., 2023</a>
and <a href="https://arxiv.org/pdf/1704.00177v1.pdf" class="external-link">Liu,
2017</a>).</li>
<li>
<strong>Authorship Attribution</strong>: Word2Vec has been applied
to authorship attribution tasks (e.g., <a href="https://arxiv.org/abs/2310.16972" class="external-link">Tripto and Ali, 2023</a>).</li>
<li>
<strong>Comparative Analysis of Multilingual Texts</strong>:
Word2Vec enables cross-lingual comparisons. Researchers have explored
multilingual embeddings to study semantic differences between languages
(e.g., <a href="https://arxiv.org/pdf/1912.10169.pdf" class="external-link">Heijden et al.,
2019</a>).</li>
<li>
<strong>Studying Cultural Concepts and Biases</strong>: Word2Vec
helps uncover cultural biases in language. Researchers have examined
biases related to race, religion, and colonialism (e.g., <a href="https://link.springer.com/article/10.1007/s00146-022-01443-w" class="external-link">Petreski
and Hashim, 2022</a>).</li>
</ul></div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="preliminary-considerations">Preliminary Considerations<a class="anchor" aria-label="anchor" href="#preliminary-considerations"></a></h2>
<hr class="half-width"><p>In determining whether or not Word2Vec is a suitable embedding method
for your research, it’s important to consider the following:</p>
<ul><li>
<strong>Analysis Relevance</strong>: Does examining the
relationships and meanings among words serve as a guideline for your
research? Are you able to pinpoint specific terms or clusters of terms
that encapsulate the broader conceptual realms you are
investigating?</li>
<li>
<strong>Data Quality</strong>: Ensure that your text corpus is of
high quality. Garbage or noisy data can adversely affect Word2Vec
embeddings.</li>
<li>
<strong>Corpus Size</strong>: Word2Vec performs better with larger
corpora. Having substantial text data improves the quality of learned
word vectors.</li>
<li>
<strong>Domain-Specific Data Availability</strong>: Choose a dataset
relevant to your DH research. If you’re analyzing historical texts, use
historical documents. For sentiment analysis, domain-specific data
matters.</li>
</ul></section><section><h2 class="section-heading" id="exploring-word2vec-in-python">Exploring Word2Vec in Python<a class="anchor" aria-label="anchor" href="#exploring-word2vec-in-python"></a></h2>
<hr class="half-width"><p>With that said, let’s see what we can do with meaningful word
vectors. The pre-trained model we loaded earlier was trained on a Google
News dataset (about 100 billion words). We loaded this model as the
variable <code>wv</code> earlier. Let’s check the type of this
object.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(wv))</span></code></pre>
</div>
<p>Gensim stores “KeyedVectors” representing the Word2Vec model. They’re
called keyed vectors because you can use words as keys to extract the
corresponding vectors. Let’s take a look at the vector representaton of
<em>whale</em>.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>wv[<span class="st">'whale'</span>] </span></code></pre>
</div>
<p>We can also check the shape of this vector with…</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="bu">print</span>(wv[<span class="st">'whale'</span>].shape) </span></code></pre>
</div>
<p>In this model, each word has a 300-dimensional representation. You
can think of these 300 dimensions as 300 different features that encode
a word’s meaning. Unlike LSA, which produces (somewhat) interpretable
features (i.e., topics) relevant to a text, the features produced by
Word2Vec will be treated as a black box. That is, we won’t actually know
what each dimension of the vector represents. However, if the vectors
have certain desirable properties (e.g., similar words produce similar
vectors), they can still be very useful. Let’s check this with the help
of the cosine similarity measure.</p>
<p><strong>Cosine Similarity (Review)</strong>: Recall from earlier in
the workshop that cosine similarity helps evaluate vector similarity in
terms of the angle that separates the two vectors, irrespective of
vector magnitude. It can take a value ranging from -1 to 1, with…</p>
<ul><li>1 indicating that the two vectors share the same angle</li>
<li>0 indicating that the two vectors are perpendicular or 90 degrees to
one another</li>
<li>-1 indicating that the two vectors are 180 degrees apart.</li>
</ul><p>Words that occur in similar contexts should have similar
vectors/embeddings. How similar are the word vectors representing
<em>whale</em> and <em>dolphin</em>?</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>wv.similarity(<span class="st">'whale'</span>,<span class="st">'dolphin'</span>)</span></code></pre>
</div>
<p>How about <em>whale</em> and <em>fish</em>?</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>wv.similarity(<span class="st">'whale'</span>,<span class="st">'fish'</span>)</span></code></pre>
</div>
<p>How about <em>whale</em> and… <em>potato</em>?</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>wv.similarity(<span class="st">'whale'</span>,<span class="st">'potato'</span>)</span></code></pre>
</div>
<p>Our similarity scale seems to be on the right track. We can also use
the similarity function to quickly extract the top N most similar words
to <em>whale</em>.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>wv.most_similar(positive<span class="op">=</span>[<span class="st">'whale'</span>], topn<span class="op">=</span><span class="dv">10</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('whales', 0.8474178910255432),
 ('humpback_whale', 0.7968777418136597),
 ('dolphin', 0.7711714506149292),
 ('humpback', 0.7535837292671204),
 ('minke_whale', 0.7365031838417053),
 ('humpback_whales', 0.7337379455566406),
 ('dolphins', 0.7213870882987976),
 ('humpbacks', 0.7138717174530029),
 ('shark', 0.7011443376541138),
 ('orca', 0.7007412314414978)]</code></pre>
</div>
<p>Based on our ability to recover similar words, it appears the
Word2Vec embedding method produces fairly good (i.e., semantically
meaningful) word representations.</p>
<div id="exploring-words-with-multiple-meanings" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="exploring-words-with-multiple-meanings" class="callout-inner">
<h3 class="callout-title">Exploring Words With Multiple Meanings</h3>
<div class="callout-content">
<p>Use Gensim’s <code>most_similar</code> function to find the top 10
most similar words to each of the following words (separately): “bark”,
“pitcher”, “park”. Note that all of these words have multiple meanings
depending on their context. Does Word2Vec capture the meaning of these
words well? Why or why not?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>wv.most_similar(positive<span class="op">=</span>[<span class="st">'bark'</span>], topn<span class="op">=</span><span class="dv">15</span>) <span class="co"># all seem to reflect tree bark</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>wv.most_similar(positive<span class="op">=</span>[<span class="st">'park'</span>], topn<span class="op">=</span><span class="dv">15</span>) <span class="co"># all seem to reflect outdoor parks</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>wv.most_similar(positive<span class="op">=</span>[<span class="st">'pitcher'</span>], topn<span class="op">=</span><span class="dv">15</span>) <span class="co"># all seem to reflect baseball pitching</span></span></code></pre>
</div>
<p>Based on these three lists, it looks like Word2Vec is biased towards
representing the predominant meaning or sense of a word. In fact, the
Word2Vec does not explicitly differentiate between multiple meanings of
a word during training. Instead, it treats each occurrence of a word in
the training corpus as a distinct symbol, regardless of its meaning. As
a result, resulting embeddings may be biased towards the most frequent
meaning or sense of a word. This is because the more frequent a word
sense appears in the training data, the more opportunities the algorithm
has to learn that particular meaning.</p>
<p>Note that while this can be a limitation of Word2Vec, there are some
techniques that can be applied to incorporate word sense disambiguation.
One common approach is to train multiple embeddings for a word, where
each embedding corresponds to a specific word sense. This can be done by
pre-processing the training corpus to annotate word senses, and then
training Word2Vec embeddings separately for each sense. This approach
allows Word2Vec to capture different word senses as separate vectors,
effectively representing the polysemy of the word.</p>
</div>
</div>
</div>
</div>
<div id="word2vec-applications-in-digital-humanities" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="word2vec-applications-in-digital-humanities" class="callout-inner">
<h3 class="callout-title">Word2Vec Applications in Digital
Humanities</h3>
<div class="callout-content">
<p>From the above exercise, we see that the vectors produced by Word2Vec
will reflect how words are typically used in a specific dataset. By
training Word2Vec on large corpora of text from historical documents,
literary works, or cultural artifacts, researchers can uncover semantic
relationships between words and analyze word usage patterns over time,
across genres, or within specific cultural contexts.</p>
<p>Taking this into consideration, what are some possible ways we could
make use of Word2Vec to explore newspaper articles from the years
1900-2000?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>One possible approach with this data is to investigate how the
meaning of certain words can evolve over time by training separate
models for different chunks of time (e.g., 1900-1950, 1951-2000, etc.).
A few words that have changed their meaning over time include:</p>
<ul><li>Nice: This word used to mean “silly, foolish, simple.”</li>
<li>Silly: In its earliest uses, it referred to things worthy or
blessed; from there it came to refer to the weak and vulnerable, and
more recently to those who are foolish.</li>
<li>Awful: Awful things used to be “worthy of awe”.</li>
</ul><p>We’ll explore how training a Word2Vec model on specific texts can
yield insights into those texts later in this lesson.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="adding-and-subtracting-vectors-king---man-woman-queen">Adding and Subtracting Vectors: King - Man + Woman = Queen<a class="anchor" aria-label="anchor" href="#adding-and-subtracting-vectors-king---man-woman-queen"></a></h3>
<p>We can also add and subtract word vectors to reveal latent meaning in
words. As a canonical example, let’s see what happens if we take the
word vector representing <em>King</em>, subtract the <em>Man</em> vector
from it, and then add the <em>Woman</em> vector to the result. We should
get a new vector that closely matches the word vector for
<em>Queen</em>. We can test this idea out in Gensim with:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="bu">print</span>(wv.most_similar(positive<span class="op">=</span>[<span class="st">'woman'</span>,<span class="st">'king'</span>], negative<span class="op">=</span>[<span class="st">'man'</span>], topn<span class="op">=</span><span class="dv">3</span>))</span></code></pre>
</div>
<p>Behind the scenes of the most_similar function, Gensim first unit
normalizes the <em>length</em> of all vectors included in the positive
and negative function arguments. This is done before adding/subtracting,
which prevents longer vectors from unjustly skewing the sum. Note that
length here refers to the linear algebraic definition of summing the
squared values of each element in a vector followed by taking the square
root of that sum.</p>
</div>
<div class="section level3">
<h3 id="visualizing-word-vectors-with-pca">Visualizing word vectors with PCA<a class="anchor" aria-label="anchor" href="#visualizing-word-vectors-with-pca"></a></h3>
<p>Similar to how we visualized our texts in the previous lesson to show
how they relate to one another, we can visualize how a sample of words
relate by plotting their respecitve word vectors.</p>
<p>Let’s start by extracting some word vectors from the pre-trained
Word2Vec model.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>words <span class="op">=</span> [<span class="st">'man'</span>,<span class="st">'woman'</span>,<span class="st">'boy'</span>,<span class="st">'girl'</span>,<span class="st">'king'</span>,<span class="st">'queen'</span>,<span class="st">'prince'</span>,<span class="st">'princess'</span>]</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>sample_vectors <span class="op">=</span> np.array([wv[word] <span class="cf">for</span> word <span class="kw">in</span> words])</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>sample_vectors.shape <span class="co"># 8 words, 300 dimensions </span></span></code></pre>
</div>
<p>Recall that each word vector has 300 dimensions that encode a word’s
meaning. Considering humans can only visualize up to 3 dimensions, this
dataset presents a plotting challenge. We could certainly try plotting
just the first 2 dimensions or perhaps the dimensions with the largest
amount of variability, but this would overlook a lot of the information
stored in the other dimensions/variables. Instead, we can use a
<em>dimensionality-reduction</em> technique known as Principal Component
Analysis (PCA) to allow us to capture most of the information in the
data with just 2 dimensions.</p>
<div class="section level4">
<h4 id="principal-component-analysis-pca">Principal Component Analysis (PCA)<a class="anchor" aria-label="anchor" href="#principal-component-analysis-pca"></a></h4>
<p>Principal Component Analysis (PCA) is a data transformation technique
that allows you to linearly combine a set of variables from a matrix
(<em>N</em> observations and <em>M</em> variables) into a smaller set of
variables called components. Specifically, it remaps the data onto new
dimensions that are strictly orthogonal to one another and can be
ordered according to the amount of information or variance they carry.
The allows you to easily visualize <em>most</em> of the variability in
the data with just a couple of dimensions.</p>
<p>We’ll use scikit-learn’s (a popular machine learning library) PCA
functionality to explore the power of PCA, and matplotlib as our
plotting library.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre>
</div>
<p>In the code below, we will assess how much variance is stored in each
dimension following PCA. The new dimensions are often referred to as
principal components or eigenvectors, which relates to the underlying
math behind this algorithm.</p>
<p>Notice how the first two dimensions capture around 70% of the
variability in the dataset.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>pca <span class="op">=</span> PCA() <span class="co"># init PCA object</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>pca.fit(sample_vectors) <span class="co"># the fit function determines the new dimensions or axes to represent the data -- the result is sent back to the pca object</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a><span class="co"># Calculate cumulative variance explained</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>cumulative_variance_explained <span class="op">=</span> np.cumsum(pca.explained_variance_ratio_)<span class="op">*</span><span class="dv">100</span></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="co"># Plot cumulative variance explained</span></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a>plt.figure()</span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(cumulative_variance_explained) <span class="op">+</span> <span class="dv">1</span>), cumulative_variance_explained, <span class="st">'-o'</span>)</span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of Principal Components"</span>)</span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a>plt.ylabel(<span class="st">"Cumulative Variance Explained (%)"</span>)</span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a>plt.title(<span class="st">"Cumulative Variance Explained by Principal Components"</span>)</span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/wordEmbeddings_word2vecPCA_cumulative_variance_explained.jpg" alt="PCA Variance Explained" class="figure mx-auto d-block"></figure><p>We can now use these new dimensions to transform the original
data.</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="co"># transform the data</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>result <span class="op">=</span> pca.transform(sample_vectors)</span></code></pre>
</div>
<p>Once transformed, we can plot the first two principal components
representing each word in our list:
<code>['man', 'woman', 'boy', 'girl', 'king', 'queen', 'prince', 'princess']</code></p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>plt.figure()</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>plt.scatter(result[:,<span class="dv">0</span>], result[:,<span class="dv">1</span>])</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>  plt.annotate(word, xy<span class="op">=</span>(result[i, <span class="dv">0</span>], result[i, <span class="dv">1</span>]))</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>plt.xlabel(<span class="st">"PC1"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>plt.ylabel(<span class="st">"PC2"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/wordEmbed_PCAviz.jpg" alt="Visualizing Word Embeddings with PCA" class="figure mx-auto d-block"></figure><p>Note how the principal component 1 seems to represent the royalty
dimension, while the principal component 2 seems to represent male vs
female.</p>
</div>
</div>
</section><section><h2 class="section-heading" id="recap">Recap<a class="anchor" aria-label="anchor" href="#recap"></a></h2>
<hr class="half-width"><p>In summary, Word2Vec is a powerful text-embedding method that allows
researchers to explore how different words relate to one another based
on past observations (i.e., by being trained on a large list of
sentences). Unlike LSA, which produces topics as features of the text to
investigate, Word2Vec produces “black-box” features which have to be
compared relative to one another. By training Word2Vec text from
historical documents, literary works, or cultural artifacts, researchers
can uncover semantic relationships between words and analyze word usage
patterns over time, across genres, or within specific cultural
contexts.</p>
<p>In the next section, we’ll explore the technology behind Word2Vec
before training a Word2Vec model on some of the text data used in this
workshop.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul><li>Word emebddings can help us derive additional meaning stored in text
at the level of individual words</li>
<li>Word embeddings have many use-cases in text-analysis and NLP related
tasks</li>
</ul></div>
</div>
</div>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="06-lsa.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="08-wordEmbed_word2vec-algorithm.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="06-lsa.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Latent Semantic
        </a>
        <a class="chapter-link float-end" href="08-wordEmbed_word2vec-algorithm.html" rel="next">
          Next: The Word2Vec...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries-incubator/python-text-analysis/edit/main/episodes/07-wordEmbed_intro.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries-incubator/python-text-analysis/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/python-text-analysis/" class="external-link">Source</a></p>
        <p>
        <a href="https://github.com/carpentries-incubator/python-text-analysis/blob/main/" class="external-link">Cite</a>
        | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
		</div>
		<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.3" class="external-link">sandpaper (0.17.3)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.9" class="external-link">varnish (1.0.9)</a></p>
		</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://carpentries-incubator.github.io/python-text-analysis/07-wordEmbed_intro.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "Intro to Word Embeddings",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/python-text-analysis/07-wordEmbed_intro.html",
  "identifier": "https://carpentries-incubator.github.io/python-text-analysis/07-wordEmbed_intro.html",
  "dateCreated": "2020-10-12",
  "dateModified": "2025-05-01",
  "datePublished": "2025-12-09"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo --><script>
          var _paq = window._paq = window._paq || [];
          /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
          _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
          _paq.push(["setDomains", ["*.lessons.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
          _paq.push(["setDoNotTrack", true]);
          _paq.push(["disableCookies"]);
          _paq.push(["trackPageView"]);
          _paq.push(["enableLinkTracking"]);
          (function() {
              var u="https://matomo.carpentries.org/";
              _paq.push(["setTrackerUrl", u+"matomo.php"]);
              _paq.push(["setSiteId", "1"]);
              var d=document, g=d.createElement("script"), s=d.getElementsByTagName("script")[0];
              g.async=true; g.src="https://matomo.carpentries.org/matomo.js"; s.parentNode.insertBefore(g,s);
          })();
        </script><!-- End Matomo Code --></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

