<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Text Analysis in Python: The Word2Vec Algorithm</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css"><script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/cp/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="favicons/cp/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="favicons/cp/favicon-16x16.png"><link rel="manifest" href="favicons/cp/site.webmanifest"><link rel="mask-icon" href="favicons/cp/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav carpentries"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="cp-logo" alt="Lesson Description" src="assets/images/carpentries-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/08-wordEmbed_word2vec-algorithm.html';">Instructor View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav carpentries" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="assets/images/carpentries-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Text Analysis in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Text Analysis in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><li><a class="dropdown-item" href="discuss.html">Discussion</a></li><li><a class="dropdown-item" href="reference.html">Glossary</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Text Analysis in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 44%" class="percentage">
    44%
  </div>
  <div class="progress carpentries">
    <div class="progress-bar carpentries" role="progressbar" style="width: 44%" aria-valuenow="44" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/08-wordEmbed_word2vec-algorithm.html">Instructor View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-basicConcepts.html">1. Introduction to Natural Language Processing</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-apis.html">2. Corpus Development- Text Data Collection</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-preprocessing.html">3. Preparing and Preprocessing Your Data</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-vectorSpace.html">4. Vector Space and Distance</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-tf-idf-documentEmbeddings.html">5. Document Embeddings and TF-IDF</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-lsa.html">6. Latent Semantic Analysis</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-wordEmbed_intro.html">7. Intro to Word Embeddings</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        8. The Word2Vec Algorithm
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#related-carpentries-workshops">Related Carpentries workshops</a></li>
<li><a href="#mapping-inputs-to-outputs-using-neural-networks">Mapping inputs to outputs using neural networks</a></li>
<li><a href="#the-perceptron">The Perceptron</a></li>
<li><a href="#the-multilayer-perceptron-mlp">The multilayer perceptron (MLP)</a></li>
<li><a href="#deriving-new-features-from-neural-networks">Deriving New Features from Neural Networks</a></li>
<li><a href="#training-word2vec-to-learn-word-embeddings">Training Word2Vec to Learn Word Embeddings</a></li>
<li><a href="#recap">Recap</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush10">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading10">
        <a href="09-wordEmbed_train-word2vec.html">9. Training Word2Vec</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush11">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading11">
        <a href="10-finetuning-transformers.html">10. Finetuning LLMs</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush12">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading12">
        <a href="11-ethics.html">11. Ethics and Text Analysis</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="discuss.html">Discussion</a></li><li><a href="reference.html">Glossary</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="07-wordEmbed_intro.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="09-wordEmbed_train-word2vec.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="07-wordEmbed_intro.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Intro to Word
        </a>
        <a class="chapter-link float-end" href="09-wordEmbed_train-word2vec.html" rel="next">
          Next: Training Word2Vec...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>The Word2Vec Algorithm</h1>
        <p>Last updated on 2025-03-21 |

        <a href="https://github.com/josenino95/python-text-analysis/edit/main/episodes/08-wordEmbed_word2vec-algorithm.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>How does the Word2Vec model produce meaningful word embeddings?</li>
<li>How is a Word2Vec model trained?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Introduce artificial neural networks and their structure.</li>
<li>Understand the two training methods employed by the Word2Vec, CBOW
and Skip-gram.</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="related-carpentries-workshops">Related Carpentries workshops<a class="anchor" aria-label="anchor" href="#related-carpentries-workshops"></a></h2>
<hr class="half-width"><p>We could spend an entire workshop on neural networks (see <a href="https://carpentries-incubator.github.io/machine-learning-novice-sklearn/06-neural-networks/index.html" class="external-link">here</a>
and <a href="https://carpentries-incubator.github.io/deep-learning-intro/" class="external-link">here</a>
for a couple of related lessons). Here, we will distill some of the most
important concepts needed to understand them in the context of
text-analysis.</p>
</section><section><h2 class="section-heading" id="mapping-inputs-to-outputs-using-neural-networks">Mapping inputs to outputs using neural networks<a class="anchor" aria-label="anchor" href="#mapping-inputs-to-outputs-using-neural-networks"></a></h2>
<hr class="half-width"><p>How is it that Word2Vec is able to represent words in such a
semantically meaningful way? The key technology behind Word2Vec is an
<strong>artificial neural network</strong>. Neural networks are highly
prevalent in many fields now due to their exceptional ability to learn
functions that can map a set of input features to some output (e.g., a
label or predicted value for some target variable). Because of this
general capability, they can be used for a wide assortment of tasks
including…</p>
<ul><li>Predicting the weather tomorrow given historical weather
patterns</li>
<li>Classifying if an email is spam or not</li>
<li>Classifying if an image contains a person or not</li>
<li>Predicting a person’s weight based on their height</li>
<li>Predicting commute times given traffic conditions</li>
<li>Predicting house prices given stock market prices</li>
</ul><div class="section level3">
<h3 id="supervised-learning">Supervised learning<a class="anchor" aria-label="anchor" href="#supervised-learning"></a></h3>
<p>Most machine learning systems “learn” by taking tabular input data
with N observations (rows), M features (cols), and an associated output
(e.g., a class label or predicted value for some target variable), and
using it to form a model. The maths behind the machine learning doesn’t
care what the data is as long as it can represented numerically or
categorised. When the model learns this function based on observed data,
we call this “training” the model.</p>
<div class="section level4">
<h4 id="training-dataset-example">Training Dataset Example<a class="anchor" aria-label="anchor" href="#training-dataset-example"></a></h4>
<p>As a example, maybe we have recorded tail lengths, weights, and snout
lengths from a disorganized vet clinic database that is missing some of
the animals’ labels (e.g., cat vs dog). For simplicity, let’s say that
this vet clinic only treats cats and dogs. With the help of neural
networks, we could use a labelled dataset to learn a function mapping
from tail length, weight, and snout length to the animal’s species label
(i.e., a cat or a dog).</p>
<table class="table"><colgroup><col width="26%"><col width="36%"><col width="28%"><col width="8%"></colgroup><thead><tr class="header"><th>Tail length (in)</th>
<th>Weight (lbs)</th>
<th>Snout length (in)</th>
<th>Label</th>
</tr></thead><tbody><tr class="odd"><td>12.2</td>
<td>10.1</td>
<td>1.1</td>
<td>cat</td>
</tr><tr class="even"><td>11.6</td>
<td>9.8</td>
<td>.82</td>
<td>cat</td>
</tr><tr class="odd"><td>9.5</td>
<td>61.2</td>
<td>2.6</td>
<td>dog</td>
</tr><tr class="even"><td>9.1</td>
<td>65.7</td>
<td>2.9</td>
<td>dog</td>
</tr><tr class="odd"><td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr><tr class="even"><td>11.2</td>
<td>12.1</td>
<td>.91</td>
<td>cat</td>
</tr></tbody></table><p>In the above table used to train a neural network model, the model
learns how best to map the observed features (tail length, weight, and
snout length) to their assigned classes. After the model is trained, it
can be used to infer the labels of unlabelled samples (so long as they
hae tail length, weight, and snouth length recorded).</p>
</div>
</div>
</section><section><h2 class="section-heading" id="the-perceptron">The Perceptron<a class="anchor" aria-label="anchor" href="#the-perceptron"></a></h2>
<hr class="half-width"><figure><img src="fig/wordEmbed_NN-perceptron.svg" alt="Single artificial neuron" class="figure mx-auto d-block"></figure><p>The diagram above shows a perceptron — the computational unit that
makes up artificial neural networks. Perceptrons are inspired by real
biological neurons. From the diagram, we can see that the
perceptron…</p>
<ul><li>
<strong>Input features</strong>: Receives multiple input features
and returns a single output</li>
<li>
<strong>Weights connecting features</strong>: Has adjustable weights
which scale the impact of individual inputs</li>
<li>
<strong>Nonlinear activation function</strong>: Has a nonlinear
activation function which takes as input, the weighted sum of inputs. If
the sum is above some threshold, the neuron “fires” a signal (outputs -1
or 1 which represents two different class labels)</li>
</ul><p>The goal then is to determine what specific weight values will allow
us to separate the two classes based on the input features (e.g., shown
below).</p>
<figure><img src="fig/wordEmbed_NN-cats-dogs-linear-boundary.png" alt="Linear Decision Boundary" class="figure mx-auto d-block"></figure><p><a href="https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/" class="external-link">Image
Source</a></p>
<p>In order to determine the optimal weights, we will need to “train”
the model on a labelled “training” dataset. As we pass each observation
in the training data to the model, the model is able to adjust its
weights in a direction that leads better performance. By training the
model on many observations, we can derive weights that can accurately
classify cats and dogs based on the observed input features. More
explicitly, its training method can be outlined as follows:</p>
<div class="section level3">
<h3 id="training-algorithm">Training algorithm<a class="anchor" aria-label="anchor" href="#training-algorithm"></a></h3>
<ol style="list-style-type: decimal"><li><p><strong>Initialize weights</strong>: The perceptron model starts
with randomly initialized weights. These weights are the
parameters/coefficients that the model will learn during training to
make accurate predictions.</p></li>
<li><p><strong>Input data</strong>: The perceptron model takes in the
input data, which consists of feature vectors representing the input
samples, and their corresponding labels or target values.</p></li>
<li><p><strong>Compute weighted sum</strong>: The model computes the
weighted sum of the input features by multiplying the feature values
with their corresponding weights, and summing them up. This is followed
by adding the bias term.</p></li>
<li><p><strong>Activation function</strong>: The perceptron model
applies an activation function, typically a step function or a threshold
function, to the computed weighted sum. The activation function
determines the output of the perceptron, usually producing a binary
output of 0 or 1.</p></li>
<li><p><strong>Compare with target label</strong>: The output of the
perceptron is compared with the target label of the input sample to
determine the prediction error. If the prediction is correct, no weight
updates are made. If the prediction is incorrect, the weights and bias
are updated to minimize the error.</p></li>
<li><p><strong>Update weights</strong>: The perceptron model updates the
weights based on a learning rate and the prediction error. The learning
rate determines the step size of the weight updates, and it is a
hyperparameter that needs to be tuned. The weights are updated using the
formula:</p></li>
</ol><div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>weight_new <span class="op">=</span> weight_old <span class="op">+</span> learning_rate <span class="op">*</span> (target <span class="op">-</span> prediction) <span class="op">*</span> feature</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="perceptron-limitations">Perceptron limitations<a class="anchor" aria-label="anchor" href="#perceptron-limitations"></a></h3>
<p>A single perceptron cannot solve any function that is not linearly
separable, meaning that we need to be able to divide the classes of
inputs and outputs with a straight line. To overcome this key limitation
of the perceptron (a single aritifical neuron), we need to stack
together multiple perceptrons in a hierarchical fashion. Such models are
referred to as <em>multilayer perceptrons</em> or simply <em>neural
networks</em></p>
</div>
</section><section><h2 class="section-heading" id="the-multilayer-perceptron-mlp">The multilayer perceptron (MLP)<a class="anchor" aria-label="anchor" href="#the-multilayer-perceptron-mlp"></a></h2>
<hr class="half-width"><p>To overcome the limitation of the perceptron, we can stack together
multiple perceptrons in a multilayer neural network (shown below) called
a multilayer perceptron (MLP). An MLP refers to a type of artificial
neural network (ANN) that consists of multiple layers of interconnected
nodes (neurons) organized in a feedforward manner. It typically has one
or more hidden layers between the input and output layers, with each
hidden layer applying an activation function to the weighted sum of its
inputs. By stacking together layers of perceptrons, the MLP model can
learn complex non-linear relationships in the data and make predictions
based on those learned patterns.</p>
<figure><img src="fig/wordEmbed_NN-MLP.svg" alt="Multilayer neural network" class="figure mx-auto d-block"></figure><p>In the diagram above, the general structure of a multilayer neural
network is shown with…</p>
<ul><li>
<strong>Input Layer</strong>: The input layer is the first layer of
the MLP and consists of input nodes that receive the features of the
input data. Each node in the input layer represents a feature or
attribute of the input data. The input layer is not involved in any
computation or activation; it simply passes the input features to the
next layer.</li>
<li>
<strong>Hidden Layer(s)</strong>: The hidden layers are the
intermediate layers between the input and output layers. In the above
diagram, there is only 1 hidden layer, but MLPs often have more. They
are called “hidden” because their outputs are not directly visible in
the input or output data. Each hidden layer consists of multiple nodes
(neurons) that compute a weighted sum of the inputs from the previous
layer, followed by an activation function. The number of hidden layers
and the number of nodes in each hidden layer are hyperparameters that
can be tuned during model design and training.</li>
<li>
<strong>Weighted Connections</strong>: Each connection between nodes
in adjacent layers has a weight associated with it. These weights are
the parameters that the model learns during training to determine the
strength of the connections. The weighted sum of inputs to a node is
computed by multiplying the input values with their corresponding
weights and summing them up. Also referred to as “weights” for
short.</li>
<li>
<strong>Weights</strong>: The weights of each neuron send its
(weighted) output to each neuron in the subsequent layer</li>
<li>
<strong>Output Layer</strong>: The output layer is the last layer of
the MLP and produces the final output of the model. It typically
consists of one or more nodes, depending on the specific task. For
binary classification, a single output node with a sigmoid activation
function is commonly used. For multi-class classification, multiple
output nodes with a softmax activation function are used. For regression
tasks, a single output node with a linear activation function is often
used.</li>
</ul><div class="section level3">
<h3 id="training-algorithm-1">Training algorithm<a class="anchor" aria-label="anchor" href="#training-algorithm-1"></a></h3>
<p>Similar to the perceptron, the MLP is trained using a supervised
learning algorithm that updates the weights iteratively based on the
prediction error of each training sample.</p>
<ol style="list-style-type: decimal"><li>
<strong>Initialization</strong>: The network’s weights are randomly
initialized.</li>
<li>
<strong>Forward Propagation</strong>: Input data is fed through the
network from input nodes to output nodes, with weights applied at each
connection, and the output is computed.</li>
<li>
<strong>Error Calculation</strong>: The difference between the
predicted output and the actual output (target) is calculated as the
error.</li>
<li>
<strong>Backpropagation</strong>: The error is propagated backward
through the network, and the weights are adjusted to minimize the
error.</li>
<li>
<strong>Iterative Process</strong>: Steps 2-4 are repeated for
multiple iterations or epochs, with input data fed through the network
and weights updated until the network’s performance converges to a
satisfactory level.</li>
<li>
<strong>Function Mapping</strong>: Once the network is trained, it
can be used to map new input data to corresponding outputs, leveraging
the learned weights.</li>
</ol></div>
</section><section><h2 class="section-heading" id="deriving-new-features-from-neural-networks">Deriving New Features from Neural Networks<a class="anchor" aria-label="anchor" href="#deriving-new-features-from-neural-networks"></a></h2>
<hr class="half-width"><p>After training a neural network, the neural weights encode new
features of the data that are conducive to performing well on whatever
task the neural network is given. This is due to the feedforward
processing built into the network — the outputs of previous layers are
sent to subsequent layers, and the so additional transformations get
applied to the original inputs as they transcend the network.</p>
<p>Generally speaking, the deeper the neural network is, the more
complicated/abstract these features can become. We call this a
<strong>hierarchical feature representation</strong>. For example, in
deep convolutional neural networks (a special kind of neural network
designed for image processing), the features in each layer look
something like the image shown below when the model is trained on a
facial recognition task.</p>
<figure><img src="fig/wordEmbed_NN-hierarchical-features.png" alt="Hierarchical Feature Representations - Face Detection" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="training-word2vec-to-learn-word-embeddings">Training Word2Vec to Learn Word Embeddings<a class="anchor" aria-label="anchor" href="#training-word2vec-to-learn-word-embeddings"></a></h2>
<hr class="half-width"><p>Recall that the ultimate goal of the Word2Vec method is to output
meaningful word embeddings/vectors. How can we train a neural network
for such a task? We could try to tediously hand-craft a large list of
word vectors that have the properties we seek (e.g., similar words have
similar vectors), and then train a neural network to learn this mapping
before applying it to new words. However, crafting a list of vectors
manually would be an arudous task. Furthermore, it is not immediately
clear what kind of vector representation would be best.</p>
<p>Instead, we can capitalize on the fact that neural networks are well
posed to learn new features from the input data. Specifically, the new
features will be features that are useful for whatever task the model is
assigned. With this consideration, we can devise a language related task
that allows a neural network model to learn interesting features of
words which can then be extracted from the model as a word embedding
representation (i.e., a vector). We’ll unpack how the embedding gets
extracted from the trained model shortly. For now, let’s focus on what
kind of language-related task to give the model.</p>
<div class="section level3">
<h3 id="predicting-context-words">Predicting context words<a class="anchor" aria-label="anchor" href="#predicting-context-words"></a></h3>
<p>What task can we give a neural network to learn meaningful word
embeddings? Our friend RJ Firth gives us a hint when he says, “You shall
know a word by the company it keeps.” Using the <em>distributional
hypothesis</em> as motivation, which states that words that repeatedly
occur in similar contexts probably have similar meanings, we can ask a
neural network to predict the <em>context</em> words that surround a
given word in a sentence. The Skip-gram algorithm shown on the right
side of the below diagram does just that.</p>
<figure><img src="fig/wordEmbed_NN-training-methods.png" alt="Skipgram" class="figure mx-auto d-block"></figure><div class="section level4">
<h4 id="sentence-processing-with-skip-gram">Sentence Processing With Skip-Gram<a class="anchor" aria-label="anchor" href="#sentence-processing-with-skip-gram"></a></h4>
<p>The Skip-gram version takes as input each word in a sentence, and
tries to guess the most likely surrounding context words associated with
that word. It does this for all sentences and words in a corpus in order
to learn a function that can map each word to its most likely context
words.</p>
<p><em>Have a very nice day.</em></p>
<table class="table"><thead><tr class="header"><th>Input</th>
<th>Output (context words)</th>
</tr></thead><tbody><tr class="odd"><td>Have</td>
<td>a, very</td>
</tr><tr class="even"><td>a</td>
<td>Have, very, nice</td>
</tr><tr class="odd"><td>very</td>
<td>Have, a, nice, day</td>
</tr><tr class="even"><td>nice</td>
<td>a, very, day</td>
</tr><tr class="odd"><td>day</td>
<td>very, nice</td>
</tr></tbody></table><p>In the process of training, the model’s weights learn to derive new
features (weight optimized perceptrons) associated with the input data
(single words). These new learned features will be conducive to
accurately predicting the context words for each word. We will see next
how we can extract these features as word vectors.</p>
</div>
</div>
<div class="section level3">
<h3 id="extracting-word-embeddings-from-the-model">Extracting Word Embeddings From the Model<a class="anchor" aria-label="anchor" href="#extracting-word-embeddings-from-the-model"></a></h3>
<p>With a model trained to predict context words, how can we extract the
model’s learned features as word embeddings? For this, we need a set of
model weights associated with each word fed into the model. We can
achieve this property by:</p>
<ol style="list-style-type: decimal"><li>Converting each input word into a one-hot encoded vector
representation. The vector dimensionality will be equal to the size of
the vocabularly contained in the training data.</li>
<li>Connecting each element of the one-hot encoded vector to each
node/neuron in the subsequent hidden layer of neurons</li>
</ol><p>These steps can be visualized in the Word2Vec model diagram shown
below, with Sigmas representing individual neurons and their ability to
integrate input from previous layers.</p>
<figure><img src="fig/wordEmbed_NN-SG-model-architecture.png" alt="Word2Vec Model Architecture (Skip-gram)" class="figure mx-auto d-block"></figure><p><a href="https://israelg99.github.io/2017-03-23-Word2Vec-Explained/#:~:text=How%20does%20Word2Vec%20produce%20word,to%20reduce%20a%20loss%20function." class="external-link">Image
Source</a></p>
<p>In the above digram, we can see…</p>
<ul><li>The input layer has 10,000 dimensions representing 10,000 words in
this model’s vocabulary</li>
<li>The hidden layer of the model has 300 neurons. Note that this number
also corresponds to the dimensionality of the word vectors extracted
from the model.</li>
<li>The output layer has one neuron for each possible word in the
model’s vocabulary</li>
</ul><p>The word vectors, themselves, are stored in the weights connecting
the input layer to the hidden layer of neurons. Each word will have its
own set of learned weights which we call word vectors. You can think of
each element of the word vectors as encoding different features which
are relevant to the prediction task at hand — predicting context
words.</p>
<div class="section level4">
<h4 id="continuous-bag-of-words-cbow"><strong>Continuous Bag-of-Words (CBOW)</strong><a class="anchor" aria-label="anchor" href="#continuous-bag-of-words-cbow"></a></h4>
<figure><img src="fig/wordEmbed_NN-training-methods.png" alt="Image from Word2Vec research paper, by Mikolov et al" class="figure mx-auto d-block"></figure><p>Before wrapping up with the mechanisms underlying the Word2Vec model,
it is important to mention that the Skip-gram algorithm is not the only
way to train word embeddings using Word2Vec. A similar method known as
the Continuous Bag-of-Words (CBOW) takes as an input the context words
surrounding a target word, and tries to guess the target word based on
those words. Thus, it flips the prediction task faced by Skip-gram. The
CBOW algorithm does not care how far away different context words are
from the target word, which is why it is called a bag-of-words method.
With this task setup, the neural network will learn a function that can
map the surrounding context words to a target word. Similar to
Skip-gram, the CBOW method will generate word vectors stored as weights
of the neural network. However, given the slight adjustment in task, the
weights extracted from CBOW are the ones that connect the hidden layer
of neurons to the output layer.</p>
</div>
</div>
<div class="section level3">
<h3 id="cbow-vs-skip-gram">CBOW vs Skip-gram<a class="anchor" aria-label="anchor" href="#cbow-vs-skip-gram"></a></h3>
<p>Since there are two popular word2vec training methods, how should we
decide which one to pick? Like with many things in machine learning, the
best course of action is typically to take a data-driven approach to see
which one works better for your specific application. However, as
general guidelines according to Mikolov et al.,</p>
<ol style="list-style-type: decimal"><li>Skip-Gram works well with smaller datasets and has been found to
perform better in terms of its ability to represent rarer words</li>
<li>CBOW trains several times faster than Skip-gram and has slightly
better accuracy for more frequent words</li>
</ol></div>
</section><section><h2 class="section-heading" id="recap">Recap<a class="anchor" aria-label="anchor" href="#recap"></a></h2>
<hr class="half-width"><p>Artificial neural networks are powerful machine learning models that
can learn to map input data containing features to a predicted label or
continuous value. In addition, neural networks learn to encode the input
data as hierarchical features of the text during training. The Word2Vec
model exploits this capability, and trains the model on a word
prediction task in order to generate features of words which are
conducive to the prediction task at hand.</p>
<p>In the next episode, we’ll train a Word2Vec model using both training
methods and empirically evaluate the performance of each. We’ll also see
how training Word2Vec models from scratch (rather than using a
pretrained model) can be beneficial in some circumstances.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul><li>Artificial neural networks (ANNs) are powerful models that can
approximate any function given sufficient training data.</li>
<li>The best method to decide between training methods (CBOW and
Skip-gram) is to try both methods and see which one works best for your
specific application.</li>
</ul></div>
</div>
</div>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="07-wordEmbed_intro.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="09-wordEmbed_train-word2vec.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="07-wordEmbed_intro.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Intro to Word
        </a>
        <a class="chapter-link float-end" href="09-wordEmbed_train-word2vec.html" rel="next">
          Next: Training Word2Vec...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/josenino95/python-text-analysis/edit/main/episodes/08-wordEmbed_word2vec-algorithm.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/josenino95/python-text-analysis/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/josenino95/python-text-analysis/" class="external-link">Source</a></p>
				<p><a href="https://github.com/josenino95/python-text-analysis/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.11" class="external-link">sandpaper (0.16.11)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.5" class="external-link">varnish (1.0.5)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://josenino95.github.io/python-text-analysis/08-wordEmbed_word2vec-algorithm.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "The Word2Vec Algorithm",
  "creativeWorkStatus": "active",
  "url": "https://josenino95.github.io/python-text-analysis/08-wordEmbed_word2vec-algorithm.html",
  "identifier": "https://josenino95.github.io/python-text-analysis/08-wordEmbed_word2vec-algorithm.html",
  "dateCreated": "2020-10-12",
  "dateModified": "2025-03-21",
  "datePublished": "2025-04-29"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo --><script>
          var _paq = window._paq = window._paq || [];
          /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
          _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
          _paq.push(["setDomains", ["*.lessons.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
          _paq.push(["setDoNotTrack", true]);
          _paq.push(["disableCookies"]);
          _paq.push(["trackPageView"]);
          _paq.push(["enableLinkTracking"]);
          (function() {
              var u="https://matomo.carpentries.org/";
              _paq.push(["setTrackerUrl", u+"matomo.php"]);
              _paq.push(["setSiteId", "1"]);
              var d=document, g=d.createElement("script"), s=d.getElementsByTagName("script")[0];
              g.async=true; g.src="https://matomo.carpentries.org/matomo.js"; s.parentNode.insertBefore(g,s);
          })();
        </script><!-- End Matomo Code --></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

